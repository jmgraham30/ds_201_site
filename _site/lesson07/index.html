<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="JMG">

<title>DS 201 - Lesson 7</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<link href="../site_libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet">
<script src="../site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">DS 201</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html" rel="" target="">
 <span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../links.html" rel="" target="">
 <span class="menu-text">Links</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jmgraham30/ds_201_site" rel="" target=""><i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  <li><a href="#readings-etc." id="toc-readings-etc." class="nav-link" data-scroll-target="#readings-etc.">Readings, etc.</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#introduction-to-machine-statistical-learning" id="toc-introduction-to-machine-statistical-learning" class="nav-link" data-scroll-target="#introduction-to-machine-statistical-learning">Introduction to Machine (Statistical) Learning</a>
  <ul class="collapse">
  <li><a href="#fitting-supervised-models" id="toc-fitting-supervised-models" class="nav-link" data-scroll-target="#fitting-supervised-models">Fitting Supervised Models</a></li>
  <li><a href="#complexity-vs.-interpretability" id="toc-complexity-vs.-interpretability" class="nav-link" data-scroll-target="#complexity-vs.-interpretability">Complexity Vs. Interpretability</a></li>
  <li><a href="#training-error-vs.-test-error" id="toc-training-error-vs.-test-error" class="nav-link" data-scroll-target="#training-error-vs.-test-error">Training Error Vs. Test Error</a></li>
  <li><a href="#the-bias-variance-trade-off" id="toc-the-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-bias-variance-trade-off">The Bias-Variance Trade-Off</a></li>
  </ul></li>
  <li><a href="#examples-of-supervised-learning" id="toc-examples-of-supervised-learning" class="nav-link" data-scroll-target="#examples-of-supervised-learning">Examples of Supervised Learning</a>
  <ul class="collapse">
  <li><a href="#nearest-neighbors" id="toc-nearest-neighbors" class="nav-link" data-scroll-target="#nearest-neighbors">Nearest Neighbors</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural Networks</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lesson 7</h1>
<p class="subtitle lead">Fundamental Concepts of Supervised Machine Learning</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>JMG </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="learning-objectives" class="level2">
<h2 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h2>
<p>After this lesson, students will be able to:</p>
<ul>
<li><p>Define supervised machine learning in general terms.</p></li>
<li><p>Distinguish between regression and classification problems.</p></li>
<li><p>Understand the concept of error and the significance of test versus training error.</p></li>
<li><p>Appreciate the trade-off between model flexibility and interpretability, and between bias and variance.</p></li>
<li><p>Describe the nearest neighbors, linear regression, and neural networks models for supervised machine learning.</p></li>
</ul>
</section>
<section id="readings-etc." class="level2">
<h2 class="anchored" data-anchor-id="readings-etc.">Readings, etc.</h2>
<ol type="1">
<li><p>Read Chapters 5, 6, 7, &amp; 8 from from <a href="https://datasciencebook.ca/preface.html">Data Science: A First Introduction</a> <span class="citation" data-cites="timbers2022data">(<a href="#ref-timbers2022data" role="doc-biblioref">Timbers, Campbell, and Lee 2022</a>)</span>. <a href="https://datasciencebook.ca/preface.html">View book online</a>.</p></li>
<li><p>Read Chapter 2 of <em>An Introduction to Statistical Learning</em> <span class="citation" data-cites="tibshirani2017introduction">(<a href="#ref-tibshirani2017introduction" role="doc-biblioref">Tibshirani, James, and Trevor 2017</a>)</span>.</p></li>
<li><p>The following two video lectures are also recommended:</p></li>
</ol>
<ul>
<li>Motivating problems for machine (statistical) learning. <a href="https://youtu.be/LvySJGj-88U">Watch video on YouTube</a>.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div class="vembedr" align="center">
<div>
<iframe src="https://www.youtube.com/embed/LvySJGj-88U" width="800" height="450" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</div>
</div>
<ul>
<li>Supervised and unsupervised learning. <a href="https://youtu.be/B9s8rpdNxU0">Watch video on YouTube</a>.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div class="vembedr" align="center">
<div>
<iframe src="https://www.youtube.com/embed/B9s8rpdNxU0" width="800" height="450" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</div>
</div>
</section>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This lesson is essentially about modeling data. Specifically, we are interested to model data that is paired or labeled, where we view one or more variables as predictors and one variable as a corresponding response or label. Let’s consider a motivating example. The <code>penguins</code> data set available through either the <code>modeldata</code> package or the <code>palmerpenguins</code> package records observations on 344 penguins from three species of penguins collected from three islands in the Palmer Archipelago, Antarctica. <a href="#tbl-penguins">Table&nbsp;1</a> shows the first 10 rows of the data set.</p>
<div class="cell">
<div class="cell-output-display">
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; overflow-x: scroll; width:100%; ">
<div id="tbl-penguins" class="anchored">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table&nbsp;1: The <code>penguins</code> data set records observations on 344 penguins from three species of penguins collected from three islands in the Palmer Archipelago, Antarctica.</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: left; position: sticky; top: 0; background-color: #FFFFFF;">species</th>
<th data-quarto-table-cell-role="th" style="text-align: left; position: sticky; top: 0; background-color: #FFFFFF;">island</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">bill_length_mm</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">bill_depth_mm</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">flipper_length_mm</th>
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">body_mass_g</th>
<th data-quarto-table-cell-role="th" style="text-align: left; position: sticky; top: 0; background-color: #FFFFFF;">sex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">39.1</td>
<td style="text-align: right;">18.7</td>
<td style="text-align: right;">181</td>
<td style="text-align: right;">3750</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">39.5</td>
<td style="text-align: right;">17.4</td>
<td style="text-align: right;">186</td>
<td style="text-align: right;">3800</td>
<td style="text-align: left;">female</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">40.3</td>
<td style="text-align: right;">18.0</td>
<td style="text-align: right;">195</td>
<td style="text-align: right;">3250</td>
<td style="text-align: left;">female</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">36.7</td>
<td style="text-align: right;">19.3</td>
<td style="text-align: right;">193</td>
<td style="text-align: right;">3450</td>
<td style="text-align: left;">female</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">39.3</td>
<td style="text-align: right;">20.6</td>
<td style="text-align: right;">190</td>
<td style="text-align: right;">3650</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">38.9</td>
<td style="text-align: right;">17.8</td>
<td style="text-align: right;">181</td>
<td style="text-align: right;">3625</td>
<td style="text-align: left;">female</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">39.2</td>
<td style="text-align: right;">19.6</td>
<td style="text-align: right;">195</td>
<td style="text-align: right;">4675</td>
<td style="text-align: left;">male</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">34.1</td>
<td style="text-align: right;">18.1</td>
<td style="text-align: right;">193</td>
<td style="text-align: right;">3475</td>
<td style="text-align: left;">NA</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adelie</td>
<td style="text-align: left;">Torgersen</td>
<td style="text-align: right;">42.0</td>
<td style="text-align: right;">20.2</td>
<td style="text-align: right;">190</td>
<td style="text-align: right;">4250</td>
<td style="text-align: left;">NA</td>
</tr>
</tbody>
</table>
</div>
</div>

</div>
</div>
<p>One problem we might be interested in is to model the body mass of a penguin based on the three other body measurements and its sex and species. Here body mass would be our response variable and bill length, bill depth, flipper length, species, and sex would be our predictor variables. In this case, since the response variable is numerical, we say that we have a regression problem. <a href="#fig-penguin-regression">Figure&nbsp;1</a> illustrates this regression problem.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-penguin-regression" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-penguin-regression-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption class="figure-caption">Figure&nbsp;1: A regression problem for the <code>penguins</code> data set.</figcaption>
</figure>
</div>
</div>
</div>
<p>Instead, we might be interested to model the sex of a penguin based on its physical measurements and its species. In this case, since the response is categorical, we say that we have a classification problem. <a href="#fig-penguin-classification">Figure&nbsp;2</a> illustrates this classification problem.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-penguin-classification" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-penguin-classification-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption class="figure-caption">Figure&nbsp;2: A classification problem for the <code>penguins</code> data set.</figcaption>
</figure>
</div>
</div>
</div>
<p>Our approach to modeling data will be via <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised machine learning</a>, also known as statistical learning that uses data to build mathematical models to model and gain insight from data. The methods of machine learning are currently very popular in data science for the role they play in predictive modeling but are also commonly used for inferential purposes. Machine learning is also a currently prominent approach to developing artificial intelligence technologies. <a href="#fig-ai-ml">Figure&nbsp;3</a> illustrates the relationship between AI, machine learning, and deep learning. Of course, what one means by a “useful insight” is highly dependent on the domain of specialization or area of application. Thus, machine learning is an inherently interdisciplinary field that intersects with many disciplines such as computer science, data science, mathematics and statistics and a variety of other fields.</p>
<div id="fig-ai-ml" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://i.vas3k.blog/7vw.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Illustration credit: https://vas3k.com/blog/machine_learning/</figcaption>
</figure>
</div>
<p><a href="https://en.wikipedia.org/wiki/Supervised_learning"><strong>Supervised machine learning</strong></a> builds models to predict response values based on corresponding predictor values by using example data that comes as a source and target pair. <a href="#fig-supervised">Figure&nbsp;4</a> illustrates the supervised learning paradigm.</p>
<div id="fig-supervised" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://static.javatpoint.com/tutorial/machine-learning/images/supervised-machine-learning.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Supervised machine learning builds models to predict response values based on corresponding predictor values by using example data that comes as a source and target pair. Illustration credit: https://www.javatpoint.com</figcaption>
</figure>
</div>
<p>In this lesson, we will learn the basic concepts of supervised machine learning. We will use the <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">nearest neighbors algorithm</a> to facilitate our understanding of the basic principles of supervised learning. This algorithm is a simple but useful machine learning algorithm. In future lessons, we will learn about additional commonly used supervised learning methods.</p>
</section>
<section id="introduction-to-machine-statistical-learning" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-machine-statistical-learning">Introduction to Machine (Statistical) Learning</h2>
<p><a href="https://en.wikipedia.org/wiki/Machine_learning"><strong>Machine learning</strong></a> or statistical learning generally refers to methods or tools that seek to derive insight or understanding from data by using <em>models</em>. Here by model we mean a mathematical or computational representation of some part of the real world. In machine learning, we <em>fit</em> or <em>learn</em> a model or class of models to data. The goal of fitting models is usually one of the following:</p>
<ol type="1">
<li><p><strong>Prediction</strong> - using what is known or has been observed to make informed (hopefully accurate) claims about what we want to know or has yet to be observed.</p></li>
<li><p><strong>Inference</strong> - using a sample to make informed (hopefully accurate) claims about a larger population. For example, we might want to know which predictors are associated with a response, or what is the relationship between the response and each predictor.</p></li>
</ol>
<p>For an example of prediction, suppose that we are advertising experts working with a customer that sales video games. Our customer cannot directly control their sales but they can directly control their marketing by deciding how much to invest in advertising. Say for example that our customer has three ways to advertise: via YouTube, via podcasts, or via Spotify. We can use our past knowledge about how much our customer has spent <em>i.e.</em>, their advertising budget and the corresponding sales to make predictions using a model about how sales will be in the future depending on how the company changes its advertising in each of the three media.</p>
<p>For an example of inference, suppose we want to know if caffeine consumption is associated with exam performance for students at the University of Scranton. We could collect data for a number of students on how much caffeine they’ve had before and exam and then record the corresponding exam performance. Here we probably aren’t interested to predict what grade someone will get based on consuming a certain amount of caffeine but rather we are interested in whether or not there is an association between caffeine consumption and exam performance. We could use a model to make inferences about this association.</p>
<p>While this course will only look at supervised learning, it’s worth taking a moment to point out that there are actually two prominent broad classes of machine learning models:</p>
<ol type="1">
<li><strong>Supervised</strong> - In supervised learning, data comes in pairs <span class="math inline">\((y_{i},{\bf x}_{i})\)</span> where we view <span class="math inline">\({\bf x}_{i}\)</span> (which may be a vector) as a predictor and <span class="math inline">\(y_{i}\)</span> as a response. Often, We the predictors are something we can influence directly like the advertising budget from our earlier example while the response is something we don’t have direct control over like the sales from our example. Thus, there is an assumed functional relationship between predictors and the response of the form</li>
</ol>
<p><span class="math display">\[
y = f({\bf x}) + \epsilon
\]</span></p>
<p>where we think of <span class="math inline">\(f({\bf x})\)</span> as the mean value for <span class="math inline">\(y\)</span> viewed as a <strong>random variable</strong> and <span class="math inline">\(\epsilon\)</span> as containing the variance of <span class="math inline">\(y\)</span> so that <span class="math inline">\(E[\epsilon] = 0\)</span>.</p>
<p>We note that <span class="math inline">\(y\)</span> may be numerical in which case we have a <strong>regression</strong> problem or it may be categorical in which case we have a <strong>classification</strong> problem.</p>
<ol start="2" type="1">
<li><strong>Unsupervised</strong> - In unsupervised learning, there is no response variable. Some common unsupervised problems include clustering and density estimation. Both of these essentially seek to discover a pattern in the data.</li>
</ol>
<p><a href="#fig-mls">Figure&nbsp;5</a> illustrates the distinction between supervised and unsupervised learning models.</p>
<div id="fig-mls" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://i.vas3k.blog/7w1.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Illustration credit: https://vas3k.com/blog/machine_learning/</figcaption>
</figure>
</div>
<p>In this course, we will focus on supervised learning and save discussions on unsupervised learning for future courses.</p>
<section id="fitting-supervised-models" class="level3">
<h3 class="anchored" data-anchor-id="fitting-supervised-models">Fitting Supervised Models</h3>
<p>Fitting a supervised learning model typically amounts to estimating the function <span class="math inline">\(f\)</span> in the assumed relationship</p>
<p><span class="math display">\[
y = f({\bf x}) + \epsilon
\]</span> between the predictor and response variables. When we estimate <span class="math inline">\(f\)</span> we denote the estimate by <span class="math inline">\(\hat{f}\)</span>. Then, we can use <span class="math inline">\(\hat{f}\)</span> to predict the response for each predictor value <span class="math inline">\({\bf x}\)</span> by computing</p>
<p><span class="math display">\[
\hat{y} = \hat{f}({\bf x})
\]</span></p>
<p>How do we estimate a function <span class="math inline">\(f\)</span>? In machine learning, we use the data together with some algorithm to construct <span class="math inline">\(\hat{f}\)</span>. The general steps are:</p>
<ol type="1">
<li><p>Specify a class of functions from which to choose <span class="math inline">\(\hat{f}\)</span>.</p></li>
<li><p>Specify a <strong>loss</strong> function that measures how well a given <span class="math inline">\(\hat{f}\)</span> fits the data. That is, the loss functions is a quantitative comparison between the observed response values and predicted response values. The loss function is often denoted by <span class="math inline">\(L(y,\hat{y}) = L(y,\hat{f}({\bf x}))\)</span>.</p></li>
<li><p>Find the <span class="math inline">\(\hat{f}\)</span> that minimizes the loss function. Note that this involves solving an optimization problem.</p></li>
</ol>
<section id="regression" class="level4">
<h4 class="anchored" data-anchor-id="regression">Regression</h4>
<p>Let’s consider an illustrative example where <span class="math inline">\({\bf x}\)</span> represents the years of education of some individuals and <span class="math inline">\(y\)</span> is the income they earn in their profession. Thus, both variables are numerical so we are dealing with a regression problem. We are assuming that there is a true but unknown functional relationship between the years of education and the income they earn.</p>
<p>The left panel of <a href="#fig-sl-reg">Figure&nbsp;6</a> shows a scatter plot of our education versus income data while the right panel shows the data again but with a curve corresponding to the graph of a function <span class="math inline">\(\hat{f}\)</span> that passes through the data.</p>
<div id="fig-sl-reg" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://www.dropbox.com/scl/fi/4zmi7ql5ooxcafkym0csb/2_2.jpg?rlkey=jj0j32tvr43xcn88i2via58jd&amp;dl=1" style="width:8in;height:4in" alt="Figure with two panels. The left shows a scatter plot of data while the right shows the same scatter plot but with curve fitted to the data." class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Illustration of supervised learning through a regression problem. Figure from <span class="citation" data-cites="tibshirani2017introduction">(<a href="#ref-tibshirani2017introduction" role="doc-biblioref">Tibshirani, James, and Trevor 2017</a>)</span>.</figcaption>
</figure>
</div>
<p>How did we come up with the function <span class="math inline">\(\hat{f}\)</span>? Basically, we minimized the <strong>residual error</strong> between our predicted and observed response. That is, for each response value <span class="math inline">\({\bf x}\)</span> we minimized how far <span class="math inline">\(y=f({\bf x})\)</span> can be from <span class="math inline">\(\hat{y}=\hat{f}({\bf x})\)</span>. There are three important points that need to be addressed before we can implement regression in a practical situation.</p>
<ol type="1">
<li><p>The set of all functions is too large to work with in practice so we must make some choices that allow us to narrow down the class of functions from which <span class="math inline">\(\hat{f}\)</span> will be taken. For example, we could restrict to only linear functions, or only quadratic functions, or only polynomial functions. These classes of functions are easy to describe because these types of functions are uniquely described by a finite number of parameters. However, sometimes data can not be modeled well by, <em>e.g.</em>, polynomials so more sophisticated non-parametric ways of describing classes of functions have been developed that allow for more flexible modeling.</p></li>
<li><p>We must decide on how we will define and measure error. That is, we must specify an appropriate loss function. For regression problems, a typical way to measure error is the <strong>squared-error</strong>. Referring back to the right side of <a href="#fig-sl-reg">Figure&nbsp;6</a>, we define the <span class="math inline">\(i\)</span>-th <strong>residual</strong> <span class="math inline">\(r_{i}\)</span> to be the vertical (signed) distance between the observed response value <span class="math inline">\(y_{i}\)</span> and the corresponding predicted value <span class="math inline">\(\hat{y}_{i} = \hat{f}({\bf x}_{i})\)</span>. That is,</p></li>
</ol>
<p><span class="math display">\[
r_{i} = y_{i} - \hat{y}_{i}
\]</span> Then the squared error (SE) is</p>
<p><span class="math display">\[
\text{SE} = \sum_{i=1}^{n}r_{i}^{2} = \sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2} = \sum_{i=1}^{n}(y_{i} - \hat{f}({\bf{x}_{i}}))^{2}
\]</span></p>
<p>In this case, we take <span class="math inline">\(\hat{f}\)</span> to be the function from some specified class of functions such that it minimizes the corresponding SE.</p>
<p><strong>Important Point:</strong> A main component of many if not most supervised machine learning problems is solving some kind of optimization problem. Usually when one speaks of a machine learning algorithm (or learning algorithm), what they are actually referring to is some algorithm that is used to solve an appropriate optimization problem.</p>
<ol start="3" type="1">
<li>We have to distinguish between <em>reducible error</em> and <em>irreducible error</em>. No machine learning model will ever be perfect. Suppose that we have an estimate <span class="math inline">\(\hat{f}\)</span> that yields a prediction <span class="math inline">\(\hat{y} = \hat{f}({\bf x})\)</span>. Since in reality the response is a random variable</li>
</ol>
<p><span class="math display">\[
y = f({\bf x}) + \epsilon
\]</span> we have</p>
<p><span class="math display">\[
\begin{align*}
\text{E}[(y - \hat{y})^{2}] &amp;= \text{E}[(f({\bf x}) + \epsilon - \hat{f}({\bf x}))^2] \\
&amp;= \text{E}[((f({\bf x}) - \hat{f}({\bf x})) + \epsilon)^2] \\
&amp;= \text{E}[(f({\bf x}) - \hat{f}({\bf x}))^2 - 2\epsilon (f({\bf x}) - \hat{f}({\bf x})) + \epsilon^2] \\
&amp;= \text{E}[(f({\bf x}) - \hat{f}({\bf x}))^2] - 2(f({\bf x}) - \hat{f}({\bf x}))\text{E}[\epsilon] + \text{E}[(\epsilon - 0)^2] \\
&amp;= \text{E}[(f({\bf x}) - \hat{f}({\bf x}))^2] + \text{Var}[\epsilon]
\end{align*}
\]</span></p>
<p>By choosing a good enough family of functions or a good enough learning algorithm we can reduce <span class="math inline">\(\text{E}[(f({\bf x}) - \hat{f}({\bf x}))^2]\)</span> as much as we want. This corresponds to the <strong>reducible error.</strong> However, we have no control over <span class="math inline">\(\text{Var}[\epsilon]\)</span> and this corresponds to the <strong>irreducible error</strong>.</p>
</section>
<section id="classification" class="level4">
<h4 class="anchored" data-anchor-id="classification">Classification</h4>
<p>For classification problems in supervised machine learning, the response variable is <strong>categorical</strong>. <a href="#fig-sl-cl">Figure&nbsp;7</a> illustrates this, showing a scatter plot of data where coloring is used to distinguish the data points as belonging to one of two different classes.</p>
<div id="fig-sl-cl" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://www.dropbox.com/scl/fi/qch7u8vzepor8egwgres5/2_15.png?rlkey=uc3vbxxu1tvrb3a92qxo4ssgg&amp;dl=1" style="width:6in;height:6in" alt="Figure showing a scatter plot of data where coloring is used to distinguish the data points as belonging to one of two different classes." class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Illustration of a classification problem in which the response variable is a binary categorical variable. Figure from <span class="citation" data-cites="tibshirani2017introduction">(<a href="#ref-tibshirani2017introduction" role="doc-biblioref">Tibshirani, James, and Trevor 2017</a>)</span>.</figcaption>
</figure>
</div>
<p>For classification problems, our goal is still to estimate a functional relationship of the form <span class="math inline">\(y = f({\bf x}) + \epsilon\)</span>. However, we can no longer measure error using the squared error because the response values are not numerical. A common method for measuring error in classification problems is <strong>classification error</strong> (CE) defined by</p>
<p><span class="math display">\[
\text{CE} = \sum_{i=1}^{n}I(y_{i} \neq \hat{y}_{i})
\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the <em>indicator function</em> that is equal to 1 whenever <span class="math inline">\(y_{i} \neq \hat{y}_{i}\)</span> and equal to 0 whenever <span class="math inline">\(y_{i} = \hat{y}_{i}\)</span>. Essentially, CE counts the number of misclassifications.</p>
<p>Similar to regression, fitting a classification model involves finding a function <span class="math inline">\(\hat{f}\)</span> from some specified class of functions such that the corresponding CE is minimized.</p>
<p>Note that it is possible to convert a regression problem to a classification problem by binning or discretizing the response variable in some way.</p>
</section>
</section>
<section id="complexity-vs.-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="complexity-vs.-interpretability">Complexity Vs. Interpretability</h3>
<p>Another issue that needs to be taken into account when fitting models is the tradeoff between how easy it is to interpret a model versus the maximum degree of accuracy for the model. <a href="#fig-mod-complex">Figure&nbsp;8</a> illustrates this through a representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity.</p>
<div id="fig-mod-complex" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://www.dropbox.com/scl/fi/507z03ox81b15dbxe1pqk/2_7.png?rlkey=vs3rp0x3h5o0qvik8zhjbngsv&amp;dl=1" style="width:6in;height:6in" alt="" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;8: A representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity. Figure from <span class="citation" data-cites="tibshirani2017introduction">(<a href="#ref-tibshirani2017introduction" role="doc-biblioref">Tibshirani, James, and Trevor 2017</a>)</span>.</figcaption>
</figure>
</div>
</section>
<section id="training-error-vs.-test-error" class="level3">
<h3 class="anchored" data-anchor-id="training-error-vs.-test-error">Training Error Vs. Test Error</h3>
<p>When we fit a model to data, say by minimizing the error the resulting estimate function we get depends on the data used to fit the model. We refer to this data as the <strong>training data</strong> and the corresponding error as the <strong>training error</strong>. By choosing a sufficiently flexible set of functions from which to fit to the data, we can make the training error as small as we want. This might seem like a great thing, but there is a major problem with it.</p>
<p>Suppose we want to use a model to make predictions about future unseen values of our predictor <span class="math inline">\({\bf x}\)</span>. If a model is fit too well to the training data, then in general it tends not to be very good at making accurate predictions for future values. One says that models that are <strong>overfit</strong> to the training data are poor at <strong>generalization</strong>.</p>
<p>How do we build models that generalize well and avoid overfitting? A common approach is to separate data into a training set that is used to fit a model and a test set which is used to assess how well the models generalizes to unseen data via the <strong>test error</strong>. <a href="#fig-bv-trade">Figure&nbsp;9</a> shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data.</p>
<div id="fig-bv-trade" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://www.dropbox.com/scl/fi/1arxit27ttbfmi527iare/2_9.png?rlkey=ohyvt8c4ao38hq0ggaul5ykm8&amp;dl=1" style="width:8in;height:4in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9: The left panel shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data. Figure from <span class="citation" data-cites="tibshirani2017introduction">(<a href="#ref-tibshirani2017introduction" role="doc-biblioref">Tibshirani, James, and Trevor 2017</a>)</span>.</figcaption>
</figure>
</div>
<p>While the training/test set approach to fitting accurate models while avoiding overfitting is very good in principle, there are some practical limitations. For example,</p>
<ol type="1">
<li><p>How do we know the training data is sufficiently representative?</p></li>
<li><p>What if we don’t have a sufficiently large data set to split into a training and a test set?</p></li>
<li><p>How do we know what the minimum possible test error is?</p></li>
</ol>
<p>We will spend a lot of time later talking more about these issues and ways to deal with them.</p>
</section>
<section id="the-bias-variance-trade-off" class="level3">
<h3 class="anchored" data-anchor-id="the-bias-variance-trade-off">The Bias-Variance Trade-Off</h3>
<p>Referring back to <a href="#fig-bv-trade">Figure&nbsp;9</a>, notice the distinct U-shape in the curve for the test error. This is more than just a curiosity, it is the result of another type of trade-off known as the <strong>bias-variance</strong> trade-off.</p>
<p>Let’s try to get a sense for this starting with some intuition. Suppose we having a regression problem with a single predictor. If we restrict to the class of linear functions, that is functions with graph that is a straight line in the plane, then any such function is uniquely specified by two parameters, the slope and intercept. Intuitively, such as model is highly biased because it’s going to make very rigid predictions. However, linear functions have low variance in the sense than models fit to similar data will have very similar slope and intercept values. On the other hand, a cubic polynomial being described uniquely by four parameters is much less biased than a linear function but will have higher variance.</p>
<p>It is outside the scope of this course, but it can be shown that the expected squared error for an observed value <span class="math inline">\({\bf x}_{0}\)</span> can be decomposed as follows:</p>
<p><span class="math display">\[
\text{E}[(y_{0} - \hat{f}({\bf x}))^2] = \text{Var}(\hat{f}({\bf x}_{0})) + [\text{Bias}(\hat{f}({\bf x}_{0}))]^2 + \text{Var}(\epsilon)
\]</span></p>
<p>We refer to the first two terms as</p>
<ul>
<li><p>the variance of <span class="math inline">\(\hat{f}({\bf x}_{0})\)</span></p></li>
<li><p>the squared bias of <span class="math inline">\(\hat{f}({\bf x}_{0})\)</span></p></li>
</ul>
<p><a href="#fig-bv">Figure&nbsp;10</a> illustrates this formula.</p>
<div id="fig-bv" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://www.dropbox.com/scl/fi/nof2kpjta6p5mlg8nxycd/2_12.png?rlkey=3ubpphur6n0h1ics55k0fm38e&amp;dl=1" style="width:8in;height:4in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Squared bias (blue curve), variance (orange curve), Var(ε) (dashed line), and test MSE (red curve). The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE. Figure from <span class="citation" data-cites="tibshirani2017introduction">(<a href="#ref-tibshirani2017introduction" role="doc-biblioref">Tibshirani, James, and Trevor 2017</a>)</span>.</figcaption>
</figure>
</div>
<p><strong>Important:</strong> What you should keep in mind as we proceed through the course is the following:</p>
<ul>
<li><p>Simple models tend to have high bias but much lower variance.</p></li>
<li><p>Complex models tend to have lower bias but much higher variance.</p></li>
</ul>
<p>Anytime you choose a particular modeling approach for a specific application or data set, you should take into account the bias-variance trade-off.</p>
</section>
</section>
<section id="examples-of-supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="examples-of-supervised-learning">Examples of Supervised Learning</h2>
<section id="nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="nearest-neighbors">Nearest Neighbors</h3>
<p>The <strong>nearest neighbors</strong> approach to supervised machine learning is a very simple and intuitive approach to modeling. For example, suppose we have a data set consisting of a single predictor <span class="math inline">\({\bf x}\)</span> and a response <span class="math inline">\(y\)</span>. The nearest neighbors approach to regression is to predict the response for a new value of <span class="math inline">\({\bf x}\)</span> by averaging the responses for the <span class="math inline">\(k\)</span> nearest values of <span class="math inline">\({\bf x}\)</span> in the training data. Nearest neighbors can also be used for classification problems. In this case, we predict the class of a new value of <span class="math inline">\({\bf x}\)</span> by taking a majority vote of the classes of the <span class="math inline">\(k\)</span> nearest values of <span class="math inline">\({\bf x}\)</span> in the training data.</p>
<p>Let’s proceed to an online interactive demo for nearest neighbors applied to a classification problem. <a href="https://codepen.io/gangtao/pen/PPoqMW">View the demo</a>.</p>
<p><a href="https://github.com/jmgraham30/knn_tidy_examples">This GitHub repository</a> contains code in R that implements nearest neighbors for both regression and classification problems. Let’s go through this together.</p>
</section>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">Linear Regression</h3>
<p>Recall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:</p>
<p><span class="math display">\[
y = f({\bf x}) + \epsilon
\]</span></p>
<p>and then we seek to find a function <span class="math inline">\(\hat{f}\)</span> from some specified class of functions that does a good job in approximating <span class="math inline">\(f\)</span>. Let’s study this problem in more detail but in a very simple setting. Specifically, we will assume that <span class="math inline">\({\bf x}\)</span> and <span class="math inline">\(y\)</span> are both single numerical variables and that <span class="math inline">\(f\)</span> is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> such that</p>
<p><span class="math display">\[
y = \beta_{0} + \beta_{1} x + \epsilon
\]</span> for all values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Recall that we are assuming that <span class="math inline">\(\text{E}[\epsilon] = 0\)</span> so <span class="math inline">\(\epsilon\)</span> is a random variable with expected value (or mean) equal to zero.</p>
<p>If we restrict ourselves to the class of single-variable linear functions, then finding an approximation to <span class="math inline">\(f(x) = \beta_{0} + \beta_{1} x\)</span> is equivalent to finding values <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> so that</p>
<p><span class="math display">\[
\hat{f}(x) = \hat{\beta}_{0} + \hat{\beta}_{1} x \approx f(x) = \beta_{0} + \beta_{1} x
\]</span></p>
<p>Thus, this would be a <strong>parametric</strong> model since any candidate approximating function is uniquely specified by specifying the values for the parameters <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>. Note that this is in contrast to nearest neighbors which is <strong>nonparametric</strong>.</p>
<p><a href="#fig-slr">Figure&nbsp;11</a> shows the plot of data that has been generated by a relationship of the form <span class="math inline">\(y = \beta_{0} + \beta_{1} x + \epsilon\)</span>. You should examine the code used to create or simulate the data in this example and see how it relates to the expression <span class="math inline">\(y = \beta_{0} + \beta_{1} x + \epsilon\)</span>.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1287</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="at">mean=</span><span class="dv">72</span>,<span class="at">sd=</span><span class="dv">12</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">1.2</span> <span class="sc">+</span> <span class="fl">0.75</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(N,<span class="at">sd=</span><span class="dv">2</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>xy_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>xy_data <span class="sc">%&gt;%</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)) <span class="sc">+</span> </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-slr" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-slr-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;11: A data set with two numerical variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> generated by an underlying linear function so that <span class="math inline">\(y = \beta_{0} + \beta_{1}x + \epsilon\)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>From a (supervised) machine learning perspective, fitting a line to such data means “learning” the values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> from the data. How do we learn <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>? One way to do this is to minimize an appropriate <strong>loss</strong> function which is a function that provides a measure of error between the observed response values and the response values predicted by the model.</p>
<p><a href="#fig-resids">Figure&nbsp;12</a> shows the same data as in <a href="#fig-slr">Figure&nbsp;11</a> but where we have added a best fit line as well as all the residual values. One way to learn the values for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> is to minimize the squared error for the residuals.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>fitted_linear_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data=</span>xy_data) <span class="sc">%&gt;%</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">augment</span>()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>fitted_linear_model <span class="sc">%&gt;%</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)) <span class="sc">+</span> </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">"lm"</span>,<span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">xend =</span> x, <span class="at">yend =</span> .fitted), </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">color=</span><span class="st">"red"</span>,<span class="at">lwd=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-resids" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-resids-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;12: The same data as shown in <a href="#fig-slr">Figure&nbsp;11</a> but with a best fit line as well as all residuals also shown.</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that we can write our loss function, that is, the squared error for the residuals as a function of two variables <span class="math inline">\(L(\beta_{0},\beta_{1})\)</span> defined by</p>
<p><span class="math display">\[
L(\beta_{0},\beta_{1}) = \sum_{i=1}^{n}(y_{i} - \beta_{0} - \beta_{1}x_{i})^2
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations in the data set. To fit our model, we need to find the values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> that minimize <span class="math inline">\(L(\beta_{0},\beta_{1})\)</span>.</p>
<section id="multiple-linear-regression" class="level4">
<h4 class="anchored" data-anchor-id="multiple-linear-regression">Multiple Linear Regression</h4>
<p>Suppose that we have data of the form <span class="math inline">\((y_{i},{\bf x}_{i}) = (y_{i},x_{i1},x_{i2},\ldots , x_{ip})\)</span> so that there are <span class="math inline">\(p\)</span> predictor variables. A multiple linear regression model takes the form</p>
<p><span class="math display">\[
y = \beta_{0} + \beta_{1}{\bf x}_{1} + \beta_{2}{\bf x}_{2} + \cdots + \beta_{p}{\bf x}_{p} + \epsilon
\]</span> Multiple linear regression is a significant generalization of simple linear regression because it not only allows us to account for multiple predictor variables, but also allows us to account for certain types of nonlinearity and also predictor variables that are categorical. This is because:</p>
<ol type="1">
<li><p>The “linear” part of linear regression refers to linearity with respect to the coefficients <span class="math inline">\({\bf \beta}\)</span>.</p></li>
<li><p>We can use dummy variables to represent categorical predictor variables.</p></li>
</ol>
</section>
</section>
<section id="neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks">Neural Networks</h3>
<p>Neural networks are a class of machine learning models that are inspired by the structure of the brain. They are composed of a series of layers of nodes or “neurons” that are connected to each other. Each artificial neuron is a simple computational unit that takes in a set of inputs, performs a computation, and produces an output. The output of one neuron is then used as the input to the next neuron. The first layer of artificial neurons is called the input layer and the last layer of neurons is called the output layer. The layers in between the input and output layers are called <strong>hidden layers</strong>. The number of hidden layers in a neural network is called the depth of the network. The number of neurons in each layer is called the width of the network. <a href="#fig-slnn">Figure&nbsp;13</a> shows a neural network with one hidden layer consisting of 5 neurons or nodes.</p>
<div id="fig-slnn" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://www.dropbox.com/scl/fi/t16o0um8wenwr0kq0t6i4/10_1.png?rlkey=epw6xbd5x8qpu9xqtsdmryqaq&amp;raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;13: A neural network with a single hidden layer consisting of four neurons or nodes.</figcaption>
</figure>
</div>
<p>Neural networks are conceptually simple but the mathematical details can be confusing. The general idea is that a neural network takes an input of <span class="math inline">\(p\)</span> predictor variables <span class="math inline">\(X = (X_{1},X_{2},\ldots , X_{p})\)</span> and builds a <em>nonlinear</em> function <span class="math inline">\(f(X)\)</span> to predict the response <span class="math inline">\(Y\)</span>. What distinguishes neural networks from other nonlinear methods is the particular structure of the model function <span class="math inline">\(f\)</span>.</p>
<section id="exploring-a-neural-network-interactively" class="level4">
<h4 class="anchored" data-anchor-id="exploring-a-neural-network-interactively">Exploring a Neural Network Interactively</h4>
<p>In order to develop some intuition, we will start by exploring an interactive visualization of a neural network via the <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.92779&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">Neural Network Playground</a> website. <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.92779&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">Visit the Neural Network Playground</a>.</p>
<p>The visualization allows you to create a neural network and then train it on a data set. The data set can be a classification problem or a regression problem. The visualization allows you to change the defining components of a neural network such as activation function, the number of hidden layers, the number of neurons in each layer.</p>
</section>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-tibshirani2017introduction" class="csl-entry" role="listitem">
Tibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. <em>An Introduction to Statistical Learning</em>. springer publication.
</div>
<div id="ref-timbers2022data" class="csl-entry" role="listitem">
Timbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. <em>Data Science: A First Introduction</em>. CRC Press.
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Expand for Session Info
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>─ Session info ───────────────────────────────────────────────────────────────
 setting  value
 version  R version 4.3.1 (2023-06-16)
 os       macOS Sonoma 14.0
 system   aarch64, darwin20
 ui       X11
 language (EN)
 collate  en_US.UTF-8
 ctype    en_US.UTF-8
 tz       America/New_York
 date     2023-10-25
 pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)
 quarto   1.3.450 @ /usr/local/bin/quarto

─ Packages ───────────────────────────────────────────────────────────────────
 package      * version date (UTC) lib source
 broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)
 dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)
 dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)
 forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)
 ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)
 ggthemes     * 4.2.4   2021-01-20 [1] CRAN (R 4.3.0)
 infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)
 ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)
 kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)
 lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)
 modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)
 parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)
 purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)
 readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)
 recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)
 rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)
 scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.3.0)
 sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)
 stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)
 tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)
 tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)
 tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)
 tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)
 tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)
 tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)
 workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)
 workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)
 yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)

 [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library

──────────────────────────────────────────────────────────────────────────────</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode"><img src="http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1" class="img-fluid figure-img" style="width:15.0%"></a></p>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div>CC BY-NC-SA 4.0</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>