[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS 201: Introduction to Data Science",
    "section": "",
    "text": "Welcome to the course website for DS 201: Introduction to Data Science for Fall 2023.\nThis course is offered at the University of Scranton and is being taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "DS 201: Introduction to Data Science",
    "section": "License",
    "text": "License\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html#required-readings",
    "href": "syllabus.html#required-readings",
    "title": "Syllabus",
    "section": "Required Readings",
    "text": "Required Readings\n\nR for Data Science by Hadley Wickham & Garrett Grolemund, view the free online version of the text.\nData Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee, view the free online version of the text."
  },
  {
    "objectID": "syllabus.html#additional-references",
    "href": "syllabus.html#additional-references",
    "title": "Syllabus",
    "section": "Additional References",
    "text": "Additional References\n\nAn Introduction to Statistical Learning 2nd Ed. by James, Witten, Hastie, and Tibshirani, freely available online here.\nTree-Based Methods for Statistical Learning by Brandon M. Greenwell, freely available online here."
  },
  {
    "objectID": "syllabus.html#recommended-readings",
    "href": "syllabus.html#recommended-readings",
    "title": "Syllabus",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nHands-On Programming with R by Garrett Grolemund, view the free online version of the text.\nTelling Stories with Data by Rohan Alexander, view the free online version of the text.\nData Visualization A Practical Introduction by Healy, view the free online version of the text.\nStatistical Inference via Data Science A ModernDive into R and the Tidyverse by Chester Ismay & Albert Y. Kim, view the free online version of the text."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the repository.\nThe R Graph Gallery, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "links.html#texts",
    "href": "links.html#texts",
    "title": "Links",
    "section": "Texts",
    "text": "Texts\n\nR Related\n\nggplot2: Elegant Graphics for Data Analysis (3e), view the free online version of the text\nSpatial Data Science with applications in R, view the free online version of the text.\nAnalyzing US Census Data: Methods, Maps, and Models in R textbook, view the free online version of the text.\nTidy Finance with R, view the free online version of the text.\nOutstanding User Interfaces with Shiny textbook, view the free online version of the text.\nCrime by the Numbers: A Criminologist’s Guide to R textbook, view the free online version of the text.\n\n\n\nPython Related\n\nPython for Data Analysis, 3E, view the free online version of the text"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "DS 201: Introduction to Data Science",
    "section": "",
    "text": "Welcome to the course website for DS 201: Introduction to Data Science for Fall 2023.\nThis course is offered at the University of Scranton and is being taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "links.html#websites",
    "href": "links.html#websites",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the repository.\nThe R Graph Gallery, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "lesson01/index.html#readings-etc.",
    "href": "lesson01/index.html#readings-etc.",
    "title": "Lesson 1",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead the preface of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). View the free online version of the text..\nSkim the README for the Tidy Tuesday data repository (Mock 2018). View the repository. Throughout the semester, we will use example data from the Tidy Tuesday data repository."
  },
  {
    "objectID": "lesson01/index.html#references",
    "href": "lesson01/index.html#references",
    "title": "Lesson 1",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. n.d. “Telling Stories with Data: With Applications in r.”\n\n\nChristian, Brian. 2020. The Alignment Problem: Machine Learning and Human Values. WW Norton & Company.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020. Swirl: Learn r, in r. https://CRAN.R-project.org/package=swirl.\n\n\nMock, J. Thomas. 2018. “Tidy Tuesday.” GitHub Repository. https://github.com/rfordatascience/tidytuesday; GitHub.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-08-27\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder   * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra  * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson01/index.html",
    "href": "lesson01/index.html",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nGive a general definition of data science.\nExplain what is meant by reproducible and auditable workflows.\nUse the Gapminder online tools ( view website) to explore data on certain social and economic issues.\nExplain and give examples of numerical and categorical data types.\nDo basic arithmetic with R."
  },
  {
    "objectID": "lesson01/index.html#learning-objectives",
    "href": "lesson01/index.html#learning-objectives",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nGive a general definition of data science.\nExplain what is meant by reproducible and auditable workflows.\nUse the Gapminder online tools ( view website) to explore data on certain social and economic issues.\nExplain and give examples of numerical and categorical data types.\nDo basic arithmetic with R."
  },
  {
    "objectID": "links.html#videos",
    "href": "links.html#videos",
    "title": "Links",
    "section": "Videos",
    "text": "Videos\n\nGetting started with Quarto YouTube video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nDatasheets for Datasets video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nHow to Make a Custom R Package video, watch the video on YouTube."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nAn introduction to basic data science workflow following current best practices. This course will introduce students to computational or algorithmic ways to think about and learn from data. Emphasis will be placed on data visualization, exploratory data analysis, and foundational modeling principles and techniques implemented using an appropriate programming language.\n\n\nPrerequisites\n\nMath Placement PT score of 14 or higher, or ALEKS score of 76 or higher, or MATH 114, or permission of instructor"
  },
  {
    "objectID": "syllabus.html#student-learning-objectives-and-assessment",
    "href": "syllabus.html#student-learning-objectives-and-assessment",
    "title": "Syllabus",
    "section": "Student Learning Objectives and Assessment:",
    "text": "Student Learning Objectives and Assessment:\n\n\n\n\nTable 1: Course objectives and assessment.\n\n\nCourse SLO\nAssessment\n\n\n\n\nAfter completing this course, students will be able to import, format, and transform common data-set types programmatically and build effective visualizations of data\nHomework, Data Package Assignment, and Project\n\n\nAfter completing this course, students will be able to apply appropriate exploration and modeling techniques to learn from data.\nHomework and Project\n\n\nAfter completing this course, students will be able to present and communicate results obtained via data analysis in an effective manner.\nHomework, Data Package Assignment, and Project"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nGrade Policy\nThe overall course grade will be based on (roughly twelve) weekly homework assignment totaling 30% of the overall course grade, a data package assignment totaling 20% of the overall course grade, and a semester project totaling 50% of the overall course grade.\n\n\nGrade Scale\nLetter grades will be assigned based on the following scale:\n\n\n\n\nTable 2: Letter grade scale.\n\n\nGrade Range\nLetter Grade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n83-86\nB\n\n\n80-82\nB-\n\n\n76-79\nC+\n\n\n72-75\nC\n\n\n69-71\nC-\n\n\n65-68\nD+\n\n\n60-64\nD\n\n\n&lt;60\nF"
  },
  {
    "objectID": "syllabus.html#weekly-schedule",
    "href": "syllabus.html#weekly-schedule",
    "title": "Syllabus",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nWeek 1: Introduction to data\nWeek 2: Basic programming for data science\nWeek 3: Data wrangling; Project component 1 due\nWeek 4: Creating data packages\nWeek 5: Data visualization\nWeek 6: Exploratory data analysis; Project component 2 due\nWeek 7: Intermediate programming for data science\nWeek 8: Introduction to probability for data science\nWeek 9: Introduction to statistics for data science; Project component 3 due\nWeek 10: Classification\nWeek 11: Regression\nWeek 12: Clustering; Project component 4 due\nWeek 13: Introduction to machine learning\nWeek 14: Ethical considerations in data science\nWeek 15: Putting everything together; Project final version due"
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important Dates",
    "text": "Important Dates\n\n\n\n\nTable 3: Important dates.\n\n\nEvent\nDate\n\n\n\n\nClasses begin\n08-28\n\n\nLast day to add classes\n90-01\n\n\nHoliday, no classes\n09-04\n\n\n100% tuition refund\n09-06\n\n\nDrop (no grade)\n09-27\n\n\nFall break\n10-07 to 10-10\n\n\nMid-semester\n10-18\n\n\nWithdraw with W\n11-10\n\n\nThanksgiving break\n11-22 to 11-26\n\n\nLast week\n12-05 to 12-11\n\n\nFinals\n12-12 to 12-16"
  },
  {
    "objectID": "syllabus.html#students-with-disabilities",
    "href": "syllabus.html#students-with-disabilities",
    "title": "Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nReasonable academic accommodations may be provided to students who submit relevant and current documentation of their disability. Students are encouraged to contact the Center for Teaching and Learning Excellence (CTLE) at disabilityservices@scranton.edu or (570) 941-4038 if they have or think they may have a disability and wish to determine eligibility for any accommodations. For more information, please visit http://www.scranton.edu/disabilities."
  },
  {
    "objectID": "syllabus.html#writing-center-services",
    "href": "syllabus.html#writing-center-services",
    "title": "Syllabus",
    "section": "Writing Center Services",
    "text": "Writing Center Services\nThe Writing Center focuses on helping students become better writers. Consultants will work one-on-one with students to discuss students’ work and provide feedback at any stage of the writing process. Scheduling appointments early in the writing progress is encouraged.\nTo meet with a writing consultant, call (570) 941-6147 to schedule an appointment, or send an email with your available meeting times, the course for which you need assistance, and your phone number to: writing-center@scranton.edu. The Writing Center does offer online appointments for our distance learning students."
  },
  {
    "objectID": "syllabus.html#academic-honesty-and-integrity",
    "href": "syllabus.html#academic-honesty-and-integrity",
    "title": "Syllabus",
    "section": "Academic Honesty and Integrity",
    "text": "Academic Honesty and Integrity\nEach student is expected to do their own work. It is also expected that each student respect and abide by the Academic Code of Honesty as set forth in the University of Scranton student handbook. Conduct that violates the Academic Code of Honesty includes plagiarism, duplicate submission of the same work, collusion, providing false information, unauthorized use of computers, theft and destruction of property, and unauthorized possession of tests and other materials. Steps taken in response to suspected violations may include a discussion with the instructor, an informal meeting with the dean of the college, and a hearing before the Academic Dishonesty Hearing Board. Students who are found to have violated the Code will ordinarily be assigned the grade F by the instructor and may face other sanctions. The complete Academic Code of Honesty is located on the University website at https://www.scranton.edu/academics/wml/acad-integ/acad-code-honesty.shtml."
  },
  {
    "objectID": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "href": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "title": "Syllabus",
    "section": "My Reporting Obligation as a Responsible Employee",
    "text": "My Reporting Obligation as a Responsible Employee\nAs a faculty member, I am deeply invested in the well-being of each student I teach. I am here to assist you with your work in this course. Additionally, if you come to me with other non-course-related concerns, I will do my best to help. It is important for you to know that all faculty members are required to report incidents of sexual harassment or sexual misconduct involving students. This means that I cannot keep information about sexual harassment, sexual assault, sexual exploitation, intimate partner violence or stalking confidential if you share that information with me. I will keep the information as private as I can but am required to bring it to the attention of the University’s Title IX Coordinator, Elizabeth M. Garcia, or Deputy Title IX Coordinator, Diana M. Collins, who, in conversation with you, will explain available support, resources, and options. I will not report anything to anybody without first letting you know and discussing choices as to how to proceed. The University’s Counseling Center (570-941-7620) is available to you as a confidential resource; counselors (in the counseling center) do not have an obligation to report to the Title IX Coordinator."
  },
  {
    "objectID": "syllabus.html#non-discrimination-statement",
    "href": "syllabus.html#non-discrimination-statement",
    "title": "Syllabus",
    "section": "Non-discrimination Statement",
    "text": "Non-discrimination Statement\nThe University is committed to providing an educational, residential, and working environment that is free from harassment and discrimination. Members of the University community, applicants for employment or admissions, guests, and visitors have the right to be free from harassment or discrimination based on race, color, religion, ancestry, gender, sex, pregnancy, sexual orientation, gender identity or expression, age, disability, genetic information, national origin, veteran status, or any other status protected by applicable law.\nStudents who believe they have been subject to harassment or discrimination based on any of the above class of characteristics, or experience sexual harassment, sexual misconduct or gender discrimination should contact Elizabeth M. Garcia, Title IX Coordinator, (570) 941-6645 elizabeth.garcia2@scranton.edu, Deputy Title IX Coordinators Diana M. Collins (570) 941-6645 diana.collins@scranton.edu, or Ms. Lauren Rivera, AVP for Student Life and Dean of Students, at (570)941-7680 lauren.rivera@scranton.edu. The United States Department of Education’s Office for Civil Rights (OCR) enforces Title IX. Information regarding OCR may be found at &lt;www.ed.gov/about/offices/list/ocr/index.html&gt;\nThe University of Scranton Sexual Harassment and Sexual Misconduct Policy can be found online at https://www.scranton.edu/diversity. All reporting options and resources are available at https://www.scranton.edu/CARE.\n\nAbout Pronouns\nIt is easy to make assumptions about an individual’s pronouns, but we try not to! Please tell us in class or via a private email if you would like to let us know what your pronouns are, if/when you would like us (and others) to use them, and certainly feel free to correct us or others if we make a mistake. Using the pronouns that a person has indicated they prefer is considered both professional and polite, and as such we ask that all members of our class use the appropriate pronouns.\nIf you have questions about this, please feel free to look up more information here (https://www.mypronouns.org/) or email jason.graham@scranton.edu with any questions."
  },
  {
    "objectID": "syllabus.html#student-mental-health-suggestions-and-resources",
    "href": "syllabus.html#student-mental-health-suggestions-and-resources",
    "title": "Syllabus",
    "section": "Student Mental Health: Suggestions and Resources",
    "text": "Student Mental Health: Suggestions and Resources\nMany students experience mental health challenges at some point in college. Struggles vary and might be related to academics, anxiety, depression, relationships, grief/loss, substance abuse, and other challenges. There are resources to help you and getting help is the smart and courageous thing to do.\n\nCounseling Center (6th Floor O’Hara Hall; 570-941-7620) – Free, confidential individual and group counseling is available on campus.\nTeletherapy – For students who wish to access therapy via video, phone, and/or chat, the University offers a teletherapy resource. Please contact the Counseling Center (570-941-7620) to inquire about teletherapy.\nMental Health Screenings – Confidential, online “check up from your neck up” to help you determine if you should connect with a mental health professional.\nDean of Students Office (201 DeNaples Center; 570-941-7680) – Private support and guidance for students navigating personal challenges that may impact success at the University"
  },
  {
    "objectID": "syllabus.html#final-note",
    "href": "syllabus.html#final-note",
    "title": "Syllabus",
    "section": "Final Note",
    "text": "Final Note\nThe instructor reserve the right to modify this syllabus; students will immediately be notified of any such changes and an updated syllabus will be made available to the class via the course learning management system."
  },
  {
    "objectID": "lesson01/index.html#what-is-data-science",
    "href": "lesson01/index.html#what-is-data-science",
    "title": "Lesson 1",
    "section": "What is Data Science?",
    "text": "What is Data Science?\nFor this course, we think of data science as a methodology or set of methodologies for gaining insight from data. Data can be pretty much any type of organized information. In this course, data will often be organized in a tabular format such as is common with spreadsheets. However, text, images, videos, and sound are other types of data that data scientists often work with. Further, data may have a temporal or spatial component.\nIt is important to note that data science is an interdisciplinary endeavor. Data science brings together mathematics and statistics, computer science and machine learning, and a domain of application or expertise. This is illustrated by Figure 1.\n\n\n\nFigure 1: Data science is an inherently interdisciplinary field.\n\n\n\nData Science Workflow\nIn this course, we emphasize a certain workflow for typical data science tasks. Generally, the steps in the workflow are:\n\nGather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application.\nDocument the data and data collection process.\n\nIt is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See The Alignment Problem by Brian Christian for a thoughtful discussion on the issues related to these types of considerations (Christian 2020).\n\nImport the data for analysis.\nExplore and clean the data. Data visualization is essential at this step.\nGenerate initial insight or more detailed questions.\nDecide what type(s) of analysis or analyses are to be performed.\n\nMake sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See Table 1.1 from (Timbers, Campbell, and Lee 2022) for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table here.\n\nAssess the analysis.\nAt this stage, it may be necessary to repeat steps 1 - 7.\nReport your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis.\n\nIt is essential the our data science workflow be reproducible and auditable. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility). Figure 2 illustrates the concept of reproducibility.\n\n\n\nFigure 2: Artwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#exploring-data-an-introduction-through-gapminder",
    "href": "lesson01/index.html#exploring-data-an-introduction-through-gapminder",
    "title": "Lesson 1",
    "section": "Exploring Data: An Introduction Through Gapminder",
    "text": "Exploring Data: An Introduction Through Gapminder\nLet’s begin our adventure in data science by engaging with some of the interactive tools on the Gapminder website, View Gapminder. The Gapminder organization is a non-profit entity that seeks to use data to educate people about large-scale social issues. Gapminder has a particular focus on addressing certain types of common misconceptions about the world.\nView the interactive data tools by Gapminder.\nQuestion: What information is conveyed by the visualization shown at this link? How are colors used in the visualization?\nQuestion What is the relationship between income and life expectancy based on the initial bubble plot? How do income and life expectancy each change over time? Does the relationship between income and life expectancy change over time?\nQuestion: Do you think the visualizations on the Gapminder sight are useful and effective? If so, what makes them useful or effective. If not, what changes would you make to make them more useful or effective?"
  },
  {
    "objectID": "lesson01/index.html#classification-of-basic-data-types",
    "href": "lesson01/index.html#classification-of-basic-data-types",
    "title": "Lesson 1",
    "section": "Classification of Basic Data Types",
    "text": "Classification of Basic Data Types\nA small portion of the data available from Gapminder is shown in Table 1 below: 2\n\n\n\n\nTable 1: Portion of the Gapminder data set\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\n\nThere are several useful things to note about this data:\n\nEach row corresponds to a unique observation.\nEach column is a unique feature or variable.\nEvery cell contains a single value.\nSome of the information in the data set is quantitative while other information is qualitative.\n\nA data set that satisfies properties 1 - 3 is said to be tidy or in tidy format. We will often find it very useful to work with data that conforms to the tidy format.\nWe refer to variables made up of quantitative information as numerical and variables made up of qualitative information as categorical. For example, in the Gapminder data, the country and continent variables are categorical while the year, life expectancy, population, and per capita GDP are all numerical variables. The type of variable(s) often influences the type of visualization or analysis that is used to explore and model data.\nWe can further classify the type of a variable by the following classifications:\n\nNumerical (quantitative):\n\nContinuous: Can take on any value (possibly over a specific range) and any value between two values. Typically will involve decimal values. Physical measurements are common types of continuous numerical data.\nDiscrete: Any numerical type that is not continuous. Often whole numbers. Counts are a common type of discrete numerical data.\n\nCategorical (qualitative):\n\nNominal: Lacks any particular ordering. For example, eyecolor would be nominal categorical data.\nOrdinal: Possesses a particular ordering. For example, the place (first, second, third, etc.) that a sports team comes in at the end of a competition or tournament.\nBinary: A categorical variable with only two possible outcomes. Think yes/no, true/false, etc.\n\n\nFigure 3 illustrates numerical variables and the distinction between continuous and discrete numerical variables, while Figure 4 illustrates categorical variables and the distinction between nominal, ordinal, and binary categorical variables.\n\n\n\nFigure 3: Numerical variables and the distinction between a continuous numerical variable and a discrete numerical variable. Artwork by Allison Horst\n\n\n\n\n\nFigure 4: Categorical variables and the distinction between nominal, ordinal, and binary categorical variables. Artwork by Allison Horst\n\n\nOne thing to be careful about is that numbers can and often are used to label the outcomes for a categorical variable. For example, suppose we collect data about students living in a particular dormitory. We may use 1 to denote that a student is enrolled as a first-year student, 2 to denote that a student is enrolled as a second-year student, etc. Even though we’ve used number, the year a student is enrolled as is really a categorical variable.\nSometimes, there may even be an ambiguity in the type for a variable. For example, suppose we are collecting data on commercial buildings in a particular city and for each building in the data set, we record (among other things) the number of stories of the building using 1 to denote a building with one story, 2 to denote a building with two stories, etc. On the one hand, we could view our stories variable as a count and hence a discrete numerical variable. On the other hand, it might be better to think of the stories variable as a ordinal categorical variable. One way to decide which is more appropriate is to think about if it makes sense or is desirable to take a average or not. If you want to take a average, then the variable must be numerical."
  },
  {
    "objectID": "lesson01/index.html#a-tiny-bit-of-r",
    "href": "lesson01/index.html#a-tiny-bit-of-r",
    "title": "Lesson 1",
    "section": "A Tiny Bit of R",
    "text": "A Tiny Bit of R\n\n\n\nArtwork by Allison Horst\n\n\n\nWhy R?\nThe R language (R Core Team 2023) for statistical computing is one of the most popular computing tools for data science, among the other most popular are Python and Julia. Some of the strengths of R include\n\nfree and open source which facilitates reproducibility and auditability,\necosystem of packages that greatly extend the functionality of R,\nrmarkdown (Xie, Dervieux, and Riederer 2020) and Quarto frameworks for literate programming enable presentation and communication of data analyses and facilitate reproducibility and auditability,\nRStudio integrated development environment (IDE) by Posit enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages,\na strong and collaborative user community, see R Community Explorer website here.\n\nAs the course progresses, we will learn a lot of R. For this lesson, we will start with something very simple, that is, doing basic calculations with R.\n\n\nCalculations with R\nIn R, it is easy to perform basic arithmetic operations such as addition, subtraction, multiplication, division, exponentiation, and modular division. Each of these is illustrated in the following code blocks:\n\n# any line starting with # is a comment and is not run\n# addition\n1.6 + 2.31\n\n[1] 3.91\n\n\n\n# subtraction\n4.87 - 2.31\n\n[1] 2.56\n\n\n\n# multiplication\n1.6 * 2.31\n\n[1] 3.696\n\n\n\n# division\n6.0 / 2.31\n\n[1] 2.597403\n\n\n\n# exponentiation\n2^3\n\n[1] 8\n\n\n\n# modular division, a %% b returns the remainder after\n# dividing a by b\n\n7 %% 2\n\n[1] 1\n\n\nYou have to be careful with the order of operations, use parentheses to be precise about intended order of operations. For example,\n\n(6 / 3) - 1\n\n[1] 1\n\n\ndivides 6 by three and then subtracts 1, while\n\n6 / (3 - 1)\n\n[1] 3\n\n\nsubtracts 1 from 3 and divides 6 by the result. While the following code runs and has a specific meaning to the computer, it is ambiguous to the human reader and should be modified appropriately with parentheses:\n\n6 / 3 - 1\n\n[1] 1\n\n\n\n\nVariables and Assignment\nIn R programming, a variable is a name given to a R object such as a numeric value that can be used to store, reuse, or modify that object. In R, variable assignments are made use the assignment operator &lt;-. For example,\n\nmy_variable &lt;- 2.5\n\nassigns the numeric value 2.5 to the variable my_variable. Now, we can peform operations to my_variable, for example,\n\n(2 * my_variable)\n\n[1] 5\n\n(my_variable^3)\n\n[1] 15.625\n\n\nMany programming languages use = for assignment. While it is valid to use = in R, best practices dictate using &lt;-.\nSee the tidyverse style guide here for best practices in naming objects in R.\nIf you want to engage further with R programming, we highly recommend the swirl package (Kross et al. 2020) for learning R in R. To install, load, and use swirl, run the following commands in the R console:\n\n# install swirl\ninstall.packages(\"swirl\") # only need to run once\n# load swirl \nlibrary(swirl) # must run in every new R session\n# start swirl\nswirl()"
  },
  {
    "objectID": "lesson01/index.html#readings-etc.-1",
    "href": "lesson01/index.html#readings-etc.-1",
    "title": "Lesson 1",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead the preface of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). This textbook is available online here.\nThroughout the semester, we will use example data from the Tidy Tuesday data repository (Mock 2018). Click link here to go to the Tidy Tuesday data repository."
  },
  {
    "objectID": "lesson01/index.html#preparation-for-the-next-lesson",
    "href": "lesson01/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 1",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nPrior to starting the next lesson, please read Chapter 1 of (Timbers, Campbell, and Lee 2022), link here. It is also recommended that you read the section on reproducible workflows from (Alexander, n.d.), link here"
  },
  {
    "objectID": "lesson01/index.html#a-tiny-bit-of",
    "href": "lesson01/index.html#a-tiny-bit-of",
    "title": "Lesson 1",
    "section": "A Tiny Bit of  ",
    "text": "A Tiny Bit of  \n\n\n\nArtwork by Allison Horst\n\n\n\nWhy R?\nThe R language (R Core Team 2023) for statistical computing is one of the most popular computing tools for data science, among the other most popular are Python and Julia. Some of the strengths of R include\n\nfree and open source which facilitates reproducibility and auditability,\necosystem of packages that greatly extend the functionality of R,\nrmarkdown (Xie, Dervieux, and Riederer 2020) and Quarto frameworks for literate programming enable presentation and communication of data analyses and facilitate reproducibility and auditability,\nRStudio integrated development environment (IDE) by Posit enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages,\na strong and collaborative user community, see R Community Explorer website, view the website..\n\nAs the course progresses, we will learn a lot of R. For this lesson, we will start with something very simple, that is, doing basic calculations with R.\n\n\nCalculations with R\nIn R, it is easy to perform basic arithmetic operations such as addition, subtraction, multiplication, division, exponentiation, and modular division. Each of these is illustrated in the following code blocks:\n\n# any line starting with # is a comment and is not run\n# addition\n1.6 + 2.31\n\n[1] 3.91\n\n\n\n# subtraction\n4.87 - 2.31\n\n[1] 2.56\n\n\n\n# multiplication\n1.6 * 2.31\n\n[1] 3.696\n\n\n\n# division\n6.0 / 2.31\n\n[1] 2.597403\n\n\n\n# exponentiation\n2^3\n\n[1] 8\n\n\n\n# modular division, a %% b returns the remainder after\n# dividing a by b\n\n7 %% 2\n\n[1] 1\n\n\nYou have to be careful with the order of operations, use parentheses to be precise about intended order of operations. For example,\n\n(6 / 3) - 1\n\n[1] 1\n\n\ndivides 6 by three and then subtracts 1, while\n\n6 / (3 - 1)\n\n[1] 3\n\n\nsubtracts 1 from 3 and divides 6 by the result. While the following code runs and has a specific meaning to the computer, it is ambiguous to the human reader and should be modified appropriately with parentheses:\n\n6 / 3 - 1\n\n[1] 1\n\n\n\n\nVariables and Assignment\nIn R programming, a variable is a name given to a R object such as a numeric value that can be used to store, reuse, or modify that object. In R, variable assignments are made use the assignment operator &lt;-. For example,\n\nmy_variable &lt;- 2.5\n\nassigns the numeric value 2.5 to the variable my_variable. Now, we can peform operations to my_variable, for example,\n\n(2 * my_variable)\n\n[1] 5\n\n(my_variable^3)\n\n[1] 15.625\n\n\nMany programming languages use = for assignment. While it is valid to use = in R, best practices dictate using &lt;-.\nSee the tidyverse style guide here for best practices in naming objects in R.\nIf you want to engage further with R programming, we highly recommend the swirl package (Kross et al. 2020) for learning R in R. To install, load, and use swirl, run the following commands in the R console:\n\n# install swirl\ninstall.packages(\"swirl\") # only need to run once\n# load swirl \nlibrary(swirl) # must run in every new R session\n# start swirl\nswirl()"
  },
  {
    "objectID": "syllabus.html#use-of-ai",
    "href": "syllabus.html#use-of-ai",
    "title": "Syllabus",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial intelligence (AI) can be an effective tool in data science. For example, AI-based programming assistants like GitHub Copilot or generative model platforms like ChatGPT now help programmers and developers to write better code in less time. Learning to use AI is essentially becoming a basic skill for the modern data scientist. Because of this, I do not want to completely discourage the use of AI assistance.\nHowever, I ask that you avoid using AI platforms or tools in a manner that is inappropriate in the context of this course. This course teaches a variety of concepts, skills, and critical thinking. Using AI in such a way as to avoid learning, developing skills, or critical thinking is not appropriate. If you find yourself using AI to look up answers, search for complete solutions to problems, or things like this, then your use of AI is not acceptable. It might be helpful to think of AI as an analog to a calculator. If the goal of an assignment is for you to demonstrate that you can do a certain calculation, then using a calculator is not appropriate. On the other hand, if the goal of an assignment is for you to demonstrate that you can solve a problem for which a minor step involves doing a calculation, then using a calculator is okay. AI should be treated analogously.\nIn particular, it is expected that students will be able to explain independently and in detail what any line of code submitted as part of an assignment this semester does. Also, it is expected that students can explain independently and in detail the solution to any problem submitted as part of an assignment this semester.\nIf you have any doubts about your use of AI, then either ask the instructor if your use of AI is acceptable or just don’t use AI."
  },
  {
    "objectID": "lesson01/index.html#course-overview",
    "href": "lesson01/index.html#course-overview",
    "title": "Lesson 1",
    "section": "Course Overview",
    "text": "Course Overview\nThis course provides an introduction to data science. Broadly, we will cover concepts, skills, and methods for gaining insight from data. The things you learn in the class will be applicable in a variety of different areas, professions, and even other courses.\nThere is a website for the course, view the course website. For course logistics, see the official course syllabus, view the syllabus website. Assignments and other information specific to the course in a given semester will be posted on the course learning management system (LMS). The course website provides links to many additional resources, view the links page.\nWhile we will regularly refer to several texts (most of which have been published online as open access materials) throughout the course, most of the content will be delivered via “notebooks” like the one you’re reading now1 that intermix text, mathematical notation, figures, media, programming language code, and web links. In some cases, you will be asked to go through the notebooks on your own and sometimes we will go through the notebooks together during class. Either way, any time you encounter code in a notebook, it is expected that you will take the time to run any code (mostly by copying and pasting) for yourself. The only way to master the material is through active participation."
  },
  {
    "objectID": "lesson01/index.html#footnotes",
    "href": "lesson01/index.html#footnotes",
    "title": "Lesson 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe notebooks are created using Quarto and R markdown, topics that we will cover in more detail later.↩︎\nThis data is made available in R through the gapminder package. In order to access this data in R, is it necessary to install and load the gapminder package. We will discuss installing and loading R packages in detail soon.↩︎"
  },
  {
    "objectID": "lesson01/index.html#some-data-resources",
    "href": "lesson01/index.html#some-data-resources",
    "title": "Lesson 1",
    "section": "Some Data Resources",
    "text": "Some Data Resources\nFor future reference, we list some resources that might be useful for finding data sets to work with throughout the course.\n\nGapminder and the gapminder R package.\nUCI machine learning repository\nKaggle\nTidy Tuesday data repository\nAppendix B of Telling Stories with Data (Alexander, n.d.) lists a large number of data resources"
  },
  {
    "objectID": "syllabus.html#further-reading",
    "href": "syllabus.html#further-reading",
    "title": "Syllabus",
    "section": "Further Reading",
    "text": "Further Reading\n\nAn Introduction to Statistical Learning 2nd Ed. by James, Witten, Hastie, and Tibshirani, view the free online version of the text.\nHands-On Machine Learning with R, view the free online version of the text.\nLinks to additional resources related to the course material will be posted on the course website. View the resources link."
  },
  {
    "objectID": "syllabus.html#homework-assignments",
    "href": "syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\nThere will be roughly 12 weekly homework assignments throughout the semester. These assignments with due dates will be posted to the course learning management system. Homework problems will be a mix of hand-written and computer assignments and the problems will relate to the material covered in lectures and readings."
  },
  {
    "objectID": "syllabus.html#data-package-assignment",
    "href": "syllabus.html#data-package-assignment",
    "title": "Syllabus",
    "section": "Data Package Assignment",
    "text": "Data Package Assignment\nThe data package assignment, making up 20% of the course grade asks students to curate a data set according the best practices for reproducible data analyses. A complete data package assignment should consist of\n\nAn appropriate raw data set(s) with original source fully documented.\nAn appropriately cleaned and processed version of the original raw data set. All code or other files used to clean and process the raw data must be included as part of the final data package submission.\nAn appropriate data sheet to accompany the data package.\nA public folder, repository, R package, or other container that can be used to make your data set and associated files and documentation available to others.\n\nFurther details on the semester project such as the parameters of the assignment and a grade rubric will be posted on the course learning management system."
  },
  {
    "objectID": "syllabus.html#semester-project",
    "href": "syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\nThe semester project will incorporate all the components of a data analysis covered in the course throughout the semester applied to a data set of your choosing pending approval by the instructor. Various components of the project will be due at different times but you will have the opportunity to revise some components prior to the submission of the final product.\nA complete project, counting for 50% of the overall course grade will consist of the following:\n\nA data sheet describing the essential information about your chosen data set.\nAn initial exploratory data analysis for your chosen data set.\nAn appropriate analysis of your chosen data set with the goal to address a specific research question.\nA project report developed using Quarto. You may view the rendered version of an example report here.\nSlides for a presentation summarizing your project. You will not actually present the slides. A rendered version of examples slides may be viewed here.\nA GitHub repository containing all code (appropriately documented) written and used in your project. An example project repository may be viewed here.\n\nYour project report and presentation should be written as if it is addressed to a stake holder with some subject matter knowledge in the domain of application but not necessarily with a quantitative or programming background. Further details on the semester project such as the parameters of the assignment and a grade rubric will be posted on the course learning management system."
  },
  {
    "objectID": "lesson02/index.html",
    "href": "lesson02/index.html",
    "title": "Lesson 2",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nSet up an RStudio project.\nWork in a Quarto document.\nRead tabular data into R.\nUse the glimpse command and other R functions to examine data read into R.\nDescribe what version control is and why data analysis projects can benefit from it."
  },
  {
    "objectID": "lesson02/index.html#learning-objectives",
    "href": "lesson02/index.html#learning-objectives",
    "title": "Lesson 2",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nSet up an RStudio project.\nWork in a Quarto document.\nRead tabular data into R.\nUse the glimpse command and other R functions to examine data read into R.\nDescribe what version control is and why data analysis projects can benefit from it."
  },
  {
    "objectID": "lesson02/index.html#readings-etc.",
    "href": "lesson02/index.html#readings-etc.",
    "title": "Lesson 2",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapter 2 of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). View the free online version of the text..\nThe TidyTuesday data repository (Mock 2018)."
  },
  {
    "objectID": "lesson02/index.html#rstudio-projects",
    "href": "lesson02/index.html#rstudio-projects",
    "title": "Lesson 2",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nA RStudio project is simply a folder or directory on a computer that contains a .Rproj file. A project makes it so that your work has its own working directory, workspace, history, and source documents. Using a project facilitates reproducible and auditable analyses because it keeps all the relevant files for a specific analysis together in one place making it easier to share. A RStudio project should correspond to a single project that you are working on. For example, you should use a RStudio project for your semester project for the course.\nThe instructions for how to create a project are given in the online Posit support documents, view the projects webpage. Alternatively, view this video:\n\n\n\n\n\n\n\n\n\n\nSome Project Tips and Tricks\n\nWhen you create a project, the name of your project and the name of the corresponding folder will be the same so, use good naming conventions for projects. In particular, avoid the use of spaces or symbols. Make sure the name is descriptive and easy to remember but not too long.\nIf there is data for your project, it is useful to create a subfolder of your package folder called data where you should save all the data files relevant to your project.\nIf your project involves a lot of coding in R, its a good idea to create a subfolder of your package folder called R where you should save all the R script files relevant to your project."
  },
  {
    "objectID": "lesson02/index.html#quarto-documents",
    "href": "lesson02/index.html#quarto-documents",
    "title": "Lesson 2",
    "section": "Quarto Documents",
    "text": "Quarto Documents\nLiterate programming is a programming practice in which the programmer includes computer code, normal text, figures, mathematical notation, etc. together into a single document. The approach is routinely in data science for the purposes of reproducible research. There are several tools for literate programming that are very popular among data scientists. These include but are not limited to Jupyter notebooks and rmarkdown.\nA very recent addition to the literate programming toolkit is Quarto. Quarto brings together Jupyter notebooks, rmarkdown, and a number of other features. Throughout this course we will use Quarto which is included with the most recent versions of RStudio. To start using Quarto, we recommend:\n\nGetting started with Quarto YouTube video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\nIn class, we will go through the steps of creating a RStudio project and a Quarto document together."
  },
  {
    "objectID": "lesson02/index.html#reading-data-into-r",
    "href": "lesson02/index.html#reading-data-into-r",
    "title": "Lesson 2",
    "section": "Reading Data into R",
    "text": "Reading Data into R\n\nBasic Concepts\nThe main focus of this lesson is to show different common ways to read tabular data into R. Recall that tabular data is data that corresponds to a spreadsheet in which data is arranged into columns and rows. There are three basic things you need to know to read data into R:\n\nThe data format and file type.\n\nFor data to be tabular in structure, one must use some method to distinguish entries that belong to different columns and rows. The two most common ways are to separate entries using either a comma or a tab. Tabular data formatted using comma-separated values are saved as .csv files while tabular data formatted using tab-separated values are save as .tsv files. The other very common way to format and save tabular data is using Excel spreadsheets.\n\nWhere the data is saved and will be read in from.\n\nThere are two basic locations in which one may save and store a data file: locally on a computer, remotely. To read in data that is stored locally on a computer that you have direct access to, you just need to know the path to the data file that you want to read in. To read in data that is stored remotely, you need to know the url address for the remote location of the data.\n\nThe appropriate R function to use depending on the 1. and 2.\n\nThe readr package included as part of the tidyverse family of R packages contains functions read_csv and read_tsv for reading in .csv or .tsv files, respectively. The readxl package contains functions that can be used to read in tabular data formatted and saved as Excel spreadsheets.\nNote: There are also many other data structures, file types, and R packages or functions for reading in data. We can’t possibly cover all the possible variations in class but once you know how to read in one data type into R, it is usually straightforward to figure out how to do it with some other data or file type.\n\n\nExamples\nThe following examples assume that you are working in an active RStudio project that contains a subfolder titled data and that you have downloaded and saved the files happiness_report.csv and happiness_report.xlsx to the data folder. These are all data files for tabular data that ranks countries on happiness based on rationalized factors like economic growth, social support, etc. The data was released by the United Nations at an event celebrating International Day of Happiness.\nBefore we can read these data files into R, we need to make sure that we install and load readr and readxl. Remember that readr is part of the tidyverse which we have already installed so you probably only need to install readxl. Once you have done this, you can create and run an R code chunk with the following:\n\n# load necessary packages\nlibrary(tidyverse)\nlibrary(readxl)\n\nNow, let’s read in the data saved in the files happiness_report.csv and happiness_report.xlsx.\n\n# read in data files\nhappiness_report_csv &lt;- read_csv(\"data/happiness_report.csv\")\nhappiness_report_xl &lt;- read_xlsx(\"data/happiness_report.xlsx\")\n\nNotice that we have assigned our data to a variable after having read it in.\nIt is important to confirm that we have read in what we actually wanted. There are a number of ways to confirm that the data read into R makes sense:\n\nThe dim command will tell us the number of rows and columns for the data set we read in.\nThe glimpse command from the dplyr package (included with tidyverse) will show a glimpse of what is in the data. Specifically, glimpse displays a transposed version of you tabular data along with the number of rows and columns. This command even tells you valuable information about what type of data you have in each column. This is an extremely useful function and it is recommended that you use it immediately after reading in data.\nThe head (tail) commands will display the first (last) few rows of a data set.\nThe View command will bring up a full view of you data in a new tab in R studio. This is basically like pulling up an Excel spreadsheet in RStudio. You probably want to avoid using this function on anything other than small or maybe moderately sized data sets.\nThe summary function in base R will provide a descriptive summary of each column in a data set.\nThe skim function from the skimr package quickly provides a broad overview of a data set. It’s basically a souped-up version of summary.\n\nHere are the results of each of these commands (except View) run on the data read in from the .csv file:\n\ndim(happiness_report_csv)\n\n[1] 155   5\n\n\n\nglimpse(happiness_report_csv)\n\nRows: 155\nColumns: 5\n$ country         &lt;chr&gt; \"Norway\", \"Denmark\", \"Iceland\", \"Switzerland\", \"Finlan…\n$ happiness_score &lt;dbl&gt; 7.537, 7.522, 7.504, 7.494, 7.469, 7.377, 7.316, 7.314…\n$ GDP_per_capita  &lt;dbl&gt; 1.616463, 1.482383, 1.480633, 1.564980, 1.443572, 1.50…\n$ life_expectancy &lt;dbl&gt; 0.7966665, 0.7925655, 0.8335521, 0.8581313, 0.8091577,…\n$ freedom         &lt;dbl&gt; 0.6354226, 0.6260067, 0.6271626, 0.6200706, 0.6179509,…\n\n\n\nhead(happiness_report_csv)\n\n# A tibble: 6 × 5\n  country     happiness_score GDP_per_capita life_expectancy freedom\n  &lt;chr&gt;                 &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Norway                 7.54           1.62           0.797   0.635\n2 Denmark                7.52           1.48           0.793   0.626\n3 Iceland                7.50           1.48           0.834   0.627\n4 Switzerland            7.49           1.56           0.858   0.620\n5 Finland                7.47           1.44           0.809   0.618\n6 Netherlands            7.38           1.50           0.811   0.585\n\n\n\ntail(happiness_report_csv)\n\n# A tibble: 6 × 5\n  country                 happiness_score GDP_per_capita life_expectancy freedom\n  &lt;chr&gt;                             &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Togo                               3.49         0.305           0.247   0.380 \n2 Rwanda                             3.47         0.369           0.326   0.582 \n3 Syria                              3.46         0.777           0.501   0.0815\n4 Tanzania                           3.35         0.511           0.365   0.390 \n5 Burundi                            2.90         0.0916          0.152   0.0599\n6 Central African Republ…            2.69         0               0.0188  0.271 \n\n\n\nView(happiness_report_csv)\n\n\nsummary(happiness_report_csv)\n\n   country          happiness_score GDP_per_capita   life_expectancy \n Length:155         Min.   :2.693   Min.   :0.0000   Min.   :0.0000  \n Class :character   1st Qu.:4.505   1st Qu.:0.6634   1st Qu.:0.3699  \n Mode  :character   Median :5.279   Median :1.0646   Median :0.6060  \n                    Mean   :5.354   Mean   :0.9847   Mean   :0.5513  \n                    3rd Qu.:6.101   3rd Qu.:1.3180   3rd Qu.:0.7230  \n                    Max.   :7.537   Max.   :1.8708   Max.   :0.9495  \n    freedom      \n Min.   :0.0000  \n 1st Qu.:0.3037  \n Median :0.4375  \n Mean   :0.4088  \n 3rd Qu.:0.5166  \n Max.   :0.6582  \n\n\n\nskimr::skim(happiness_report_csv)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nhappiness_report_csv\n\n\n\n\nNumber of rows\n\n\n155\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n1\n\n\n\n\nnumeric\n\n\n4\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\ncountry\n\n\n0\n\n\n1\n\n\n4\n\n\n24\n\n\n0\n\n\n155\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nhappiness_score\n\n\n0\n\n\n1\n\n\n5.35\n\n\n1.13\n\n\n2.69\n\n\n4.51\n\n\n5.28\n\n\n6.10\n\n\n7.54\n\n\n▂▆▇▇▅\n\n\n\n\nGDP_per_capita\n\n\n0\n\n\n1\n\n\n0.98\n\n\n0.42\n\n\n0.00\n\n\n0.66\n\n\n1.06\n\n\n1.32\n\n\n1.87\n\n\n▂▅▇▇▂\n\n\n\n\nlife_expectancy\n\n\n0\n\n\n1\n\n\n0.55\n\n\n0.24\n\n\n0.00\n\n\n0.37\n\n\n0.61\n\n\n0.72\n\n\n0.95\n\n\n▂▃▃▇▅\n\n\n\n\nfreedom\n\n\n0\n\n\n1\n\n\n0.41\n\n\n0.15\n\n\n0.00\n\n\n0.30\n\n\n0.44\n\n\n0.52\n\n\n0.66\n\n\n▁▃▅▇▅\n\n\n\n\n\n\n\nQuestion: What useful information is provided by the output from each of these commands?\nExercise: Run all these commands on the data that you read in from the Excel spreadsheet.\nWe end this section by noting that the data file happiness_report.csv is also stored in a remote repository, view the repository. Since this repository has a corresponding url address, we can read the data in directly from the web:\n\nhappiness_report_csv_url &lt;- read_csv(\"https://raw.githubusercontent.com/UBC-DSCI/data-science-a-first-intro-worksheets/main/worksheet_reading/data/happiness_report.csv\")\n\nWarning: There is one thing you have to be careful about when reading in remote data. You need to make sure you are reading in from the link corresponding to the raw data file and not the one that has been rendered for visual display.\nExercise: Use one or more of the commands we have covered to make sure that this has read in the data you want."
  },
  {
    "objectID": "lesson02/index.html#preparation-for-the-next-lesson",
    "href": "lesson02/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 2",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson or two, we will look at some things that one commonly does with data very soon after reading it into R. That is,\n\nManipulate or wrangle the data.\nConduct an exploratory data analysis (EDA).\n\nPrior to starting the next lesson, please read section 2.8 and Chapter 3 of (Timbers, Campbell, and Lee 2022), link here."
  },
  {
    "objectID": "lesson02/index.html#references",
    "href": "lesson02/index.html#references",
    "title": "Lesson 2",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. n.d. “Telling Stories with Data: With Applications in r.”\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\".\n\n\nMock, J. Thomas. 2018. “Tidy Tuesday.” GitHub Repository. https://github.com/rfordatascience/tidytuesday; GitHub.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-08\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder   * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra  * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n readxl      * 1.4.3   2023-07-06 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson02/index.html#vectors-lists-and-data-frames",
    "href": "lesson02/index.html#vectors-lists-and-data-frames",
    "title": "Lesson 2",
    "section": "  Vectors, Lists, and Data Frames",
    "text": "Vectors, Lists, and Data Frames\nWhen tabular data is read into R from a file such as a .csv or an Excel spreadsheet it is represented in R by a particular data type. Specifically, tabular data in R is represented by a data frame or tibble. While we don’t want to spend too much time on the technicalities of R objects, it is useful to have a brief introduction to the vector, list, and data frame objects in R. If you want to see more details then we present here, see Chapter 5 of (Grolemund 2014), which is freely available online. View Chapter 5.\n\nVectors in R\nIn our last class, we were exposed to vectors in R through the first lesson in the R Programming swirl available through the swirl package. The most important thing to remember about vectors in R is\n\nHow to create them.\nThat a vector can only hold values of the same type.\n\nFor example, we can create a vector with three numeric values and store it as a variable my_num_vect using\n\n(my_num_vect &lt;- c(1.2,6.8,-13.9))\n\n[1]   1.2   6.8 -13.9\n\n\nWe can create a vector with three character values and store it as a variable my_chr_vect using\n\n(my_chr_vect &lt;- c(\"dog\",\"cat\",\"bird\"))\n\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\nbut we cannot create a vector that contains two numbers and one character. That is, we can’t mix types in a vector. Let’s see what happens if we try it:\n\n(my_mix_vect &lt;- c(3.14,\"cat\",\"bird\"))\n\n[1] \"3.14\" \"cat\"  \"bird\"\n\n\nExercise: Examine the output from the last command and explain what happened.\nNote that you can determine the type for elements of a vector using the typeof command. For example,\n\ntypeof(my_num_vect)\n\n[1] \"double\"\n\n\nWhile vectors are obviously useful for storing certain types of data, they are limited for most purposes because data sets like the tabular data sets we’ve loaded into R are made up of a mixture of different types of values.\n\n\nLists in R\nLists are another type of R object that do allow us to mix data types. For example\n\n(my_mix_list &lt;- list(3.14,\"cat\",\"bird\"))\n\n[[1]]\n[1] 3.14\n\n[[2]]\n[1] \"cat\"\n\n[[3]]\n[1] \"bird\"\n\n\nThere is no type conversion unlike what we saw happen when we tried to create a vector of mixed types. Let’s see what happens if we request the type for the elements in this list.\n\ntypeof(my_mix_list)\n\n[1] \"list\"\n\n\nThis just tells us that we have a list which we already knew. To determine the type of each list element, we can use a command such as\n\nmap(my_mix_list,typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"character\"\n\n[[3]]\n[1] \"character\"\n\n\nDon’t worry if you don’t understand the last command. Later we will look at map type functions which belong to the package purrr.\nLists are extremely useful and many functions such as the function map we just used actually return a list. Two things we can do with lists are will be very useful for us in this course are:\n\nGive names to the elements of a list.\nStore vectors (and even more complicated objects) as the elements in a list.\n\nFor example:\n\n(my_vector_list &lt;- list(my_num_vect = c(1.2,6.8,-13.9),\n                       my_chr_vect = c(\"dog\",\"cat\",\"bird\")))\n\n$my_num_vect\n[1]   1.2   6.8 -13.9\n\n$my_chr_vect\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\nThen we can access the elements in the list my_vector_list using the accessor operator denoted by $. For example\n\n(my_vector_list$my_num_vect)\n\n[1]   1.2   6.8 -13.9\n\n(my_vector_list$my_chr_vect)\n\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\n\n\nData Frames in R\nA data frame in R is nothing other than a list where each element of the list is a vector such that all the vectors in the list have the same length. We can create a date frame is much the same way that we just created the list that contains two vectors:\n\n(my_df &lt;- data.frame(col_a = c(1.2,6.8,-13.9),\n                       col_b = c(\"dog\",\"cat\",\"bird\")))\n\n  col_a col_b\n1   1.2   dog\n2   6.8   cat\n3 -13.9  bird\n\n\nNotice that my_df contains exactly the same information that my_vector_list does. The only real difference is the way in which the results are displayed.\nThe point of all this is just to explain that we functions like read_csv are used to read in tabular data, R stores the result as a data frame. Actually, since read_csv belongs to the tidyverse it reads in the data as something called a tibble. For our purposes, we can think of data frames and tibbles as being more or less the same things. If you want to know more about tibbles, see section 3.6 from (Wickham 2019), view the section."
  },
  {
    "objectID": "lesson02/index.html#some-data-sources",
    "href": "lesson02/index.html#some-data-sources",
    "title": "Lesson 2",
    "section": "Some Data Sources",
    "text": "Some Data Sources\nOne thing students often struggle with is finding and picking a good data set for their projects. Appendix B of the online textbook contains a very helpful list of data sources (Alexander, n.d.). View appendix B. Two other very interesting and useful sources of data are the Tidy Tuesday data repositories and Kaggle. There are also many R packages that either include data or that can be used to download data. The ROpenSci project is a good resource for finding R packages that can be used to obtain data, view the project."
  },
  {
    "objectID": "lesson02/index.html#version-control-and-remote-repositories",
    "href": "lesson02/index.html#version-control-and-remote-repositories",
    "title": "Lesson 2",
    "section": "Version Control and Remote Repositories",
    "text": "Version Control and Remote Repositories\nVersion control systems track changes to a project over its lifespan, allow sharing and editing of code across a collaborative team, and make it easier to distribute the finished project to its intended audience. Chapter 12 of (Timbers, Campbell, and Lee 2022) covers version control in detail. Let’s examine this chapter."
  },
  {
    "objectID": "lesson03/index.html",
    "href": "lesson03/index.html",
    "title": "Lesson 3",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nUse the dplyr functions select and filter to manipulate rows and columns of a data frame.\nUse the mutate function to add new columns to a data frame.\nUse group_by to group observations in one or more columns of a data frame by a grouping variable.\nUse the summarize function with the group_by function to compute grouped summaries.\nUse pivot_longer and pivot_wider functions to perform certain types of reorganizations of a data frame.\nUse the join family of functions to combine separate data sets into one."
  },
  {
    "objectID": "lesson03/index.html#learning-objectives",
    "href": "lesson03/index.html#learning-objectives",
    "title": "Lesson 3",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nUse the dplyr functions select and filter to manipulate rows and columns of a data frame.\nUse the mutate function to add new columns to a data frame.\nUse group_by to group observations in one or more columns of a data frame by a grouping variable.\nUse the summarize function with the group_by function to compute grouped summaries.\nUse pivot_longer and pivot_wider functions to perform certain types of reorganizations of a data frame.\nUse the join family of functions to combine separate data sets into one."
  },
  {
    "objectID": "lesson03/index.html#readings-etc.",
    "href": "lesson03/index.html#readings-etc.",
    "title": "Lesson 3",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapter 3 of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). View the free online version of the text..\nChapter 5 of R for Data Science by Wickham and Grolemund (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View the chapter online"
  },
  {
    "objectID": "lesson03/index.html#preparation-for-the-next-lesson",
    "href": "lesson03/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 3",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson we will begin discussing the exploratory data analysis (EDA) process starting with some data visualization.\nPrior to starting the next lesson, please read Chapter 4 of (Timbers, Campbell, and Lee 2022), link here."
  },
  {
    "objectID": "lesson03/index.html#references",
    "href": "lesson03/index.html#references",
    "title": "Lesson 3",
    "section": "References",
    "text": "References\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-13\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder    * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n nycflights13 * 1.0.2   2021-04-12 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n readxl       * 1.4.3   2023-07-06 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson03/index.html#overview",
    "href": "lesson03/index.html#overview",
    "title": "Lesson 3",
    "section": "Overview",
    "text": "Overview\nReal world data rarely comes in exactly the right form for the analysis you want to do. This has led to the implementation of methods that facilitate manipulating data in a way that allows us to more easily address some of the common problems with data sets. In this lesson, we will introduce such methods which include but are not limited to\n\nselecting certain rows or columns of tabular data,\ncreating new variables or columns, often by transforming existing ones,\ngrouping or arranging observations,\nreshaping parts of the data,\njoining multiple tabular data sets.\n\nA particularly common application of these methods is to get data into an appropriate shape required for a particular type of plot, visualization, or summary.\nThe R package dplyr which belongs to the tidyverse family of packages is one of the available implementations of the common data manipulation methods (Wickham et al. 2023). The dplyr package is robust, well-documented, easy-to-use, and efficient so we will explore it in this lesson. It is worth while to take a moment to visit the ‘dplyr’ website as it contains helpful information and resources, view the webpage."
  },
  {
    "objectID": "lesson03/index.html#data-for-the-lesson",
    "href": "lesson03/index.html#data-for-the-lesson",
    "title": "Lesson 3",
    "section": "Data For the Lesson",
    "text": "Data For the Lesson\nIn this lesson, we will make use of a few data sets. This includes:\n\nThe Gapminder data set which is available in R through the gapminder package. To refresh our memories, let’s examine the first few rows of the Gapminder data:\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\n\nData from the nycflights13 package which we load (make sure the package is installed) using\n\n\nlibrary(nycflights13)\n\nWe can see a list of the data available in the nycflights13 package with the command\n\ndata(package=\"nycflights13\")\n\nwhich produces a table with the following information:\n\n\n\nName of Data Set\nInformation\n\n\n\n\nairlines\nAirline names\n\n\nairports\nAirport metadata\n\n\nflights\nFlights data\n\n\nplanes\nPlane metadata\n\n\nweather\nHourly weather data\n\n\n\nFor example, the flights data has first few rows\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n2013\n1\n1\n517\n515\n2\n830\n819\n11\nUA\n1545\nN14228\nEWR\nIAH\n227\n1400\n5\n15\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n533\n529\n4\n850\n830\n20\nUA\n1714\nN24211\nLGA\nIAH\n227\n1416\n5\n29\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n542\n540\n2\n923\n850\n33\nAA\n1141\nN619AA\nJFK\nMIA\n160\n1089\n5\n40\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n544\n545\n-1\n1004\n1022\n-18\nB6\n725\nN804JB\nJFK\nBQN\n183\n1576\n5\n45\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n554\n600\n-6\n812\n837\n-25\nDL\n461\nN668DN\nLGA\nATL\n116\n762\n6\n0\n2013-01-01 06:00:00\n\n\n2013\n1\n1\n554\n558\n-4\n740\n728\n12\nUA\n1696\nN39463\nEWR\nORD\n150\n719\n5\n58\n2013-01-01 05:00:00"
  },
  {
    "objectID": "lesson03/index.html#the-dplyr-package",
    "href": "lesson03/index.html#the-dplyr-package",
    "title": "Lesson 3",
    "section": "  The dplyr Package",
    "text": "The dplyr Package\nThe dplyr package consists of a set of functions that help you solve the most common data manipulation challenges (Wickham et al. 2023). Before discussing the individual functions, it’s worth listing what they have in common:\n\nThe first argument is always a data frame.\nThe subsequent arguments typically describe which columns to operate on, using the variable names (without quotes).\nThe output is always a new data frame.\n\nNow we will give examples of the typical use for the common dplyr functions:\n\nSelecting Columns\nThe select function extracts specified columns from a data frame:\n\ngapminder %&gt;%\n  select(country,year,lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\n\nYou can also specify which columns you don’t want:\n\ngapminder %&gt;%\n  select(-continent)\n\n# A tibble: 1,704 × 5\n   country      year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan  1952    28.8  8425333      779.\n 2 Afghanistan  1957    30.3  9240934      821.\n 3 Afghanistan  1962    32.0 10267083      853.\n 4 Afghanistan  1967    34.0 11537966      836.\n 5 Afghanistan  1972    36.1 13079460      740.\n 6 Afghanistan  1977    38.4 14880372      786.\n 7 Afghanistan  1982    39.9 12881816      978.\n 8 Afghanistan  1987    40.8 13867957      852.\n 9 Afghanistan  1992    41.7 16317921      649.\n10 Afghanistan  1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nExercise: Select the columns dep_delay and carrier from the flights data set. Select all the columns in the flights data set except year, month, and day.\n\n\nFiltering Rows\nThe filter function retains all rows of a data frame according to some specified condition(s).\n\ngapminder %&gt;%\n  filter(country == \"Spain\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Spain   Europe     1952    64.9 28549870     3834.\n 2 Spain   Europe     1957    66.7 29841614     4565.\n 3 Spain   Europe     1962    69.7 31158061     5694.\n 4 Spain   Europe     1967    71.4 32850275     7994.\n 5 Spain   Europe     1972    73.1 34513161    10639.\n 6 Spain   Europe     1977    74.4 36439000    13237.\n 7 Spain   Europe     1982    76.3 37983310    13926.\n 8 Spain   Europe     1987    76.9 38880702    15765.\n 9 Spain   Europe     1992    77.6 39549438    18603.\n10 Spain   Europe     1997    78.8 39855442    20445.\n11 Spain   Europe     2002    79.8 40152517    24835.\n12 Spain   Europe     2007    80.9 40448191    28821.\n\n\nNotice that to specify equality for a condition in filter one must use the double equals ==.\nHere are some other variations using filter:\n\ngapminder %&gt;%\n  filter(country == \"Spain\" | country == \"Portugal\")\n\n# A tibble: 24 × 6\n   country  continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Portugal Europe     1952    59.8  8526050     3068.\n 2 Portugal Europe     1957    61.5  8817650     3775.\n 3 Portugal Europe     1962    64.4  9019800     4728.\n 4 Portugal Europe     1967    66.6  9103000     6362.\n 5 Portugal Europe     1972    69.3  8970450     9022.\n 6 Portugal Europe     1977    70.4  9662600    10172.\n 7 Portugal Europe     1982    72.8  9859650    11754.\n 8 Portugal Europe     1987    74.1  9915289    13039.\n 9 Portugal Europe     1992    74.9  9927680    16207.\n10 Portugal Europe     1997    76.0 10156415    17641.\n# ℹ 14 more rows\n\n\n\ngapminder %&gt;%\n  filter(year &gt;= 1979)\n\n# A tibble: 852 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1982    39.9 12881816      978.\n 2 Afghanistan Asia       1987    40.8 13867957      852.\n 3 Afghanistan Asia       1992    41.7 16317921      649.\n 4 Afghanistan Asia       1997    41.8 22227415      635.\n 5 Afghanistan Asia       2002    42.1 25268405      727.\n 6 Afghanistan Asia       2007    43.8 31889923      975.\n 7 Albania     Europe     1982    70.4  2780097     3631.\n 8 Albania     Europe     1987    72    3075321     3739.\n 9 Albania     Europe     1992    71.6  3326498     2497.\n10 Albania     Europe     1997    73.0  3428038     3193.\n# ℹ 842 more rows\n\n\n\ngapminder %&gt;%\n  filter(year &gt;= 1979 & country == \"Spain\")\n\n# A tibble: 6 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Spain   Europe     1982    76.3 37983310    13926.\n2 Spain   Europe     1987    76.9 38880702    15765.\n3 Spain   Europe     1992    77.6 39549438    18603.\n4 Spain   Europe     1997    78.8 39855442    20445.\n5 Spain   Europe     2002    79.8 40152517    24835.\n6 Spain   Europe     2007    80.9 40448191    28821.\n\n\nExercise: Use filter to select those observations in the flights data that had a departure from JFK airport. Use filter to select those observations in the flights data that had a departure from JFK airport and the airline (carrier) is United Airlines (UA). Use filter to extract those observations where there was a departure delay that was an hour or more.\n\n\nGrouping\nThe group_by function allows us to group observations in a data frame by one or more grouping variables. The syntax for group_by is:\n\ngapminder %&gt;%\n  group_by(year)\n\n# A tibble: 1,704 × 6\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nor for more than one variable:\n\ngapminder %&gt;%\n  group_by(country,year)\n\n# A tibble: 1,704 × 6\n# Groups:   country, year [1,704]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nBy itself, group_by isn’t very useful. However, used with other functions it can be extremely useful. For example, we can combine group_by and summarise to create grouped summaries:\n\ngapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_lifeEx = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year mean_lifeEx\n   &lt;int&gt;       &lt;dbl&gt;\n 1  1952        49.1\n 2  1957        51.5\n 3  1962        53.6\n 4  1967        55.7\n 5  1972        57.6\n 6  1977        59.6\n 7  1982        61.5\n 8  1987        63.2\n 9  1992        64.2\n10  1997        65.0\n11  2002        65.7\n12  2007        67.0\n\n\nHere is another example:\n\nflights %&gt;%\n  filter(dep_delay &gt;= 60) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(n_carrier = n())\n\n# A tibble: 16 × 2\n   carrier n_carrier\n   &lt;chr&gt;       &lt;int&gt;\n 1 9E           1991\n 2 AA           2034\n 3 AS             39\n 4 B6           4655\n 5 DL           2699\n 6 EV           6984\n 7 F9             75\n 8 FL            323\n 9 HA             11\n10 MQ           2037\n11 OO              4\n12 UA           3899\n13 US            779\n14 VX            365\n15 WN           1084\n16 YV             80\n\n\nThis tells us the number of times each carrier has a departure delay of at least an hour. We can add the arrange function to put these in order:\n\nflights %&gt;%\n  filter(dep_delay &gt;= 60) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(n_carrier = n()) %&gt;%\n  arrange(n_carrier)\n\n# A tibble: 16 × 2\n   carrier n_carrier\n   &lt;chr&gt;       &lt;int&gt;\n 1 OO              4\n 2 HA             11\n 3 AS             39\n 4 F9             75\n 5 YV             80\n 6 FL            323\n 7 VX            365\n 8 US            779\n 9 WN           1084\n10 9E           1991\n11 AA           2034\n12 MQ           2037\n13 DL           2699\n14 UA           3899\n15 B6           4655\n16 EV           6984\n\n\nor if we want the opposite order:\n\nflights %&gt;%\n  filter(dep_delay &gt;= 60) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(n_carrier = n()) %&gt;%\n  arrange(desc(n_carrier))\n\n# A tibble: 16 × 2\n   carrier n_carrier\n   &lt;chr&gt;       &lt;int&gt;\n 1 EV           6984\n 2 B6           4655\n 3 UA           3899\n 4 DL           2699\n 5 MQ           2037\n 6 AA           2034\n 7 9E           1991\n 8 WN           1084\n 9 US            779\n10 VX            365\n11 FL            323\n12 YV             80\n13 F9             75\n14 AS             39\n15 HA             11\n16 OO              4\n\n\nExercise: Use group_by and summarise to compute the average departure delay by carrier. Use group_by and summarise to find the number of times each airport has had departure delays of an hour or more, use arrange to place the airports with the most departure delays at the top.\nWe can group by more than one variable:\n\ngapminder %&gt;%\n  group_by(year,continent) %&gt;%\n  summarise(continent_count = n())\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 60 × 3\n# Groups:   year [12]\n    year continent continent_count\n   &lt;int&gt; &lt;fct&gt;               &lt;int&gt;\n 1  1952 Africa                 52\n 2  1952 Americas               25\n 3  1952 Asia                   33\n 4  1952 Europe                 30\n 5  1952 Oceania                 2\n 6  1957 Africa                 52\n 7  1957 Americas               25\n 8  1957 Asia                   33\n 9  1957 Europe                 30\n10  1957 Oceania                 2\n# ℹ 50 more rows\n\n\nand also summarise more than one variable after grouping.\n\ngapminder %&gt;%\n  group_by(year,continent) %&gt;%\n  summarise(continent_count = n(),\n            mean_lifeExp = mean(lifeExp))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 60 × 4\n# Groups:   year [12]\n    year continent continent_count mean_lifeExp\n   &lt;int&gt; &lt;fct&gt;               &lt;int&gt;        &lt;dbl&gt;\n 1  1952 Africa                 52         39.1\n 2  1952 Americas               25         53.3\n 3  1952 Asia                   33         46.3\n 4  1952 Europe                 30         64.4\n 5  1952 Oceania                 2         69.3\n 6  1957 Africa                 52         41.3\n 7  1957 Americas               25         56.0\n 8  1957 Asia                   33         49.3\n 9  1957 Europe                 30         66.7\n10  1957 Oceania                 2         70.3\n# ℹ 50 more rows\n\n\nWe will spend a lot of time later talking about plots and visualizations. Just to see what is possible, let’s make a plot using the results of a grouped summary:\n\na_grouped_summary &lt;- gapminder %&gt;%\n  group_by(year,continent) %&gt;%\n  summarise(continent_count = n(),\n            mean_lifeExp = mean(lifeExp))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\na_grouped_summary %&gt;%\n  ggplot(aes(x=year,y=mean_lifeExp,color=continent)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nFigure 1: A plot made using data obtained from a grouped summary.\n\n\n\n\nExercise: Modify the code in the last code chunk to make a plot of the average departure delay by airport per each day. That is, the x-axis should be the day, the y-axis should be the average departure delay and the colors should distinguish the different airports. Hint: When you compute the mean, you will have to add an extra argument, na.rm=TRUE because there are missing values in the data.\n\n\nReshaping\nTypically, we want our data to be in tidy format. This means\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFigure 2 shows the rules visually.\n\n\n\n\n\nFigure 2: The following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells. Illustration credit: https://github.com/hadley/r4ds/blob/main/images/tidy-1.png\n\n\n\n\nOften, getting data into tidy format requires reshaping the data. But there maybe reasons to reshape the data from tidy format to another format. The pivot_longer and pivot_wider functions facilitate these types of reshaping.\nFor example, suppose that for some reason we want to view the life expectancy for each country in the Gapminder data so that the values for each year are in their own column. This can be achieved with pivot_wider:\n\ngapminder %&gt;%\n  select(country,year,lifeExp) %&gt;%\n  pivot_wider(names_from = \"year\",values_from = lifeExp)\n\n# A tibble: 142 × 13\n   country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghan…   28.8   30.3   32.0   34.0   36.1   38.4   39.9   40.8   41.7   41.8\n 2 Albania   55.2   59.3   64.8   66.2   67.7   68.9   70.4   72     71.6   73.0\n 3 Algeria   43.1   45.7   48.3   51.4   54.5   58.0   61.4   65.8   67.7   69.2\n 4 Angola    30.0   32.0   34     36.0   37.9   39.5   39.9   39.9   40.6   41.0\n 5 Argent…   62.5   64.4   65.1   65.6   67.1   68.5   69.9   70.8   71.9   73.3\n 6 Austra…   69.1   70.3   70.9   71.1   71.9   73.5   74.7   76.3   77.6   78.8\n 7 Austria   66.8   67.5   69.5   70.1   70.6   72.2   73.2   74.9   76.0   77.5\n 8 Bahrain   50.9   53.8   56.9   59.9   63.3   65.6   69.1   70.8   72.6   73.9\n 9 Bangla…   37.5   39.3   41.2   43.5   45.3   46.9   50.0   52.8   56.0   59.4\n10 Belgium   68     69.2   70.2   70.9   71.4   72.8   73.9   75.4   76.5   77.5\n# ℹ 132 more rows\n# ℹ 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt;\n\n\nAs an application, we can use such data to make a table like the following one:\n\ngapminder %&gt;%\n  select(country,year,lifeExp) %&gt;%\n  pivot_wider(names_from = \"year\",values_from = lifeExp) %&gt;%\n  select(country,`1952`,`2007`) %&gt;%\n  filter(country %in% c(\"Spain\",\"Portugal\")) %&gt;%\n  kable()\n\n\n\n\ncountry\n1952\n2007\n\n\n\n\nPortugal\n59.82\n78.098\n\n\nSpain\n64.94\n80.941\n\n\n\n\n\n\n\nExercise: Use pivot_wider to create a data frame starting with the Gapminder data that has the population values for each country but with each year as its own column.\nExercise: Explain what the following command does:\n\nflights %&gt;% \n  pivot_wider(names_from = origin,values_from = distance)\n\nThe billboard data set records the billboard rank of songs in the year 2000:\n\nbillboard %&gt;%\n  head() %&gt;%\n  kable()\n\n\n\n\nartist\ntrack\ndate.entered\nwk1\nwk2\nwk3\nwk4\nwk5\nwk6\nwk7\nwk8\nwk9\nwk10\nwk11\nwk12\nwk13\nwk14\nwk15\nwk16\nwk17\nwk18\nwk19\nwk20\nwk21\nwk22\nwk23\nwk24\nwk25\nwk26\nwk27\nwk28\nwk29\nwk30\nwk31\nwk32\nwk33\nwk34\nwk35\nwk36\nwk37\nwk38\nwk39\nwk40\nwk41\nwk42\nwk43\nwk44\nwk45\nwk46\nwk47\nwk48\nwk49\nwk50\nwk51\nwk52\nwk53\nwk54\nwk55\nwk56\nwk57\nwk58\nwk59\nwk60\nwk61\nwk62\nwk63\nwk64\nwk65\nwk66\nwk67\nwk68\nwk69\nwk70\nwk71\nwk72\nwk73\nwk74\nwk75\nwk76\n\n\n\n\n2 Pac\nBaby Don't Cry (Keep...\n2000-02-26\n87\n82\n72\n77\n87\n94\n99\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2Ge+her\nThe Hardest Part Of ...\n2000-09-02\n91\n87\n92\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3 Doors Down\nKryptonite\n2000-04-08\n81\n70\n68\n67\n66\n57\n54\n53\n51\n51\n51\n51\n47\n44\n38\n28\n22\n18\n18\n14\n12\n7\n6\n6\n6\n5\n5\n4\n4\n4\n4\n3\n3\n3\n4\n5\n5\n9\n9\n15\n14\n13\n14\n16\n17\n21\n22\n24\n28\n33\n42\n42\n49\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3 Doors Down\nLoser\n2000-10-21\n76\n76\n72\n69\n67\n65\n55\n59\n62\n61\n61\n59\n61\n66\n72\n76\n75\n67\n73\n70\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n504 Boyz\nWobble Wobble\n2000-04-15\n57\n34\n25\n17\n17\n31\n36\n49\n53\n57\n64\n70\n75\n76\n78\n85\n92\n96\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n98^0\nGive Me Just One Nig...\n2000-08-19\n51\n39\n34\n26\n26\n19\n2\n2\n3\n6\n7\n22\n29\n36\n47\n67\n66\n84\n93\n94\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each. Here, the column names are one variable (the week) and the cell values are another (the rank).\nTo tidy this data, we’ll use pivot_longer():\n\nbillboard %&gt;%\n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n\nExercise: Consider the diamonds data set from the ggplot2 package which is part of the tidyverse. The first few rows of the diamonds data looks as follows:\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\n\nApply the pivot_longer function to the diamonds data to combine the x, y, and z columns into a single column.\nTo see that pivot_longer and pivot_wider are inverses of one another, examine the following code:\n\ngapminder %&gt;%\n  select(country,year,lifeExp) %&gt;%\n  pivot_wider(names_from = \"year\",values_from = lifeExp) %&gt;%\n  pivot_longer(cols=-c(\"country\"),names_to = \"year\", values_to = \"lifeExp\")\n\n# A tibble: 1,704 × 3\n   country     year  lifeExp\n   &lt;fct&gt;       &lt;chr&gt;   &lt;dbl&gt;\n 1 Afghanistan 1952     28.8\n 2 Afghanistan 1957     30.3\n 3 Afghanistan 1962     32.0\n 4 Afghanistan 1967     34.0\n 5 Afghanistan 1972     36.1\n 6 Afghanistan 1977     38.4\n 7 Afghanistan 1982     39.9\n 8 Afghanistan 1987     40.8\n 9 Afghanistan 1992     41.7\n10 Afghanistan 1997     41.8\n# ℹ 1,694 more rows\n\n\nPivoting is covered in more detail in Chapter 6 of (Wickham, Çetinkaya-Rundel, and Grolemund 2023)."
  },
  {
    "objectID": "lesson03/index.html#footnotes",
    "href": "lesson03/index.html#footnotes",
    "title": "Lesson 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe song will be included as long as it was in the top 100 at some point in 2000, and is tracked for up to 72 weeks after it appears.↩︎"
  },
  {
    "objectID": "lesson04/index.html",
    "href": "lesson04/index.html",
    "title": "Lesson 4",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nChoose an appropriate visualization for exploring and summarizing a tabular data set.\nUse methods from the ggplot2 package to create an appropriate visualization for exploring and summarizing a tabular data set.\nApply steps from exploratory data analysis to generate or refine questions about a data set."
  },
  {
    "objectID": "lesson04/index.html#learning-objectives",
    "href": "lesson04/index.html#learning-objectives",
    "title": "Lesson 4",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nChoose an appropriate visualization for exploring and summarizing a tabular data set.\nUse methods from the ggplot2 package to create an appropriate visualization for exploring and summarizing a tabular data set.\nApply steps from exploratory data analysis to generate or refine questions about a data set."
  },
  {
    "objectID": "lesson04/index.html#readings-etc.",
    "href": "lesson04/index.html#readings-etc.",
    "title": "Lesson 4",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapters 2 and 11 from R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View book online.\nChapter 4 from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online."
  },
  {
    "objectID": "lesson04/index.html#overview",
    "href": "lesson04/index.html#overview",
    "title": "Lesson 4",
    "section": "Overview",
    "text": "Overview\nAn exploratory data analysis (EDA) is any initial investigation of a data set or data sets with the goal to simply see what is in the data and what questions one may ask from the data.\nImportant: Every statistical or machine learning analysis should start with an EDA.\nYour goal during EDA is to develop an understanding of your data. Two major themes for EDA are:\n\nTo explore what type of variation occurs within variables.\nTo explore what type of covariation occurs between variables.\n\nThe two most important tools for EDA are\n\nNumerical data summaries.\nVisual data summaries.\n\nIn this lesson, we will first build our toolkit for EDA with a major focus on gaining tools and skills for data visualization. Then, we will work through some EDA case studies."
  },
  {
    "objectID": "lesson04/index.html#introduction-to-data-visualization",
    "href": "lesson04/index.html#introduction-to-data-visualization",
    "title": "Lesson 4",
    "section": "Introduction to Data Visualization",
    "text": "Introduction to Data Visualization\nMost people process visual information quickly and easily. Few people can learn much at all by scrolling through a spreadsheet of raw data. Thus, it is important for a data scientist to develop excellent skills in data visualization. By data visualization, we mean the process of representing data via a visual format.\nIn this course, we will utilize the ggplot2 package (and maybe some packages that extend ggplot2) for data visualization (Wickham 2016). There are many excellent resources on data visualization. Two that are highly recommended and available for free online are Data Visualization a Practical Introduction and Fundamentals of Data Visualization (Healy 2018; Wilke 2019). View Data Visualization a Practical Introduction. View Fundamentals of Data Visualization. Not only are these two books free, they are also recent and utilize R and ggplot2.\nData visualizations come with a risk of confusing people. Further, visualizations might not work well for everyone. So, it is very important to choose visualizations that are as clear, accessible, and clean as possible. For example, you should:\n\nAvoid the use of colors that are not easy to distinguish.\nUse appropriate scales.\nUse alt-text and figure captions.\n\nThere are other practices and techniques for making effective and accessible visualizations that we will discuss later.\n\n  The ggplot2 Package\nThe ggplot2 package implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\nFigure 1 shows the typical structure of the grammar of graphics as implemented in the ggplot2 package1.\n\n\n\n\n\nFigure 1: The basic ggplot2 syntax. Figure source (Timbers, Campbell, and Lee 2022).\n\n\n\n\nLet’s see what happens when we run the commands from Figure 1.\n\nggplot(ten_lang, aes(x = language, y = mother_tongue)) +\n  geom_bar(stat = \"identity\")\n\n# the following code is equivalent\n# ten_lang %&gt;% \n#   ggplot(aes(x = language, y = mother_tongue)) +\n#   geom_bar(stat = \"identity\")\n\n\n\n\nFigure 2: The bar plot resulting from running code explained in Figure 1\n\n\n\n\nBefore we dive further into the details of using ggplot2 and see more examples, let’s think about what kinds of plots or graphs we might want to make.\n\n\nTypes of Plots\nThere are many different types of plots one can make so there isn’t necessarily a unique choice to be made when deciding how to visualize data. However, there are a few considerations that will have a strong influence on what type of plot(s) you might create to visualize some data.\nAmong the most important considerations is, the type of variable(s) involved in the parts of your data that you want to display visually. Table 1 lists the most common variable types.\n\n\nTable 1: Types of variables encountered in typical data visualization scenarios, table from (Wilke 2019).\n\n\n\n\n\n\n\n\nType of variable\nExamples\nAppropriate scale\nDescription\n\n\n\n\nquantitative/numerical continuous\n1.3, 5.7, 83, 1.5x10-2\ncontinuous\nArbitrary numerical values. These can be integers, rational numbers, or real numbers.\n\n\nquantitative/numerical discrete\n1, 2, 3, 4\ndiscrete\nNumbers in discrete units. These are most commonly but not necessarily integers. For example, the numbers 0.5, 1.0, 1.5 could also be treated as discrete if intermediate values cannot exist in the given dataset.\n\n\nqualitative/categorical unordered\ndog, cat, fish\ndiscrete\nCategories without order. These are discrete and unique categories that have no inherent order. These variables are also called factors.\n\n\nqualitative/categorical ordered\ngood, fair, poor\ndiscrete\nCategories with order. These are discrete and unique categories with an order. For example, “fair” always lies between “good” and “poor”. These variables are also called ordered factors.\n\n\ndate or time\nJan. 5 2018, 8:03am\ncontinuous or discrete\nSpecific days and/or times. Also generic dates, such as July 4 or Dec. 25 (without year).\n\n\ntext\nThe quick brown fox jumps over the lazy dog.\nnone, or discrete\nFree-form text. Can be treated as categorical if needed.\n\n\n\n\nThe reason why data types such as those listed in Table 1 influence the type of plot(s) used to display data is because the data type of a variable determines what kind of aesthetics can be used in a plot.\nBy an aesthetic, we mean a visual element that can be used to describe aspects of a given graphic. Figure 3 shows some common aesthetics for plots or graphs used in data visualization.\n\n\n\n\n\nFigure 3: Common aesthetics\n\n\n\n\nReturning to Figure 1, we notice that one of the arguments that must be given to ggplot is aes which is short for aesthetic.\nFigure 4 shows several of the most common types of plots for a single or pair of variables. These plot types are:\n\nscatter plots visualize the relationship between two quantitative variables,\nline plots visualize trends with respect to an independent, ordered quantity (e.g., time),\nbar plots visualize comparisons of amounts, and\nhistograms visualize the distribution of one quantitative variable (i.e., all its possible values and how often they occur)\n\n\n\n\n\n\nFigure 4: Examples of scatter, line and bar plots, as well as histograms.\n\n\n\n\n\n\nTips for Good Visualizations\nWe will soon learn how to make these types of plots and more with ggplot2. Before that, here is a list of tips for making sure that our visualizations are good:\n\nMake sure the visualization answers the question you have asked most simply and plainly as possible.\nUse legends and labels so that your visualization is understandable without reading the surrounding text.\nEnsure the text, symbols, lines, etc., on your visualization are big enough to be easily read.\nEnsure the data are clearly visible; don’t hide the shape/distribution of the data behind other objects (e.g., a bar).\nMake sure to use color schemes that are easily visible by those with colorblindness (a surprisingly large fraction of the overall population—from about 1% to 10%, depending on sex and ancestry (Deeb 2005)). For example, ColorBrewer and the RColorBrewer R package (Neuwirth 2014) provide the ability to pick such color schemes, and you can check your visualizations after you have created them by uploading to online tools such as a color blindness simulator.\nRedundancy can be helpful; sometimes conveying the same message in multiple ways reinforces it for the audience.\nUse colors sparingly. Too many different colors can be distracting, create false patterns, and detract from the message.\nBe wary of overplotting. Overplotting is when marks that represent the data overlap, and is problematic as it prevents you from seeing how many data points are represented in areas of the visualization where this occurs. If your plot has too many dots or lines and starts to look like a mess, you need to do something different.\nOnly make the plot area (where the dots, lines, bars are) as big as needed. Simple plots can be made small.\nDon’t adjust the axes to zoom in on small differences. If the difference is small, show that it’s small!\n\nIf you’re not sure what plot to make for your data, the data-to-viz website can be very helpful. View the data-to-viz site. Once you know what type of plot to make, the R Graph Gallery website is helpful for finding the code to make the plot with ggplot2. View the R Graph Gallery"
  },
  {
    "objectID": "lesson04/index.html#preparation-for-the-next-lesson",
    "href": "lesson04/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 4",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nChapter 26 from R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View book online."
  },
  {
    "objectID": "lesson04/index.html#references",
    "href": "lesson04/index.html#references",
    "title": "Lesson 4",
    "section": "References",
    "text": "References\n\n\nDeeb, Sameer. 2005. “The Molecular Basis of Variation in Human Color Vision.” Clinical Genetics 67: 369–77.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nNeuwirth, Erich. 2014. RColorBrewer: ColorBrewer Palettes. https://cran.r-project.org/web/packages/RColorBrewer/index.html.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-16\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n canlang      * 0.0.1   2023-09-13 [1] Github (ttimbers/canlang@1a54305)\n cowplot      * 1.1.1   2020-12-30 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder    * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magick       * 2.7.5   2023-08-07 [1] CRAN (R 4.3.0)\n patchwork    * 1.1.3   2023-08-14 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n RColorBrewer * 1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n readxl       * 1.4.3   2023-07-06 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson04/index.html#eda-case-studies",
    "href": "lesson04/index.html#eda-case-studies",
    "title": "Lesson 4",
    "section": "EDA Case Studies",
    "text": "EDA Case Studies\nIn this section, we will work through a couple of short EDA case studies. If you want to see and EDA live coded by a master data scientist, check out the following video:\n\nExploratory data analysis worked example video by Hadley Wickham, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nEDA Case Study 1:\n\n\nEDA Case Study 1:"
  },
  {
    "objectID": "lesson04/index.html#footnotes",
    "href": "lesson04/index.html#footnotes",
    "title": "Lesson 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use the pipe operator %&gt;% (or even |&gt;) to input a data frame into the first argument for the ggplot function.↩︎"
  },
  {
    "objectID": "lesson04/index.html#using-the-ggplot2-package",
    "href": "lesson04/index.html#using-the-ggplot2-package",
    "title": "Lesson 4",
    "section": "  Using the ggplot2 Package",
    "text": "Using the ggplot2 Package\nIn this section, we will go through many use cases for ggplot2. For the examples and exercises in this section, we will work with three data sets:\n\nThe mpg data set in the ggplot2 package. The first few rows for mpg are shown in Table 2.\nThe diamonds data set in the ggplot2 package. The first few rows for diamonds are shown in Table 3.\nThe tornados data set from the Tidy Tuesday data repository. The first few rows for this data are shown in Table 4.\n\nTo access the mpg and diamonds data sets, we just need to load the ggplot2 package. To access the tornados data, we need to load the .csv file which we do with the following code:\n\ntornados &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-16/tornados.csv')\n\nLet’s look at the first few rows for each of the three data sets:\n\nmpg\n\n\n\n\n\nTable 2: Table showing first few rows of the mpg data set.\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\n\n\n\n\n\n\n\ndiamonds\n\n\n\n\n\nTable 3: Table showing first few rows of the diamonds data set.\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\n\n\n\ntornados\n\n\n\n\n\nTable 4: Table showing first few rows of the tornados data set.\n\n\nom\nyr\nmo\ndy\ndate\ntime\ntz\ndatetime_utc\nst\nstf\nmag\ninj\nfat\nloss\nslat\nslon\nelat\nelon\nlen\nwid\nns\nsn\nf1\nf2\nf3\nf4\nfc\n\n\n\n\n192\n1950\n10\n1\n1950-10-01\n21:00:00\nAmerica/Chicago\n1950-10-02 03:00:00\nOK\n40\n1\n0\n0\n5e+04\n36.73\n-102.52\n36.88\n-102.30\n15.8\n10\n1\n1\n25\n0\n0\n0\nFALSE\n\n\n193\n1950\n10\n9\n1950-10-09\n02:15:00\nAmerica/Chicago\n1950-10-09 08:15:00\nNC\n37\n3\n3\n0\n5e+05\n34.17\n-78.60\n0.00\n0.00\n2.0\n880\n1\n1\n47\n0\n0\n0\nFALSE\n\n\n195\n1950\n11\n20\n1950-11-20\n02:20:00\nAmerica/Chicago\n1950-11-20 08:20:00\nKY\n21\n2\n0\n0\n5e+05\n37.37\n-87.20\n0.00\n0.00\n0.1\n10\n1\n1\n177\n0\n0\n0\nFALSE\n\n\n196\n1950\n11\n20\n1950-11-20\n04:00:00\nAmerica/Chicago\n1950-11-20 10:00:00\nKY\n21\n1\n0\n0\n5e+05\n38.20\n-84.50\n0.00\n0.00\n0.1\n10\n1\n1\n209\n0\n0\n0\nFALSE\n\n\n197\n1950\n11\n20\n1950-11-20\n07:30:00\nAmerica/Chicago\n1950-11-20 13:30:00\nMS\n28\n1\n3\n0\n5e+04\n32.42\n-89.13\n0.00\n0.00\n2.0\n37\n1\n1\n101\n0\n0\n0\nFALSE\n\n\n194\n1950\n11\n4\n1950-11-04\n17:00:00\nAmerica/Chicago\n1950-11-04 23:00:00\nPA\n42\n3\n1\n0\n5e+05\n40.20\n-76.12\n40.40\n-75.93\n15.9\n100\n1\n1\n71\n11\n0\n0\nFALSE\n\n\n\n\n\n\n\n\nExercise: For each variables in each of the three data sets, determine the appropriate data type.\n\nBasic ggplot2 Usage\n\nNumerical Data\nHistograms are a common way to visualize a single numerical variable. Here is how to make a basic histogram with ggplot2:\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that we get a message about the number of “bins” for our histogram.\nQuestion: What is meant by the number of bins for a histogram?\nWe can change either the number of bins or the width of bins for a histogram created with ggplot2. For example,\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(bins=22)\n\n\n\n\nor\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\nThe aesthetic value fill determines the shading of a histogram (and also other graphical objects as we will see later).\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(fill=\"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe aesthetic value color determines the boundary color of a histogram (and also other graphical objects as we will see later).\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(color=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHere is a nicer version of our histogram where we’ve included better labels for the axes:\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(fill=\"purple\",color=\"white\",bins = 25) + \n  labs(x=\"Highway gas milage (mpg)\",y=\"Count\")\n\n\n\n\nExercise: Create a nice histogram for the carat variable in the diamonds data set.\nBox plots are an alternative way to visualize numerical variables. Here is a basic box plot created using using ggplot2:\n\nmpg %&gt;%\n  ggplot(aes(y=hwy)) + \n  geom_boxplot()\n\n\n\n\nHere is an nicer version that also adds the data points over the box plot:\n\nmpg %&gt;%\n  ggplot(aes(y=hwy,x=rep(1,nrow(mpg)))) + \n  geom_boxplot(outlier.shape = NA,color=\"orange\") +\n  geom_jitter(width = 0.2,alpha=0.5,color=\"darkgreen\") + \n  labs(x=\"\",y=\"Highway gas mileage (mpg)\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\nExercise: Compare and contrast histograms and box plots.\nExercise: Create a nice box plot for the carat variable in the diamonds data set.\nDensity plots are yet another way to visualize numerical variables. They are basically smoothed versions of histograms but with the y-axis scaled so that the total area is 1.\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_density()\n\n\n\n\nExercise: What are the main aesthetics used in a density plot?\nSince density plots involves lines or better curves, we can adjust the line width. For example,\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_density(linewidth=2)\n\n\n\n\nAdding a so-called rug let’s us see where the observed values in our data fall:\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_density(linewidth=2) + \n  geom_rug()\n\n\n\n\nBy normalizing our data, we can plot a density curve over the data histogram. For example,\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(aes(y = ..density..),fill=\"purple\",color=\"white\",bins = 25) + \n  geom_density(linewidth=2,color=\"gold\") + \n  geom_rug()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nExercise: Create a nice density plot for the carat variable in the diamonds data set.\n\n\nCategorical Data\nPerhaps the most common plot type for a single\n\ndiamonds %&gt;%\n  ggplot(aes(cut)) + \n  geom_bar()\n\n\n\n\nNote that a bar plot is just a visual representation of a frequency table for the variable:\n\ndiamonds %&gt;%\n  pull(cut) %&gt;%\n  table()\n\n.\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551 \n\n\nSimilar to what we saw with histograms, the color aesthetic changes the boundary of our bars in bar plots:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,color=cut)) + \n  geom_bar()\n\n\n\n\nwhile the fill aesthetic changes the shading of our bars:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar()\n\n\n\n\nThe next plot demonstrates how to modify the color palette for a categorical variable:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar(color=\"black\") + \n  scale_fill_brewer(palette=\"Spectral\") + \n  labs(x = \"Cut of diamond\",y=\"Count\", fill=\"Cut of diamond\") \n\n\n\n\nWe can also manually change the ordering of our variables:\n\ndiamonds %&gt;%\n  mutate(cut = factor(cut,levels=c(\"Fair\",\"Good\",\"Ideal\",\"Premium\",\"Very Good\"))) %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar(color=\"black\") + \n  scale_fill_brewer(palette=\"Spectral\") + \n  labs(x = \"Cut of diamond\",y=\"Count\", fill=\"Cut of diamond\") \n\n\n\n\nExercise: Create a nice bar plot for the class variable in the mpg data set."
  },
  {
    "objectID": "lesson04/index.html#figures-in-quarto",
    "href": "lesson04/index.html#figures-in-quarto",
    "title": "Lesson 4",
    "section": "Figures in Quarto",
    "text": "Figures in Quarto"
  }
]