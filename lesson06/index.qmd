---
title: "Lesson 6"
subtitle: "Introduction to probability and random variables"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson06.bib
license: "CC BY-NC-SA 4.0"
---



## Learning Objectives

After this lesson, students will be able to: 

- Identify and describe some common probability distributions. 

- State Bayes' theorem.

- Understand the concepts of probability mass functions (PMFs), probability density functions (PDFs), and cumulative distribution functions (CDFs).

- Work with various distributions in R. 


## Overview

Probability and random variables play a fundamental role in data science and machine learning by providing the theoretical foundation for understanding uncertainty and variability in data. Later in the course, we will study inference and predictive modeling and it will be helpful to have the basic vocabulary of probability and random variables at our disposal to help describe methods for inference and predictive modeling.

Informally:

* **Probability** is the mathematical framework for quantifying uncertainty. In data science, it allows us to express how likely different outcomes are, providing a way to model the inherent variability in data. Probability is used to formulate hypotheses, make predictions, and assess the likelihood of various events occurring. Understanding probability is crucial when dealing with tasks such as statistical inference, hypothesis testing, machine learning, and Bayesian modeling.

* **Random variables** are a key concept that extends the idea of probability. A random variable is a variable whose value is subject to chance or randomness. In data science and machine learning, random variables are used to represent uncertain quantities that we wish to analyze. They can be discrete, like the outcome of a coin toss, or continuous, like the height of individuals in a population. Random variables help us describe and model data, and they serve as the basis for statistical models and machine learning algorithms.

There are other data science and mathematics courses in our curriculum that go into more detail on probability and random variables so our treatment in this course will be brief and informal.  

## Introduction to Probability

The notion of probability starts with the concept of a **random experiment**, that is, a process with a well-defined but unpredictable outcome. A quintessential example is provided by tossing a coin. We know in advance that the outcome will be either heads or tails but it is difficult to determine with certainty the outcome of any given toss. 

A working definition for probability that is often used is

> For an observation of a random experiment, the probability of a particular outcome is the proportion of times that outcome would occur in an indefinitely long sequence of like observations, under the same conditions. 

This definition is why one typically says that the probability of tossing heads for a fair coin is 0.5 (or 50%).

**Note:** According to our working definition a probability is always a number between 0 and 1 (inclusive).

For a random experiment, we define the **sample space**, often denoted by $S$ to be the set of all possible outcomes. So, the sample space for the random experiment of tossing a coin one time would be $S = \{\text{heads},\text{tails}\}$. An **event** is a subset of a sample space. We usually denote events by upper case latin letters like $A$. For example, the event of tossing a heads would be $A = \{\text{heads}\}$. 

While it takes some time and effort to wrap one's head around this, it is a fact that we can build up complicated events through logical operations. For example, we can define an event that is the event of tossing either heads or[^1] tails. Also, one can define an event that is the event of tossing heads and tails. 

[^1]: Here, "or" is used in the **inclusive** sense. 

**Question:** Suppose that $A$ is the event of tossing either heads or tails in a single toss of a coin. Why should we say that the probability of this event is 1? Suppose that  $B$ is the event of tossing both heads and tails in a single toss of a coin. Why should we say that the probability of this event is 0?

**Notation:** If $A$ is an event, we denote by $P(A)$ the probability that $A$ has occurred. Given two events $A$ and $B$, we denote by $A \cup B$ the event that $A$ or $B$ occurred; we denote by $A \cap B$ the event that $A$ and $B$ occurred. 

A notion from probability that is very important in data science is that of **conditional probability**. A probability of an event $A$, given that an event $B$ occurred, is called a conditional probability and is denoted by $P(A | B)$ and read as "the probability of $A$ given $B$". We define conditional probability mathematically by

$$
P(A | B) = \frac{P(A \cap B)}{P(B)}
$$

It is important to realize that in general $P(A|B) \neq P(B|A)$. For example, suppose that $A$ is the event that I carry an umbrella on any given day and $B$ is the event that it is raining on a given day. Then,

$$
P(A|B) = \text{the probability I will carry an umbrella given that it is raining}
$$

while

$$
P(B|A) = \text{the probability it is raining given that I'm carrying an umbrella}
$$

and there is no reason to believe that these probabilities should be the same. 

However, there is an important connection between $P(A|B)$ and  $P(B|A)$ known as **Bayes' theorem** which we state as

$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^{C})P(B^{C})}
$$
whenever $A$ is an event such that $P(A) \neq 0$ and where $B^{C}$ denotes the *complement* of the event $B$.

Let's examine a simple example. Suppose that we toss a fair coin three times and record the number of heads. Let

* $B =$ the event that the first toss is heads, and 

* $A =$ the event that at least one toss is heads. 

Then,

* $P(B|A) =$ the probability that the first toss is heads given that at least one toss is heads, and 

* $P(A|B) =$ the probability that at least one toss is heads given that the first toss is heads. 

It is probably obvious that $P(A|B) = 1$, and with just a little thought, one can reason out that $P(A) = \frac{7}{8}$ and $P(B) = \frac{1}{2}$. Then, by Bayes' theorem we should have

$$
P(B|A) = \frac{1 \cdot \frac{1}{2}}{\frac{7}{8}} = \frac{4}{7}
$$
It is often more convenient to work with numbers that it is to work with events from general sample spaces. Random variables are a mathematical tool for quantifying events.  

We say that two events $A$ and $B$ are **independent** if $P(A|B) = P(A)$. For example, suppose that we toss a fair coin three times and record the number of heads. Let $A$ be the event that the first toss is heads and $B$ be the event that the second toss is heads. Then, $A$ and $B$ are independent events. Furthermore, notice that  $P(A) = \frac{1}{2}$, $P(B) = \frac{1}{2}$, and $P(A \cap B) = \frac{1}{4}$. In general, whenever two events are independent, we have

$$
P(A \cap B) = P(A)P(B)
$$

## Introduction to Random Variables 

Our working definition of a random variable will be:

> For a random phenomenon, a **random variable** is a function that assigns a numerical value to each point in the sample space. 

We generally denote random variables by upper case latin letters near the end of the alphabet such as $X$, $Y$, etc. We use the corresponding lower case latin letters to denote that output values  of a random variable. For example, if $X$ is a radom variable and $A$ is some event, then we could write $X(A) = x$. 

Consider again, our example of tossing a coin three times. We can define a random variable by simply returning the number of heads in a given toss. If we denote this random variable by $X$, then $X(\{HHT\}) = 2$ while $X(\{THT\}) = 1$. 

> A **probability distribution** lists the possible outcomes for a random variable and their probabilities. 

For example, if $X$ is our random variable that returns the number of heads out of three tosses of a fair coin, then the probability distribution for $X$ is provided by @tbl-rv-pd.

|Event| $x$ | Probability |
|:-----|:---:|:-----------:|
|$\{TTT\}$| 0  |  $\frac{1}{8}$|
|$\{HTT,THT,TTH\}$| 1  |  $\frac{3}{8}$ |
|$\{HHT,THH,HTH\}$| 2  |  $\frac{3}{8}$ |
|$\{HHH\}$| 3  |  $\frac{1}{8}$ |

: Probability for our example random variable that counts the number of heads out of three tosses of a fair coin. {#tbl-rv-pd}

It is convenient to distinguish between two common types of random variables:

* A **discrete** random variable is one with output values from a discrete, countable set. 

* A **continuous** random variable is one for which the output values form an interval in the set of all real numbers[^2]. That is, continuous random variables have an infinite continuum of possible output values.

[^2]: This is an imprecise and limited definition of continuous random variables but to make the definition precise requires mathematical machinery that is far outside the scope of this course. 

Out coin tossing random variable provides an example of a discrete random variable. Discrete random variables are essentially described by a **probability mass function** (PMF) $p((x)$ that generate the probabilities for the possible outcomes of a random variable. For example, the last two columns of @tbl-rv-pd give a PMF for  our example random variable that counts the number of heads out of three tosses of a fair coin.

In general, if $X$ is a discrete random variable then,

* $0 \leq p(x) \leq 1$ for all output values $x$ for $X$, and

* $\sum_{x}p(x) = 1$, where the sum is taken over all possible output values for $X$.


### Binomial Distributions

Suppose that $X$ is a random variable that takes on values $0,1,\ldots n$ and let $\pi$ denote a real number in the interval $[0,1]$. Then we say that $X$ is a **binomial random variable** if it's PMF is given by

$$
p(x) = \frac{n!}{(n-x)! x!}\pi^{x}(1 - \pi)^{n -x}
$$

For example, if we let $X$ be the random variable that counts the number of heads out $n$ tosses of a coin where the probability of landing heads in a single toss is $\pi$, then $X$ is modeled by a binomial random variable. 

Later we will see how to work with binomial distributions in R.

### Normal Distributions

Continuous random variables are described by a **probability density function** (PDF) $f(x)$ where the total area under the curve $y=f(x)$ and above the $x$-axis is 1. Probably the most well-known type of continuous random variable is the **normal** family of random variables which have a PDF of the form

$$
f(x) = \frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{(x - \mu)^2}{2\sigma^{2}}}
$$

where this time $\pi$ is the constant that is the ratio of a circle's circumference to its diameter. 

Later we will see how to work with normal distributions in R.

### Expected Value and Variance

If $X$ is a discrete random variable with PMF $p(x)$, then the **expected value** of $X$, denoted by $\text{E}[X]$ is defined by

$$
\text{E}[X] = \sum_{x}xp(x)
$$

where the sum is taken over all possible output values for $X$. It is also possible to define the expected value for continuous random variables but for now we will be satisfied to simply also use $\text{E}[X]$ to denote the expected value for a continuous random variable. 

We define the **variance** $\text{Var}[X]$  of a random variable $X$ (continuous or discrete) to be the number

$$
\text{Var}[X] = \text{E}[(X - \text{E}[X])^2]
$$

For a *discrete* random variable $X$ with PMF $p$, we have that

$$
\text{Var}[X] = \sum_{x}(x - \mu)^2p(x)
$$
where the sum is taken over all possible values of $x$ and $\mu = \text{E}[X]$.

The **standard deviation** of a random variable is defined to be the positive square root of its variance. 

#### Properties of Expected Value and Variance

There are some important properties that are helpful to know about expected value and variance. Namely, 

* If $X$ and $Y$ are random variables, then $\text{E}[X + Y] = \text{E}[X] + \text{E}[Y]$. 

* If $a$ is a number and $X$ is a random variable, then $\text{E}[aX] = a\text{E}[X]$.

* If $a$ and $b$ are numbers and $X$ is a random variable, then $\text{Var}[aX + b] = a^2\text{Var}[X]$.

> For any random variable $X$, the probability $P(X \leq x)$ is a function of $x$ that returns the probability that $X$ takes values less or equal to $x$. Such a function is called the **cumulative distribution function** (CDF) for $X$. 

@tbl-rv-cdf includes a column with the output values for the CDF for each value of $x$ for the random variable that counts the number of heads out of three tosses of a fair coin.

|Event| $x$ | Probability | CDF |
|:-----|:---:|:-----------:|:-------------:|
|$\{TTT\}$| 0  |  $\frac{1}{8}$|$\frac{1}{8}$|
|$\{HTT,THT,TTH\}$| 1  |  $\frac{3}{8}$ |$\frac{1}{2}$|
|$\{HHT,THH,HTH\}$| 2  |  $\frac{3}{8}$ |$\frac{7}{8}$|
|$\{HHH\}$| 3  |  $\frac{1}{8}$ |$1$|

: PMF and CDF for our example random variable that counts the number of heads out of three tosses of a fair coin. {#tbl-rv-cdf}

### Expected Value and Variance for Binomial and Normal R.V.s

Suppose that $X$ is a normal random variable with PDF

$$
f(x) = \frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{(x - \mu)^2}{2\sigma^{2}}}
$$

then $\text{E}[X] = \mu$ and $\text{Var}[X] = \sigma^{2}$.

Now, suppose that $X$ is a binomial random variable with PMF

$$
p(x) = \frac{n!}{(n-x)! x!}\pi^{x}(1 - \pi)^{n -x}
$$

then $\text{E}[X] = n\pi$ and $\text{Var}[X] = n\pi(1-\pi)$.

### Some Common Distributions

@tbl-dist-type lists some common distributions together with their type. The name of each distribution is a link to the Wikipedia article on that distribution. 

|    Name    |    Type    |
|:----------:|:----------:|
| [Geometric](https://en.wikipedia.org/wiki/Geometric_distribution)  |  Discrete  |
| [Binomial](https://en.wikipedia.org/wiki/Binomial_distribution)   |  Discrete  |
| [Hypergeometric](https://en.wikipedia.org/wiki/Hypergeometric_distribution) | Discrete |
| [Multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) | Discrete  |
| [Negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) | Discrete |
| [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution)    |  Discrete |
| [Beta](https://en.wikipedia.org/wiki/Beta_distribution)      | Continuous |
| [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution)     | Continuous |
| [Exponential](https://en.wikipedia.org/wiki/Exponential_distribution) | Continuous |
| [Normal](https://en.wikipedia.org/wiki/Normal_distribution)   | Continuous  |
| [Uniform](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) | Continuous           |

: Some common single-variable distributions and their type. {#tbl-dist-type}


## `r icons::icon_style(icons::fontawesome("r-project"),scale=2,fill="steelblue")` Working with Distributions in R

Many probability distributions are implemented in base R and many more can be access via various packages. We will focus on those distributions that are implemented in base R. Suppose that `distname` is the name associated with a distribution that is implemented in R. Then there are typically four R functions associated with that distribution:

1. `ddistname` implements the PMF or PDF for the distribution. 

2. `pdistname` implements the CDF for the distribution. 

3. `qdistname` implements the inverse CDF for the distribution. 

4. `rdistname` implements sampling from the distribution. 

Let's work through some examples together in an RStudio project. You can access the relevant code on either the course learning management system or via [this GitHub repo](https://github.com/jmgraham30/distributions_in_r).

## Preparation for the next lesson

To prepare for the next lesson, please read:

- [Chapter 5](https://datasciencebook.ca/classification1.html) on classification and [Chapter 7](https://datasciencebook.ca/regression1.html) on regression from [Data Science A First Introduction](https://datasciencebook.ca/preface.html) [@timbers2022data].


## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)


