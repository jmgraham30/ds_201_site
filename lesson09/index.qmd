---
title: "Lesson 9"
subtitle: "Introduction to Statistics for Data Science"
author: "JMG"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson09.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| message: false
#| warning: false
#| echo: false

# load packages used in document
library(tidyverse)
library(tidymodels)
library(kableExtra)

theme_set(theme_minimal(base_size = 13))

tidymodels_prefer()
```

## Learning Objectives

After this lesson, students will be able to:

- Describe real-world examples of questions that can be answered with statistical inference.

- Define common population parameters (e.g., mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.

- Define the following statistical sampling terms: population, sample, population parameter, point estimate, and sampling distribution.

- Define bootstrapping.

- Use R to create a bootstrap distribution to approximate a sampling distribution.

## Readings, etc.

1) Read Chapter 10 from from [Data Science: A First Introduction](https://datasciencebook.ca/preface.html) [@timbers2022data]. [View book online](https://datasciencebook.ca/preface.html).

2) You might also want to look at chapters 8 & 9 from [Statistical Inference via Data Science](https://moderndive.com/).



## Overview

Statistical inference is the process of using data analysis to infer properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean "make a prediction, by evaluating an already trained model on a data point not previously seen".

In this lesson, we will get a feel for the inferential method. There are many details of statistics that we will leave for a later course. Further, our approach will be computational, utilizing the bootstrap resampling method.


## Motivating Example

Random variables and their distributions model processes that produce data. For example, a binomial random variable with probability of success $\pi = 0.5$ can be used to model the process of tossing a coin and observing the number of heads. This is an illustration of the domain of probability. Statistical inference is concerned with the inverse problem: given data, what can we say about the process that produced it? For example, given a sample of coin tosses, what can we say about the probability of heads? This is an illustration of the domain of statistics.

**Question:** Given a coin, how can you determine if it is fair or not? Think about how you could approach answering this question. 

Obviously, to address the previous question we should collect data. That is, we should toss the coin some number (probably many) times and record the number of heads. Then, we can use the data to estimate the probability of heads.

**Question:** Suppose that we toss a coin 10 times and observe 7 heads. What is your best guess for the probability of heads? Suppose that we toss a coin 100 times and observe 70 heads. What is your best guess for the probability of heads?

At this point, there are a few things to take note of:

1. When we do statistical inference, we are often trying to estimate a parameter(s) of a distribution. The parameter(s) should be viewed as fixed but with unknown values. In this case, we call the parameter or parameters a **population parameter**. In the coin example, the population parameter is the probability of heads, $\pi$.  

2. We have some process to estimate the population parameter. This process inputs observed data and returns an estimated value for the parameter(s). In this case, we call the estimate a **point estimate**. In the coin example, the point estimate is the number of heads divided by the number of tosses.

3. The point estimate is a random variable. That is, if we were to repeat the experiment, we would get a different point estimate. This is because the observed data is random. In this case, we call the point estimate a **statistic**. In the coin example, the statistic is the number of heads divided by the number of tosses.

4. Since the point estimate is a random variable, it has a distribution. In this case, we call the distribution a **sampling distribution**. In the coin example, the sampling distribution is the distribution of the number of heads divided by the number of tosses.

5. A key problem in statistical inference is to determine or describe the sampling distribution of a statistic. For example, we might be interested to know what is the mean and variance of the sampling distribution. The standard deviation of the sampling distribution of a statistic is called the **standard error**. We call the process of determining the sampling distribution **statistical inference**. In the coin example, we want to determine the sampling distribution of the number of heads divided by the number of tosses.

6. One can use the sampling distribution of a statistic to make statements about the uncertainty of point estimates. 


In mathematical statistics, there is a heavy focus on deriving closed form or asymptotically exact expressions for the sampling distribution of a statistic. This is amazing and beautiful and well worth learning. For this, we highly recommend [@wasserman2004all]. However, this approach is beyond the scope of this course. Further, the precise formulas of mathematical statistics apply in a much too limited range of application for the purposes of modern data science.   

As an alternative to mathematical statistics, one can take a computational approach. That is, use the computer to simulate the sampling distribution of a statistic. This approach is called **bootstrapping**. The bootstrap is a powerful tool that can be used to approximate the sampling distribution of a statistic. It is also a useful tool for hypothesis testing and obtaining confidence intervals. Even our coverage of the bootstrap will be only introductory. To learn more beyond what we cover, we recommend [@efron2022computer].

## Resampling Experiment

Let's start with a simulation. Here's what we will do: Simulate 100 tosses of a fair coin, that is, with $\pi = 0.5$, count the number of heads, then estimate $\pi$ by dividing the number of heads by 100. Then, we will repeat this for 500 times. The code in below does this and displays the results in a table. 

```{r}
set.seed(1234)
coin_df <- tibble(num_heads = rbinom(500,size=100,prob=0.5))

coin_df <- coin_df %>%
  mutate(p_hat = num_heads/100)

coin_df %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```


@fig-coin-plot plots the 500 different estimates for $\pi$.

```{r}
#| label: fig-coin-plot 
#| fig-cap: 500 estimates of $\pi$ from simulated data.
coin_df %>%
  ggplot(aes(x = p_hat)) +
  geom_histogram(bins = 12,color="white") + 
  labs(x = "Estimate",y = "Count") 
```


**Question:** What are your takeaways from our simulation experiment and plot in @fig-coin-plot? What, if anything, do you think we can say about the sampling distribution of $\pi$?

Now, let's try a slightly different simulation. We will do a single round of tossing a fair coin 100 times. Then, we will resample from the 100 tosses with replacement 500 times. For each resample, we will count the number of heads and again estimate $\pi$ by dividing the number of heads by 100. The code in below does this and displays the results in a table. 

```{r}    
set.seed(1234)
coin_sample <- rbinom(100,size=1,prob=0.5)

coin_resample_df <- tibble(num_heads = replicate(500,sum(sample(coin_sample,100,replace=TRUE)))) %>%
  mutate(p_hat_b = num_heads/100)


coin_resample_df %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```


Note the the number of heads in the original sample is `r sum(coin_sample)`. 

@fig-coin-resample-plot plots the 500 different estimates for $\pi$ based on the *resampled* data.

```{r}
#| label: fig-coin-resample-plot
#| fig-cap: 500 estimates of $\pi$ from resampled data.

coin_resample_df %>%
  ggplot(aes(x = p_hat_b)) +
  geom_histogram(bins = 12,color="white") + 
  labs(x = "Estimate",y = "Count") 

```


**Question:** What are your takeaways from our simulation experiment and plot in @fig-coin-resample-plot? How do the results compare and contrast with those of the previous simulation and plot in @fig-coin-plot? 

## Bootstrap

Our resampling simulation in the last section is an example of the **bootstrap**. Notice what we did, we took a sample from the population, then resampled from the sample. This is called **resampling**. The bootstrap is a special case of resampling where the resampling is done with replacement. What we did was generate a bootstrap distribution for our statistic. The idea is, 

> if the sample is representative of the population, then the bootstrap distribution of our statistics obtained by resampling with replacement from the sample should approximate the sampling distribution for our statistic.

While the mean of the bootstrap will be the mean of the sample and not necessarily th mean of the population, the standard deviation of the bootstrap will be a good estimate for the standard error of the statistic. Thus, the bootstrap distribution allows us to assess the uncertainty of an estimate. The power of the bootstrap technique is that it works for any statistic. For example, we can use the bootstrap to estimate the sampling distribution for parameters in models such as the coefficients in a linear regression model. We can even use the bootstrap to estimate the uncertainty in model predictions, even if the model is non-parametric.  


## References

::: {#refs}
:::

::: {.callout-tip collapse="true"}
## Expand for Session Info

```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```
:::

[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width="15%"}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)


