[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS 201: Introduction to Data Science",
    "section": "",
    "text": "Welcome to the course website for DS 201: Introduction to Data Science for Fall 2023.\nThis course is offered at the University of Scranton as taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "DS 201: Introduction to Data Science",
    "section": "License",
    "text": "License\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html#required-readings",
    "href": "syllabus.html#required-readings",
    "title": "Syllabus",
    "section": "Required Readings",
    "text": "Required Readings\n\nR for Data Science by Hadley Wickham & Garrett Grolemund, view the free online version of the text.\nData Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee, view the free online version of the text."
  },
  {
    "objectID": "syllabus.html#additional-references",
    "href": "syllabus.html#additional-references",
    "title": "Syllabus",
    "section": "Additional References",
    "text": "Additional References\n\nAn Introduction to Statistical Learning 2nd Ed. by James, Witten, Hastie, and Tibshirani, freely available online here.\nTree-Based Methods for Statistical Learning by Brandon M. Greenwell, freely available online here."
  },
  {
    "objectID": "syllabus.html#recommended-readings",
    "href": "syllabus.html#recommended-readings",
    "title": "Syllabus",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nHands-On Programming with R by Garrett Grolemund, view the free online version of the text.\nTelling Stories with Data by Rohan Alexander, view the free online version of the text.\nData Visualization A Practical Introduction by Healy, view the free online version of the text.\nStatistical Inference via Data Science A ModernDive into R and the Tidyverse by Chester Ismay & Albert Y. Kim, view the free online version of the text."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the repository.\nThe R Graph Gallery, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "links.html#texts",
    "href": "links.html#texts",
    "title": "Links",
    "section": "Texts",
    "text": "Texts\n\nR Related\n\nggplot2: Elegant Graphics for Data Analysis (3e), view the free online version of the text\nSpatial Data Science with applications in R, view the free online version of the text.\nAnalyzing US Census Data: Methods, Maps, and Models in R textbook, view the free online version of the text.\nTidy Finance with R, view the free online version of the text.\nOutstanding User Interfaces with Shiny textbook, view the free online version of the text.\nCrime by the Numbers: A Criminologist’s Guide to R textbook, view the free online version of the text.\n\n\n\nPython Related\n\nPython for Data Analysis, 3E, view the free online version of the text"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "DS 201: Introduction to Data Science",
    "section": "",
    "text": "Welcome to the course website for DS 201: Introduction to Data Science for Fall 2023.\nThis course is offered at the University of Scranton as taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "links.html#websites",
    "href": "links.html#websites",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the repository.\nThe R Graph Gallery, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "lesson01/index.html#readings-etc.",
    "href": "lesson01/index.html#readings-etc.",
    "title": "Lesson 1",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead the preface of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). View the free online version of the text..\nSkim the README for the Tidy Tuesday data repository (Mock 2018). View the repository. Throughout the semester, we will use example data from the Tidy Tuesday data repository."
  },
  {
    "objectID": "lesson01/index.html#references",
    "href": "lesson01/index.html#references",
    "title": "Lesson 1",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. n.d. “Telling Stories with Data: With Applications in r.”\n\n\nChristian, Brian. 2020. The Alignment Problem: Machine Learning and Human Values. WW Norton & Company.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020. Swirl: Learn r, in r. https://CRAN.R-project.org/package=swirl.\n\n\nMock, J. Thomas. 2018. “Tidy Tuesday.” GitHub Repository. https://github.com/rfordatascience/tidytuesday; GitHub.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-08-27\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder   * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra  * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson01/index.html",
    "href": "lesson01/index.html",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nGive a general definition of data science.\nExplain what is meant by reproducible and auditable workflows.\nUse the Gapminder online tools ( view website) to explore data on certain social and economic issues.\nExplain and give examples of numerical and categorical data types.\nDo basic arithmetic with R."
  },
  {
    "objectID": "lesson01/index.html#learning-objectives",
    "href": "lesson01/index.html#learning-objectives",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nGive a general definition of data science.\nExplain what is meant by reproducible and auditable workflows.\nUse the Gapminder online tools ( view website) to explore data on certain social and economic issues.\nExplain and give examples of numerical and categorical data types.\nDo basic arithmetic with R."
  },
  {
    "objectID": "links.html#videos",
    "href": "links.html#videos",
    "title": "Links",
    "section": "Videos",
    "text": "Videos\n\nGetting started with Quarto YouTube video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nDatasheets for Datasets video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nHow to Make a Custom R Package video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nThe Alignment Problem: Machine Learning and Human Values with Brian Christian, watch the video on YouTube."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nAn introduction to basic data science workflow following current best practices. This course will introduce students to computational or algorithmic ways to think about and learn from data. Emphasis will be placed on data visualization, exploratory data analysis, and foundational modeling principles and techniques implemented using an appropriate programming language.\n\n\nPrerequisites\n\nMath Placement PT score of 14 or higher, or ALEKS score of 76 or higher, or MATH 114, or permission of instructor"
  },
  {
    "objectID": "syllabus.html#student-learning-objectives-and-assessment",
    "href": "syllabus.html#student-learning-objectives-and-assessment",
    "title": "Syllabus",
    "section": "Student Learning Objectives and Assessment:",
    "text": "Student Learning Objectives and Assessment:\n\n\n\n\nTable 1: Course objectives and assessment.\n\n\nCourse SLO\nAssessment\n\n\n\n\nAfter completing this course, students will be able to import, format, and transform common data-set types programmatically and build effective visualizations of data\nHomework, Data Package Assignment, and Project\n\n\nAfter completing this course, students will be able to apply appropriate exploration and modeling techniques to learn from data.\nHomework and Project\n\n\nAfter completing this course, students will be able to present and communicate results obtained via data analysis in an effective manner.\nHomework, Data Package Assignment, and Project"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nGrade Policy\nThe overall course grade will be based on (roughly twelve) weekly homework assignment totaling 30% of the overall course grade, a data package assignment totaling 20% of the overall course grade, and a semester project totaling 50% of the overall course grade.\n\n\nGrade Scale\nLetter grades will be assigned based on the following scale:\n\n\n\n\nTable 2: Letter grade scale.\n\n\nGrade Range\nLetter Grade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n83-86\nB\n\n\n80-82\nB-\n\n\n76-79\nC+\n\n\n72-75\nC\n\n\n69-71\nC-\n\n\n65-68\nD+\n\n\n60-64\nD\n\n\n&lt;60\nF"
  },
  {
    "objectID": "syllabus.html#weekly-schedule",
    "href": "syllabus.html#weekly-schedule",
    "title": "Syllabus",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nWeek 1: Introduction to data\nWeek 2: Basic programming for data science\nWeek 3: Data wrangling; Project component 1 due\nWeek 4: Creating data packages\nWeek 5: Data visualization\nWeek 6: Exploratory data analysis; Project component 2 due\nWeek 7: Intermediate programming for data science\nWeek 8: Introduction to probability for data science\nWeek 9: Introduction to statistics for data science; Project component 3 due\nWeek 10: Classification\nWeek 11: Regression\nWeek 12: Clustering; Project component 4 due\nWeek 13: Introduction to machine learning\nWeek 14: Ethical considerations in data science\nWeek 15: Putting everything together; Project final version due"
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important Dates",
    "text": "Important Dates\n\n\n\n\nTable 3: Important dates.\n\n\nEvent\nDate\n\n\n\n\nClasses begin\n08-28\n\n\nLast day to add classes\n90-01\n\n\nHoliday, no classes\n09-04\n\n\n100% tuition refund\n09-06\n\n\nDrop (no grade)\n09-27\n\n\nFall break\n10-07 to 10-10\n\n\nMid-semester\n10-18\n\n\nWithdraw with W\n11-10\n\n\nThanksgiving break\n11-22 to 11-26\n\n\nLast week\n12-05 to 12-11\n\n\nFinals\n12-12 to 12-16"
  },
  {
    "objectID": "syllabus.html#students-with-disabilities",
    "href": "syllabus.html#students-with-disabilities",
    "title": "Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nReasonable academic accommodations may be provided to students who submit relevant and current documentation of their disability. Students are encouraged to contact the Center for Teaching and Learning Excellence (CTLE) at disabilityservices@scranton.edu or (570) 941-4038 if they have or think they may have a disability and wish to determine eligibility for any accommodations. For more information, please visit http://www.scranton.edu/disabilities."
  },
  {
    "objectID": "syllabus.html#writing-center-services",
    "href": "syllabus.html#writing-center-services",
    "title": "Syllabus",
    "section": "Writing Center Services",
    "text": "Writing Center Services\nThe Writing Center focuses on helping students become better writers. Consultants will work one-on-one with students to discuss students’ work and provide feedback at any stage of the writing process. Scheduling appointments early in the writing progress is encouraged.\nTo meet with a writing consultant, call (570) 941-6147 to schedule an appointment, or send an email with your available meeting times, the course for which you need assistance, and your phone number to: writing-center@scranton.edu. The Writing Center does offer online appointments for our distance learning students."
  },
  {
    "objectID": "syllabus.html#academic-honesty-and-integrity",
    "href": "syllabus.html#academic-honesty-and-integrity",
    "title": "Syllabus",
    "section": "Academic Honesty and Integrity",
    "text": "Academic Honesty and Integrity\nEach student is expected to do their own work. It is also expected that each student respect and abide by the Academic Code of Honesty as set forth in the University of Scranton student handbook. Conduct that violates the Academic Code of Honesty includes plagiarism, duplicate submission of the same work, collusion, providing false information, unauthorized use of computers, theft and destruction of property, and unauthorized possession of tests and other materials. Steps taken in response to suspected violations may include a discussion with the instructor, an informal meeting with the dean of the college, and a hearing before the Academic Dishonesty Hearing Board. Students who are found to have violated the Code will ordinarily be assigned the grade F by the instructor and may face other sanctions. The complete Academic Code of Honesty is located on the University website at https://www.scranton.edu/academics/wml/acad-integ/acad-code-honesty.shtml."
  },
  {
    "objectID": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "href": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "title": "Syllabus",
    "section": "My Reporting Obligation as a Responsible Employee",
    "text": "My Reporting Obligation as a Responsible Employee\nAs a faculty member, I am deeply invested in the well-being of each student I teach. I am here to assist you with your work in this course. Additionally, if you come to me with other non-course-related concerns, I will do my best to help. It is important for you to know that all faculty members are required to report incidents of sexual harassment or sexual misconduct involving students. This means that I cannot keep information about sexual harassment, sexual assault, sexual exploitation, intimate partner violence or stalking confidential if you share that information with me. I will keep the information as private as I can but am required to bring it to the attention of the University’s Title IX Coordinator, Elizabeth M. Garcia, or Deputy Title IX Coordinator, Diana M. Collins, who, in conversation with you, will explain available support, resources, and options. I will not report anything to anybody without first letting you know and discussing choices as to how to proceed. The University’s Counseling Center (570-941-7620) is available to you as a confidential resource; counselors (in the counseling center) do not have an obligation to report to the Title IX Coordinator."
  },
  {
    "objectID": "syllabus.html#non-discrimination-statement",
    "href": "syllabus.html#non-discrimination-statement",
    "title": "Syllabus",
    "section": "Non-discrimination Statement",
    "text": "Non-discrimination Statement\nThe University is committed to providing an educational, residential, and working environment that is free from harassment and discrimination. Members of the University community, applicants for employment or admissions, guests, and visitors have the right to be free from harassment or discrimination based on race, color, religion, ancestry, gender, sex, pregnancy, sexual orientation, gender identity or expression, age, disability, genetic information, national origin, veteran status, or any other status protected by applicable law.\nStudents who believe they have been subject to harassment or discrimination based on any of the above class of characteristics, or experience sexual harassment, sexual misconduct or gender discrimination should contact Elizabeth M. Garcia, Title IX Coordinator, (570) 941-6645 elizabeth.garcia2@scranton.edu, Deputy Title IX Coordinators Diana M. Collins (570) 941-6645 diana.collins@scranton.edu, or Ms. Lauren Rivera, AVP for Student Life and Dean of Students, at (570)941-7680 lauren.rivera@scranton.edu. The United States Department of Education’s Office for Civil Rights (OCR) enforces Title IX. Information regarding OCR may be found at &lt;www.ed.gov/about/offices/list/ocr/index.html&gt;\nThe University of Scranton Sexual Harassment and Sexual Misconduct Policy can be found online at https://www.scranton.edu/diversity. All reporting options and resources are available at https://www.scranton.edu/CARE.\n\nAbout Pronouns\nIt is easy to make assumptions about an individual’s pronouns, but we try not to! Please tell us in class or via a private email if you would like to let us know what your pronouns are, if/when you would like us (and others) to use them, and certainly feel free to correct us or others if we make a mistake. Using the pronouns that a person has indicated they prefer is considered both professional and polite, and as such we ask that all members of our class use the appropriate pronouns.\nIf you have questions about this, please feel free to look up more information here (https://www.mypronouns.org/) or email jason.graham@scranton.edu with any questions."
  },
  {
    "objectID": "syllabus.html#student-mental-health-suggestions-and-resources",
    "href": "syllabus.html#student-mental-health-suggestions-and-resources",
    "title": "Syllabus",
    "section": "Student Mental Health: Suggestions and Resources",
    "text": "Student Mental Health: Suggestions and Resources\nMany students experience mental health challenges at some point in college. Struggles vary and might be related to academics, anxiety, depression, relationships, grief/loss, substance abuse, and other challenges. There are resources to help you and getting help is the smart and courageous thing to do.\n\nCounseling Center (6th Floor O’Hara Hall; 570-941-7620) – Free, confidential individual and group counseling is available on campus.\nTeletherapy – For students who wish to access therapy via video, phone, and/or chat, the University offers a teletherapy resource. Please contact the Counseling Center (570-941-7620) to inquire about teletherapy.\nMental Health Screenings – Confidential, online “check up from your neck up” to help you determine if you should connect with a mental health professional.\nDean of Students Office (201 DeNaples Center; 570-941-7680) – Private support and guidance for students navigating personal challenges that may impact success at the University"
  },
  {
    "objectID": "syllabus.html#final-note",
    "href": "syllabus.html#final-note",
    "title": "Syllabus",
    "section": "Final Note",
    "text": "Final Note\nThe instructor reserve the right to modify this syllabus; students will immediately be notified of any such changes and an updated syllabus will be made available to the class via the course learning management system."
  },
  {
    "objectID": "lesson01/index.html#what-is-data-science",
    "href": "lesson01/index.html#what-is-data-science",
    "title": "Lesson 1",
    "section": "What is Data Science?",
    "text": "What is Data Science?\nFor this course, we think of data science as a methodology or set of methodologies for gaining insight from data. Data can be pretty much any type of organized information. In this course, data will often be organized in a tabular format such as is common with spreadsheets. However, text, images, videos, and sound are other types of data that data scientists often work with. Further, data may have a temporal or spatial component.\nIt is important to note that data science is an interdisciplinary endeavor. Data science brings together mathematics and statistics, computer science and machine learning, and a domain of application or expertise. This is illustrated by Figure 1.\n\n\n\nFigure 1: Data science is an inherently interdisciplinary field.\n\n\n\nData Science Workflow\nIn this course, we emphasize a certain workflow for typical data science tasks. Generally, the steps in the workflow are:\n\nGather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application.\nDocument the data and data collection process.\n\nIt is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See The Alignment Problem by Brian Christian for a thoughtful discussion on the issues related to these types of considerations (Christian 2020).\n\nImport the data for analysis.\nExplore and clean the data. Data visualization is essential at this step.\nGenerate initial insight or more detailed questions.\nDecide what type(s) of analysis or analyses are to be performed.\n\nMake sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See Table 1.1 from (Timbers, Campbell, and Lee 2022) for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table here.\n\nAssess the analysis.\nAt this stage, it may be necessary to repeat steps 1 - 7.\nReport your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis.\n\nIt is essential the our data science workflow be reproducible and auditable. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility). Figure 2 illustrates the concept of reproducibility.\n\n\n\nFigure 2: Artwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#exploring-data-an-introduction-through-gapminder",
    "href": "lesson01/index.html#exploring-data-an-introduction-through-gapminder",
    "title": "Lesson 1",
    "section": "Exploring Data: An Introduction Through Gapminder",
    "text": "Exploring Data: An Introduction Through Gapminder\nLet’s begin our adventure in data science by engaging with some of the interactive tools on the Gapminder website, View Gapminder. The Gapminder organization is a non-profit entity that seeks to use data to educate people about large-scale social issues. Gapminder has a particular focus on addressing certain types of common misconceptions about the world.\nView the interactive data tools by Gapminder.\nQuestion: What information is conveyed by the visualization shown at this link? How are colors used in the visualization?\nQuestion What is the relationship between income and life expectancy based on the initial bubble plot? How do income and life expectancy each change over time? Does the relationship between income and life expectancy change over time?\nQuestion: Do you think the visualizations on the Gapminder sight are useful and effective? If so, what makes them useful or effective. If not, what changes would you make to make them more useful or effective?"
  },
  {
    "objectID": "lesson01/index.html#classification-of-basic-data-types",
    "href": "lesson01/index.html#classification-of-basic-data-types",
    "title": "Lesson 1",
    "section": "Classification of Basic Data Types",
    "text": "Classification of Basic Data Types\nA small portion of the data available from Gapminder is shown in Table 1 below: 2\n\n\n\n\nTable 1: Portion of the Gapminder data set\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\n\nThere are several useful things to note about this data:\n\nEach row corresponds to a unique observation.\nEach column is a unique feature or variable.\nEvery cell contains a single value.\nSome of the information in the data set is quantitative while other information is qualitative.\n\nA data set that satisfies properties 1 - 3 is said to be tidy or in tidy format. We will often find it very useful to work with data that conforms to the tidy format.\nWe refer to variables made up of quantitative information as numerical and variables made up of qualitative information as categorical. For example, in the Gapminder data, the country and continent variables are categorical while the year, life expectancy, population, and per capita GDP are all numerical variables. The type of variable(s) often influences the type of visualization or analysis that is used to explore and model data.\nWe can further classify the type of a variable by the following classifications:\n\nNumerical (quantitative):\n\nContinuous: Can take on any value (possibly over a specific range) and any value between two values. Typically will involve decimal values. Physical measurements are common types of continuous numerical data.\nDiscrete: Any numerical type that is not continuous. Often whole numbers. Counts are a common type of discrete numerical data.\n\nCategorical (qualitative):\n\nNominal: Lacks any particular ordering. For example, eyecolor would be nominal categorical data.\nOrdinal: Possesses a particular ordering. For example, the place (first, second, third, etc.) that a sports team comes in at the end of a competition or tournament.\nBinary: A categorical variable with only two possible outcomes. Think yes/no, true/false, etc.\n\n\nFigure 3 illustrates numerical variables and the distinction between continuous and discrete numerical variables, while Figure 4 illustrates categorical variables and the distinction between nominal, ordinal, and binary categorical variables.\n\n\n\nFigure 3: Numerical variables and the distinction between a continuous numerical variable and a discrete numerical variable. Artwork by Allison Horst\n\n\n\n\n\nFigure 4: Categorical variables and the distinction between nominal, ordinal, and binary categorical variables. Artwork by Allison Horst\n\n\nOne thing to be careful about is that numbers can and often are used to label the outcomes for a categorical variable. For example, suppose we collect data about students living in a particular dormitory. We may use 1 to denote that a student is enrolled as a first-year student, 2 to denote that a student is enrolled as a second-year student, etc. Even though we’ve used number, the year a student is enrolled as is really a categorical variable.\nSometimes, there may even be an ambiguity in the type for a variable. For example, suppose we are collecting data on commercial buildings in a particular city and for each building in the data set, we record (among other things) the number of stories of the building using 1 to denote a building with one story, 2 to denote a building with two stories, etc. On the one hand, we could view our stories variable as a count and hence a discrete numerical variable. On the other hand, it might be better to think of the stories variable as a ordinal categorical variable. One way to decide which is more appropriate is to think about if it makes sense or is desirable to take a average or not. If you want to take a average, then the variable must be numerical."
  },
  {
    "objectID": "lesson01/index.html#a-tiny-bit-of-r",
    "href": "lesson01/index.html#a-tiny-bit-of-r",
    "title": "Lesson 1",
    "section": "A Tiny Bit of R",
    "text": "A Tiny Bit of R\n\n\n\nArtwork by Allison Horst\n\n\n\nWhy R?\nThe R language (R Core Team 2023) for statistical computing is one of the most popular computing tools for data science, among the other most popular are Python and Julia. Some of the strengths of R include\n\nfree and open source which facilitates reproducibility and auditability,\necosystem of packages that greatly extend the functionality of R,\nrmarkdown (Xie, Dervieux, and Riederer 2020) and Quarto frameworks for literate programming enable presentation and communication of data analyses and facilitate reproducibility and auditability,\nRStudio integrated development environment (IDE) by Posit enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages,\na strong and collaborative user community, see R Community Explorer website here.\n\nAs the course progresses, we will learn a lot of R. For this lesson, we will start with something very simple, that is, doing basic calculations with R.\n\n\nCalculations with R\nIn R, it is easy to perform basic arithmetic operations such as addition, subtraction, multiplication, division, exponentiation, and modular division. Each of these is illustrated in the following code blocks:\n\n# any line starting with # is a comment and is not run\n# addition\n1.6 + 2.31\n\n[1] 3.91\n\n\n\n# subtraction\n4.87 - 2.31\n\n[1] 2.56\n\n\n\n# multiplication\n1.6 * 2.31\n\n[1] 3.696\n\n\n\n# division\n6.0 / 2.31\n\n[1] 2.597403\n\n\n\n# exponentiation\n2^3\n\n[1] 8\n\n\n\n# modular division, a %% b returns the remainder after\n# dividing a by b\n\n7 %% 2\n\n[1] 1\n\n\nYou have to be careful with the order of operations, use parentheses to be precise about intended order of operations. For example,\n\n(6 / 3) - 1\n\n[1] 1\n\n\ndivides 6 by three and then subtracts 1, while\n\n6 / (3 - 1)\n\n[1] 3\n\n\nsubtracts 1 from 3 and divides 6 by the result. While the following code runs and has a specific meaning to the computer, it is ambiguous to the human reader and should be modified appropriately with parentheses:\n\n6 / 3 - 1\n\n[1] 1\n\n\n\n\nVariables and Assignment\nIn R programming, a variable is a name given to a R object such as a numeric value that can be used to store, reuse, or modify that object. In R, variable assignments are made use the assignment operator &lt;-. For example,\n\nmy_variable &lt;- 2.5\n\nassigns the numeric value 2.5 to the variable my_variable. Now, we can peform operations to my_variable, for example,\n\n(2 * my_variable)\n\n[1] 5\n\n(my_variable^3)\n\n[1] 15.625\n\n\nMany programming languages use = for assignment. While it is valid to use = in R, best practices dictate using &lt;-.\nSee the tidyverse style guide here for best practices in naming objects in R.\nIf you want to engage further with R programming, we highly recommend the swirl package (Kross et al. 2020) for learning R in R. To install, load, and use swirl, run the following commands in the R console:\n\n# install swirl\ninstall.packages(\"swirl\") # only need to run once\n# load swirl \nlibrary(swirl) # must run in every new R session\n# start swirl\nswirl()"
  },
  {
    "objectID": "lesson01/index.html#readings-etc.-1",
    "href": "lesson01/index.html#readings-etc.-1",
    "title": "Lesson 1",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead the preface of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). This textbook is available online here.\nThroughout the semester, we will use example data from the Tidy Tuesday data repository (Mock 2018). Click link here to go to the Tidy Tuesday data repository."
  },
  {
    "objectID": "lesson01/index.html#preparation-for-the-next-lesson",
    "href": "lesson01/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 1",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nPrior to starting the next lesson, please read Chapter 1 of (Timbers, Campbell, and Lee 2022), link here. It is also recommended that you read the section on reproducible workflows from (Alexander, n.d.), link here"
  },
  {
    "objectID": "lesson01/index.html#a-tiny-bit-of",
    "href": "lesson01/index.html#a-tiny-bit-of",
    "title": "Lesson 1",
    "section": "A Tiny Bit of  ",
    "text": "A Tiny Bit of  \n\n\n\nArtwork by Allison Horst\n\n\n\nWhy R?\nThe R language (R Core Team 2023) for statistical computing is one of the most popular computing tools for data science, among the other most popular are Python and Julia. Some of the strengths of R include\n\nfree and open source which facilitates reproducibility and auditability,\necosystem of packages that greatly extend the functionality of R,\nrmarkdown (Xie, Dervieux, and Riederer 2020) and Quarto frameworks for literate programming enable presentation and communication of data analyses and facilitate reproducibility and auditability,\nRStudio integrated development environment (IDE) by Posit enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages,\na strong and collaborative user community, see R Community Explorer website, view the website..\n\nAs the course progresses, we will learn a lot of R. For this lesson, we will start with something very simple, that is, doing basic calculations with R.\n\n\nCalculations with R\nIn R, it is easy to perform basic arithmetic operations such as addition, subtraction, multiplication, division, exponentiation, and modular division. Each of these is illustrated in the following code blocks:\n\n# any line starting with # is a comment and is not run\n# addition\n1.6 + 2.31\n\n[1] 3.91\n\n\n\n# subtraction\n4.87 - 2.31\n\n[1] 2.56\n\n\n\n# multiplication\n1.6 * 2.31\n\n[1] 3.696\n\n\n\n# division\n6.0 / 2.31\n\n[1] 2.597403\n\n\n\n# exponentiation\n2^3\n\n[1] 8\n\n\n\n# modular division, a %% b returns the remainder after\n# dividing a by b\n\n7 %% 2\n\n[1] 1\n\n\nYou have to be careful with the order of operations, use parentheses to be precise about intended order of operations. For example,\n\n(6 / 3) - 1\n\n[1] 1\n\n\ndivides 6 by three and then subtracts 1, while\n\n6 / (3 - 1)\n\n[1] 3\n\n\nsubtracts 1 from 3 and divides 6 by the result. While the following code runs and has a specific meaning to the computer, it is ambiguous to the human reader and should be modified appropriately with parentheses:\n\n6 / 3 - 1\n\n[1] 1\n\n\n\n\nVariables and Assignment\nIn R programming, a variable is a name given to a R object such as a numeric value that can be used to store, reuse, or modify that object. In R, variable assignments are made use the assignment operator &lt;-. For example,\n\nmy_variable &lt;- 2.5\n\nassigns the numeric value 2.5 to the variable my_variable. Now, we can peform operations to my_variable, for example,\n\n(2 * my_variable)\n\n[1] 5\n\n(my_variable^3)\n\n[1] 15.625\n\n\nMany programming languages use = for assignment. While it is valid to use = in R, best practices dictate using &lt;-.\nSee the tidyverse style guide here for best practices in naming objects in R.\nIf you want to engage further with R programming, we highly recommend the swirl package (Kross et al. 2020) for learning R in R. To install, load, and use swirl, run the following commands in the R console:\n\n# install swirl\ninstall.packages(\"swirl\") # only need to run once\n# load swirl \nlibrary(swirl) # must run in every new R session\n# start swirl\nswirl()"
  },
  {
    "objectID": "syllabus.html#use-of-ai",
    "href": "syllabus.html#use-of-ai",
    "title": "Syllabus",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial intelligence (AI) can be an effective tool in data science. For example, AI-based programming assistants like GitHub Copilot or generative model platforms like ChatGPT now help programmers and developers to write better code in less time. Learning to use AI is essentially becoming a basic skill for the modern data scientist. Because of this, I do not want to completely discourage the use of AI assistance.\nHowever, I ask that you avoid using AI platforms or tools in a manner that is inappropriate in the context of this course. This course teaches a variety of concepts, skills, and critical thinking. Using AI in such a way as to avoid learning, developing skills, or critical thinking is not appropriate. If you find yourself using AI to look up answers, search for complete solutions to problems, or things like this, then your use of AI is not acceptable. It might be helpful to think of AI as an analog to a calculator. If the goal of an assignment is for you to demonstrate that you can do a certain calculation, then using a calculator is not appropriate. On the other hand, if the goal of an assignment is for you to demonstrate that you can solve a problem for which a minor step involves doing a calculation, then using a calculator is okay. AI should be treated analogously.\nIn particular, it is expected that students will be able to explain independently and in detail what any line of code submitted as part of an assignment this semester does. Also, it is expected that students can explain independently and in detail the solution to any problem submitted as part of an assignment this semester.\nIf you have any doubts about your use of AI, then either ask the instructor if your use of AI is acceptable or just don’t use AI."
  },
  {
    "objectID": "lesson01/index.html#course-overview",
    "href": "lesson01/index.html#course-overview",
    "title": "Lesson 1",
    "section": "Course Overview",
    "text": "Course Overview\nThis course provides an introduction to data science. Broadly, we will cover concepts, skills, and methods for gaining insight from data. The things you learn in the class will be applicable in a variety of different areas, professions, and even other courses.\nThere is a website for the course, view the course website. For course logistics, see the official course syllabus, view the syllabus website. Assignments and other information specific to the course in a given semester will be posted on the course learning management system (LMS). The course website provides links to many additional resources, view the links page.\nWhile we will regularly refer to several texts (most of which have been published online as open access materials) throughout the course, most of the content will be delivered via “notebooks” like the one you’re reading now1 that intermix text, mathematical notation, figures, media, programming language code, and web links. In some cases, you will be asked to go through the notebooks on your own and sometimes we will go through the notebooks together during class. Either way, any time you encounter code in a notebook, it is expected that you will take the time to run any code (mostly by copying and pasting) for yourself. The only way to master the material is through active participation."
  },
  {
    "objectID": "lesson01/index.html#footnotes",
    "href": "lesson01/index.html#footnotes",
    "title": "Lesson 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe notebooks are created using Quarto and R markdown, topics that we will cover in more detail later.↩︎\nThis data is made available in R through the gapminder package. In order to access this data in R, is it necessary to install and load the gapminder package. We will discuss installing and loading R packages in detail soon.↩︎"
  },
  {
    "objectID": "lesson01/index.html#some-data-resources",
    "href": "lesson01/index.html#some-data-resources",
    "title": "Lesson 1",
    "section": "Some Data Resources",
    "text": "Some Data Resources\nFor future reference, we list some resources that might be useful for finding data sets to work with throughout the course.\n\nGapminder and the gapminder R package.\nUCI machine learning repository\nKaggle\nTidy Tuesday data repository\nAppendix B of Telling Stories with Data (Alexander, n.d.) lists a large number of data resources"
  },
  {
    "objectID": "syllabus.html#further-reading",
    "href": "syllabus.html#further-reading",
    "title": "Syllabus",
    "section": "Further Reading",
    "text": "Further Reading\n\nAn Introduction to Statistical Learning 2nd Ed. by James, Witten, Hastie, and Tibshirani, view the free online version of the text.\nHands-On Machine Learning with R, view the free online version of the text.\nLinks to additional resources related to the course material will be posted on the course website. View the resources link."
  },
  {
    "objectID": "syllabus.html#homework-assignments",
    "href": "syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\nThere will be roughly 12 weekly homework assignments throughout the semester. These assignments with due dates will be posted to the course learning management system. Homework problems will be a mix of hand-written and computer assignments and the problems will relate to the material covered in lectures and readings."
  },
  {
    "objectID": "syllabus.html#data-package-assignment",
    "href": "syllabus.html#data-package-assignment",
    "title": "Syllabus",
    "section": "Data Package Assignment",
    "text": "Data Package Assignment\nThe data package assignment, making up 20% of the course grade asks students to curate a data set according the best practices for reproducible data analyses. A complete data package assignment should consist of\n\nAn appropriate raw data set(s) with original source fully documented.\nAn appropriately cleaned and processed version of the original raw data set. All code or other files used to clean and process the raw data must be included as part of the final data package submission.\nAn appropriate data sheet to accompany the data package.\nA public folder, repository, R package, or other container that can be used to make your data set and associated files and documentation available to others.\n\nFurther details on the semester project such as the parameters of the assignment and a grade rubric will be posted on the course learning management system."
  },
  {
    "objectID": "syllabus.html#semester-project",
    "href": "syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\nThe semester project will incorporate all the components of a data analysis covered in the course throughout the semester applied to a data set of your choosing pending approval by the instructor. Various components of the project will be due at different times but you will have the opportunity to revise some components prior to the submission of the final product.\nA complete project, counting for 50% of the overall course grade will consist of the following:\n\nA data sheet describing the essential information about your chosen data set.\nAn initial exploratory data analysis for your chosen data set.\nAn appropriate analysis of your chosen data set with the goal to address a specific research question.\nA project report developed using Quarto. You may view the rendered version of an example report here.\nSlides for a presentation summarizing your project. You will not actually present the slides. A rendered version of examples slides may be viewed here.\nA GitHub repository containing all code (appropriately documented) written and used in your project. An example project repository may be viewed here.\n\nYour project report and presentation should be written as if it is addressed to a stake holder with some subject matter knowledge in the domain of application but not necessarily with a quantitative or programming background. Further details on the semester project such as the parameters of the assignment and a grade rubric will be posted on the course learning management system."
  },
  {
    "objectID": "lesson02/index.html",
    "href": "lesson02/index.html",
    "title": "Lesson 2",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nSet up an RStudio project.\nWork in a Quarto document.\nRead tabular data into R.\nUse the glimpse command and other R functions to examine data read into R.\nDescribe what version control is and why data analysis projects can benefit from it."
  },
  {
    "objectID": "lesson02/index.html#learning-objectives",
    "href": "lesson02/index.html#learning-objectives",
    "title": "Lesson 2",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nSet up an RStudio project.\nWork in a Quarto document.\nRead tabular data into R.\nUse the glimpse command and other R functions to examine data read into R.\nDescribe what version control is and why data analysis projects can benefit from it."
  },
  {
    "objectID": "lesson02/index.html#readings-etc.",
    "href": "lesson02/index.html#readings-etc.",
    "title": "Lesson 2",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapter 2 of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). View the free online version of the text..\nThe TidyTuesday data repository (Mock 2018)."
  },
  {
    "objectID": "lesson02/index.html#rstudio-projects",
    "href": "lesson02/index.html#rstudio-projects",
    "title": "Lesson 2",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nA RStudio project is simply a folder or directory on a computer that contains a .Rproj file. A project makes it so that your work has its own working directory, workspace, history, and source documents. Using a project facilitates reproducible and auditable analyses because it keeps all the relevant files for a specific analysis together in one place making it easier to share. A RStudio project should correspond to a single project that you are working on. For example, you should use a RStudio project for your semester project for the course.\nThe instructions for how to create a project are given in the online Posit support documents, view the projects webpage. Alternatively, view this video:\n\n\n\n\n\n\n\n\n\n\nSome Project Tips and Tricks\n\nWhen you create a project, the name of your project and the name of the corresponding folder will be the same so, use good naming conventions for projects. In particular, avoid the use of spaces or symbols. Make sure the name is descriptive and easy to remember but not too long.\nIf there is data for your project, it is useful to create a subfolder of your package folder called data where you should save all the data files relevant to your project.\nIf your project involves a lot of coding in R, its a good idea to create a subfolder of your package folder called R where you should save all the R script files relevant to your project."
  },
  {
    "objectID": "lesson02/index.html#quarto-documents",
    "href": "lesson02/index.html#quarto-documents",
    "title": "Lesson 2",
    "section": "Quarto Documents",
    "text": "Quarto Documents\nLiterate programming is a programming practice in which the programmer includes computer code, normal text, figures, mathematical notation, etc. together into a single document. The approach is routinely in data science for the purposes of reproducible research. There are several tools for literate programming that are very popular among data scientists. These include but are not limited to Jupyter notebooks and rmarkdown.\nA very recent addition to the literate programming toolkit is Quarto. Quarto brings together Jupyter notebooks, rmarkdown, and a number of other features. Throughout this course we will use Quarto which is included with the most recent versions of RStudio. To start using Quarto, we recommend:\n\nGetting started with Quarto YouTube video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\nIn class, we will go through the steps of creating a RStudio project and a Quarto document together."
  },
  {
    "objectID": "lesson02/index.html#reading-data-into-r",
    "href": "lesson02/index.html#reading-data-into-r",
    "title": "Lesson 2",
    "section": "Reading Data into R",
    "text": "Reading Data into R\n\nBasic Concepts\nThe main focus of this lesson is to show different common ways to read tabular data into R. Recall that tabular data is data that corresponds to a spreadsheet in which data is arranged into columns and rows. There are three basic things you need to know to read data into R:\n\nThe data format and file type.\n\nFor data to be tabular in structure, one must use some method to distinguish entries that belong to different columns and rows. The two most common ways are to separate entries using either a comma or a tab. Tabular data formatted using comma-separated values are saved as .csv files while tabular data formatted using tab-separated values are save as .tsv files. The other very common way to format and save tabular data is using Excel spreadsheets.\n\nWhere the data is saved and will be read in from.\n\nThere are two basic locations in which one may save and store a data file: locally on a computer, remotely. To read in data that is stored locally on a computer that you have direct access to, you just need to know the path to the data file that you want to read in. To read in data that is stored remotely, you need to know the url address for the remote location of the data.\n\nThe appropriate R function to use depending on the 1. and 2.\n\nThe readr package included as part of the tidyverse family of R packages contains functions read_csv and read_tsv for reading in .csv or .tsv files, respectively. The readxl package contains functions that can be used to read in tabular data formatted and saved as Excel spreadsheets.\nNote: There are also many other data structures, file types, and R packages or functions for reading in data. We can’t possibly cover all the possible variations in class but once you know how to read in one data type into R, it is usually straightforward to figure out how to do it with some other data or file type.\n\n\nExamples\nThe following examples assume that you are working in an active RStudio project that contains a subfolder titled data and that you have downloaded and saved the files happiness_report.csv and happiness_report.xlsx to the data folder. These are all data files for tabular data that ranks countries on happiness based on rationalized factors like economic growth, social support, etc. The data was released by the United Nations at an event celebrating International Day of Happiness.\nBefore we can read these data files into R, we need to make sure that we install and load readr and readxl. Remember that readr is part of the tidyverse which we have already installed so you probably only need to install readxl. Once you have done this, you can create and run an R code chunk with the following:\n\n# load necessary packages\nlibrary(tidyverse)\nlibrary(readxl)\n\nNow, let’s read in the data saved in the files happiness_report.csv and happiness_report.xlsx.\n\n# read in data files\nhappiness_report_csv &lt;- read_csv(\"data/happiness_report.csv\")\nhappiness_report_xl &lt;- read_xlsx(\"data/happiness_report.xlsx\")\n\nNotice that we have assigned our data to a variable after having read it in.\nIt is important to confirm that we have read in what we actually wanted. There are a number of ways to confirm that the data read into R makes sense:\n\nThe dim command will tell us the number of rows and columns for the data set we read in.\nThe glimpse command from the dplyr package (included with tidyverse) will show a glimpse of what is in the data. Specifically, glimpse displays a transposed version of you tabular data along with the number of rows and columns. This command even tells you valuable information about what type of data you have in each column. This is an extremely useful function and it is recommended that you use it immediately after reading in data.\nThe head (tail) commands will display the first (last) few rows of a data set.\nThe View command will bring up a full view of you data in a new tab in R studio. This is basically like pulling up an Excel spreadsheet in RStudio. You probably want to avoid using this function on anything other than small or maybe moderately sized data sets.\nThe summary function in base R will provide a descriptive summary of each column in a data set.\nThe skim function from the skimr package quickly provides a broad overview of a data set. It’s basically a souped-up version of summary.\n\nHere are the results of each of these commands (except View) run on the data read in from the .csv file:\n\ndim(happiness_report_csv)\n\n[1] 155   5\n\n\n\nglimpse(happiness_report_csv)\n\nRows: 155\nColumns: 5\n$ country         &lt;chr&gt; \"Norway\", \"Denmark\", \"Iceland\", \"Switzerland\", \"Finlan…\n$ happiness_score &lt;dbl&gt; 7.537, 7.522, 7.504, 7.494, 7.469, 7.377, 7.316, 7.314…\n$ GDP_per_capita  &lt;dbl&gt; 1.616463, 1.482383, 1.480633, 1.564980, 1.443572, 1.50…\n$ life_expectancy &lt;dbl&gt; 0.7966665, 0.7925655, 0.8335521, 0.8581313, 0.8091577,…\n$ freedom         &lt;dbl&gt; 0.6354226, 0.6260067, 0.6271626, 0.6200706, 0.6179509,…\n\n\n\nhead(happiness_report_csv)\n\n# A tibble: 6 × 5\n  country     happiness_score GDP_per_capita life_expectancy freedom\n  &lt;chr&gt;                 &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Norway                 7.54           1.62           0.797   0.635\n2 Denmark                7.52           1.48           0.793   0.626\n3 Iceland                7.50           1.48           0.834   0.627\n4 Switzerland            7.49           1.56           0.858   0.620\n5 Finland                7.47           1.44           0.809   0.618\n6 Netherlands            7.38           1.50           0.811   0.585\n\n\n\ntail(happiness_report_csv)\n\n# A tibble: 6 × 5\n  country                 happiness_score GDP_per_capita life_expectancy freedom\n  &lt;chr&gt;                             &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 Togo                               3.49         0.305           0.247   0.380 \n2 Rwanda                             3.47         0.369           0.326   0.582 \n3 Syria                              3.46         0.777           0.501   0.0815\n4 Tanzania                           3.35         0.511           0.365   0.390 \n5 Burundi                            2.90         0.0916          0.152   0.0599\n6 Central African Republ…            2.69         0               0.0188  0.271 \n\n\n\nView(happiness_report_csv)\n\n\nsummary(happiness_report_csv)\n\n   country          happiness_score GDP_per_capita   life_expectancy \n Length:155         Min.   :2.693   Min.   :0.0000   Min.   :0.0000  \n Class :character   1st Qu.:4.505   1st Qu.:0.6634   1st Qu.:0.3699  \n Mode  :character   Median :5.279   Median :1.0646   Median :0.6060  \n                    Mean   :5.354   Mean   :0.9847   Mean   :0.5513  \n                    3rd Qu.:6.101   3rd Qu.:1.3180   3rd Qu.:0.7230  \n                    Max.   :7.537   Max.   :1.8708   Max.   :0.9495  \n    freedom      \n Min.   :0.0000  \n 1st Qu.:0.3037  \n Median :0.4375  \n Mean   :0.4088  \n 3rd Qu.:0.5166  \n Max.   :0.6582  \n\n\n\nskimr::skim(happiness_report_csv)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nhappiness_report_csv\n\n\n\n\nNumber of rows\n\n\n155\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n1\n\n\n\n\nnumeric\n\n\n4\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\ncountry\n\n\n0\n\n\n1\n\n\n4\n\n\n24\n\n\n0\n\n\n155\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nhappiness_score\n\n\n0\n\n\n1\n\n\n5.35\n\n\n1.13\n\n\n2.69\n\n\n4.51\n\n\n5.28\n\n\n6.10\n\n\n7.54\n\n\n▂▆▇▇▅\n\n\n\n\nGDP_per_capita\n\n\n0\n\n\n1\n\n\n0.98\n\n\n0.42\n\n\n0.00\n\n\n0.66\n\n\n1.06\n\n\n1.32\n\n\n1.87\n\n\n▂▅▇▇▂\n\n\n\n\nlife_expectancy\n\n\n0\n\n\n1\n\n\n0.55\n\n\n0.24\n\n\n0.00\n\n\n0.37\n\n\n0.61\n\n\n0.72\n\n\n0.95\n\n\n▂▃▃▇▅\n\n\n\n\nfreedom\n\n\n0\n\n\n1\n\n\n0.41\n\n\n0.15\n\n\n0.00\n\n\n0.30\n\n\n0.44\n\n\n0.52\n\n\n0.66\n\n\n▁▃▅▇▅\n\n\n\n\n\n\n\nQuestion: What useful information is provided by the output from each of these commands?\nExercise: Run all these commands on the data that you read in from the Excel spreadsheet.\nWe end this section by noting that the data file happiness_report.csv is also stored in a remote repository, view the repository. Since this repository has a corresponding url address, we can read the data in directly from the web:\n\nhappiness_report_csv_url &lt;- read_csv(\"https://raw.githubusercontent.com/UBC-DSCI/data-science-a-first-intro-worksheets/main/worksheet_reading/data/happiness_report.csv\")\n\nWarning: There is one thing you have to be careful about when reading in remote data. You need to make sure you are reading in from the link corresponding to the raw data file and not the one that has been rendered for visual display.\nExercise: Use one or more of the commands we have covered to make sure that this has read in the data you want."
  },
  {
    "objectID": "lesson02/index.html#preparation-for-the-next-lesson",
    "href": "lesson02/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 2",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson or two, we will look at some things that one commonly does with data very soon after reading it into R. That is,\n\nManipulate or wrangle the data.\nConduct an exploratory data analysis (EDA).\n\nPrior to starting the next lesson, please read section 2.8 and Chapter 3 of (Timbers, Campbell, and Lee 2022), link here."
  },
  {
    "objectID": "lesson02/index.html#references",
    "href": "lesson02/index.html#references",
    "title": "Lesson 2",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. n.d. “Telling Stories with Data: With Applications in r.”\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\".\n\n\nMock, J. Thomas. 2018. “Tidy Tuesday.” GitHub Repository. https://github.com/rfordatascience/tidytuesday; GitHub.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-08\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder   * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra  * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n readxl      * 1.4.3   2023-07-06 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson02/index.html#vectors-lists-and-data-frames",
    "href": "lesson02/index.html#vectors-lists-and-data-frames",
    "title": "Lesson 2",
    "section": "  Vectors, Lists, and Data Frames",
    "text": "Vectors, Lists, and Data Frames\nWhen tabular data is read into R from a file such as a .csv or an Excel spreadsheet it is represented in R by a particular data type. Specifically, tabular data in R is represented by a data frame or tibble. While we don’t want to spend too much time on the technicalities of R objects, it is useful to have a brief introduction to the vector, list, and data frame objects in R. If you want to see more details then we present here, see Chapter 5 of (Grolemund 2014), which is freely available online. View Chapter 5.\n\nVectors in R\nIn our last class, we were exposed to vectors in R through the first lesson in the R Programming swirl available through the swirl package. The most important thing to remember about vectors in R is\n\nHow to create them.\nThat a vector can only hold values of the same type.\n\nFor example, we can create a vector with three numeric values and store it as a variable my_num_vect using\n\n(my_num_vect &lt;- c(1.2,6.8,-13.9))\n\n[1]   1.2   6.8 -13.9\n\n\nWe can create a vector with three character values and store it as a variable my_chr_vect using\n\n(my_chr_vect &lt;- c(\"dog\",\"cat\",\"bird\"))\n\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\nbut we cannot create a vector that contains two numbers and one character. That is, we can’t mix types in a vector. Let’s see what happens if we try it:\n\n(my_mix_vect &lt;- c(3.14,\"cat\",\"bird\"))\n\n[1] \"3.14\" \"cat\"  \"bird\"\n\n\nExercise: Examine the output from the last command and explain what happened.\nNote that you can determine the type for elements of a vector using the typeof command. For example,\n\ntypeof(my_num_vect)\n\n[1] \"double\"\n\n\nWhile vectors are obviously useful for storing certain types of data, they are limited for most purposes because data sets like the tabular data sets we’ve loaded into R are made up of a mixture of different types of values.\n\n\nLists in R\nLists are another type of R object that do allow us to mix data types. For example\n\n(my_mix_list &lt;- list(3.14,\"cat\",\"bird\"))\n\n[[1]]\n[1] 3.14\n\n[[2]]\n[1] \"cat\"\n\n[[3]]\n[1] \"bird\"\n\n\nThere is no type conversion unlike what we saw happen when we tried to create a vector of mixed types. Let’s see what happens if we request the type for the elements in this list.\n\ntypeof(my_mix_list)\n\n[1] \"list\"\n\n\nThis just tells us that we have a list which we already knew. To determine the type of each list element, we can use a command such as\n\nmap(my_mix_list,typeof)\n\n[[1]]\n[1] \"double\"\n\n[[2]]\n[1] \"character\"\n\n[[3]]\n[1] \"character\"\n\n\nDon’t worry if you don’t understand the last command. Later we will look at map type functions which belong to the package purrr.\nLists are extremely useful and many functions such as the function map we just used actually return a list. Two things we can do with lists are will be very useful for us in this course are:\n\nGive names to the elements of a list.\nStore vectors (and even more complicated objects) as the elements in a list.\n\nFor example:\n\n(my_vector_list &lt;- list(my_num_vect = c(1.2,6.8,-13.9),\n                       my_chr_vect = c(\"dog\",\"cat\",\"bird\")))\n\n$my_num_vect\n[1]   1.2   6.8 -13.9\n\n$my_chr_vect\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\nThen we can access the elements in the list my_vector_list using the accessor operator denoted by $. For example\n\n(my_vector_list$my_num_vect)\n\n[1]   1.2   6.8 -13.9\n\n(my_vector_list$my_chr_vect)\n\n[1] \"dog\"  \"cat\"  \"bird\"\n\n\n\n\nData Frames in R\nA data frame in R is nothing other than a list where each element of the list is a vector such that all the vectors in the list have the same length. We can create a date frame is much the same way that we just created the list that contains two vectors:\n\n(my_df &lt;- data.frame(col_a = c(1.2,6.8,-13.9),\n                       col_b = c(\"dog\",\"cat\",\"bird\")))\n\n  col_a col_b\n1   1.2   dog\n2   6.8   cat\n3 -13.9  bird\n\n\nNotice that my_df contains exactly the same information that my_vector_list does. The only real difference is the way in which the results are displayed.\nThe point of all this is just to explain that we functions like read_csv are used to read in tabular data, R stores the result as a data frame. Actually, since read_csv belongs to the tidyverse it reads in the data as something called a tibble. For our purposes, we can think of data frames and tibbles as being more or less the same things. If you want to know more about tibbles, see section 3.6 from (Wickham 2019), view the section."
  },
  {
    "objectID": "lesson02/index.html#some-data-sources",
    "href": "lesson02/index.html#some-data-sources",
    "title": "Lesson 2",
    "section": "Some Data Sources",
    "text": "Some Data Sources\nOne thing students often struggle with is finding and picking a good data set for their projects. Appendix B of the online textbook contains a very helpful list of data sources (Alexander, n.d.). View appendix B. Two other very interesting and useful sources of data are the Tidy Tuesday data repositories and Kaggle. There are also many R packages that either include data or that can be used to download data. The ROpenSci project is a good resource for finding R packages that can be used to obtain data, view the project."
  },
  {
    "objectID": "lesson02/index.html#version-control-and-remote-repositories",
    "href": "lesson02/index.html#version-control-and-remote-repositories",
    "title": "Lesson 2",
    "section": "Version Control and Remote Repositories",
    "text": "Version Control and Remote Repositories\nVersion control systems track changes to a project over its lifespan, allow sharing and editing of code across a collaborative team, and make it easier to distribute the finished project to its intended audience. Chapter 12 of (Timbers, Campbell, and Lee 2022) covers version control in detail. Let’s examine this chapter."
  },
  {
    "objectID": "lesson03/index.html",
    "href": "lesson03/index.html",
    "title": "Lesson 3",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nUse the dplyr functions select and filter to manipulate rows and columns of a data frame.\nUse the mutate function to add new columns to a data frame.\nUse group_by to group observations in one or more columns of a data frame by a grouping variable.\nUse the summarize function with the group_by function to compute grouped summaries.\nUse pivot_longer and pivot_wider functions to perform certain types of reorganizations of a data frame.\nUse the join family of functions to combine separate data sets into one."
  },
  {
    "objectID": "lesson03/index.html#learning-objectives",
    "href": "lesson03/index.html#learning-objectives",
    "title": "Lesson 3",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nUse the dplyr functions select and filter to manipulate rows and columns of a data frame.\nUse the mutate function to add new columns to a data frame.\nUse group_by to group observations in one or more columns of a data frame by a grouping variable.\nUse the summarize function with the group_by function to compute grouped summaries.\nUse pivot_longer and pivot_wider functions to perform certain types of reorganizations of a data frame.\nUse the join family of functions to combine separate data sets into one."
  },
  {
    "objectID": "lesson03/index.html#readings-etc.",
    "href": "lesson03/index.html#readings-etc.",
    "title": "Lesson 3",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapter 3 of Data Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee (Timbers, Campbell, and Lee 2022). View the free online version of the text..\nChapter 5 of R for Data Science by Wickham and Grolemund (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View the chapter online"
  },
  {
    "objectID": "lesson03/index.html#preparation-for-the-next-lesson",
    "href": "lesson03/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 3",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson we will begin discussing the exploratory data analysis (EDA) process starting with some data visualization.\nPrior to starting the next lesson, please read Chapter 4 of (Timbers, Campbell, and Lee 2022), link here."
  },
  {
    "objectID": "lesson03/index.html#references",
    "href": "lesson03/index.html#references",
    "title": "Lesson 3",
    "section": "References",
    "text": "References\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-13\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder    * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n nycflights13 * 1.0.2   2021-04-12 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n readxl       * 1.4.3   2023-07-06 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson03/index.html#overview",
    "href": "lesson03/index.html#overview",
    "title": "Lesson 3",
    "section": "Overview",
    "text": "Overview\nReal world data rarely comes in exactly the right form for the analysis you want to do. This has led to the implementation of methods that facilitate manipulating data in a way that allows us to more easily address some of the common problems with data sets. In this lesson, we will introduce such methods which include but are not limited to\n\nselecting certain rows or columns of tabular data,\ncreating new variables or columns, often by transforming existing ones,\ngrouping or arranging observations,\nreshaping parts of the data,\njoining multiple tabular data sets.\n\nA particularly common application of these methods is to get data into an appropriate shape required for a particular type of plot, visualization, or summary.\nThe R package dplyr which belongs to the tidyverse family of packages is one of the available implementations of the common data manipulation methods (Wickham et al. 2023). The dplyr package is robust, well-documented, easy-to-use, and efficient so we will explore it in this lesson. It is worth while to take a moment to visit the ‘dplyr’ website as it contains helpful information and resources, view the webpage."
  },
  {
    "objectID": "lesson03/index.html#data-for-the-lesson",
    "href": "lesson03/index.html#data-for-the-lesson",
    "title": "Lesson 3",
    "section": "Data For the Lesson",
    "text": "Data For the Lesson\nIn this lesson, we will make use of a few data sets. This includes:\n\nThe Gapminder data set which is available in R through the gapminder package. To refresh our memories, let’s examine the first few rows of the Gapminder data:\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\n\nData from the nycflights13 package which we load (make sure the package is installed) using\n\n\nlibrary(nycflights13)\n\nWe can see a list of the data available in the nycflights13 package with the command\n\ndata(package=\"nycflights13\")\n\nwhich produces a table with the following information:\n\n\n\nName of Data Set\nInformation\n\n\n\n\nairlines\nAirline names\n\n\nairports\nAirport metadata\n\n\nflights\nFlights data\n\n\nplanes\nPlane metadata\n\n\nweather\nHourly weather data\n\n\n\nFor example, the flights data has first few rows\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n2013\n1\n1\n517\n515\n2\n830\n819\n11\nUA\n1545\nN14228\nEWR\nIAH\n227\n1400\n5\n15\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n533\n529\n4\n850\n830\n20\nUA\n1714\nN24211\nLGA\nIAH\n227\n1416\n5\n29\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n542\n540\n2\n923\n850\n33\nAA\n1141\nN619AA\nJFK\nMIA\n160\n1089\n5\n40\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n544\n545\n-1\n1004\n1022\n-18\nB6\n725\nN804JB\nJFK\nBQN\n183\n1576\n5\n45\n2013-01-01 05:00:00\n\n\n2013\n1\n1\n554\n600\n-6\n812\n837\n-25\nDL\n461\nN668DN\nLGA\nATL\n116\n762\n6\n0\n2013-01-01 06:00:00\n\n\n2013\n1\n1\n554\n558\n-4\n740\n728\n12\nUA\n1696\nN39463\nEWR\nORD\n150\n719\n5\n58\n2013-01-01 05:00:00"
  },
  {
    "objectID": "lesson03/index.html#the-dplyr-package",
    "href": "lesson03/index.html#the-dplyr-package",
    "title": "Lesson 3",
    "section": "  The dplyr Package",
    "text": "The dplyr Package\nThe dplyr package consists of a set of functions that help you solve the most common data manipulation challenges (Wickham et al. 2023). Before discussing the individual functions, it’s worth listing what they have in common:\n\nThe first argument is always a data frame.\nThe subsequent arguments typically describe which columns to operate on, using the variable names (without quotes).\nThe output is always a new data frame.\n\nNow we will give examples of the typical use for the common dplyr functions:\n\nSelecting Columns\nThe select function extracts specified columns from a data frame:\n\ngapminder %&gt;%\n  select(country,year,lifeExp)\n\n# A tibble: 1,704 × 3\n   country      year lifeExp\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952    28.8\n 2 Afghanistan  1957    30.3\n 3 Afghanistan  1962    32.0\n 4 Afghanistan  1967    34.0\n 5 Afghanistan  1972    36.1\n 6 Afghanistan  1977    38.4\n 7 Afghanistan  1982    39.9\n 8 Afghanistan  1987    40.8\n 9 Afghanistan  1992    41.7\n10 Afghanistan  1997    41.8\n# ℹ 1,694 more rows\n\n\nYou can also specify which columns you don’t want:\n\ngapminder %&gt;%\n  select(-continent)\n\n# A tibble: 1,704 × 5\n   country      year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan  1952    28.8  8425333      779.\n 2 Afghanistan  1957    30.3  9240934      821.\n 3 Afghanistan  1962    32.0 10267083      853.\n 4 Afghanistan  1967    34.0 11537966      836.\n 5 Afghanistan  1972    36.1 13079460      740.\n 6 Afghanistan  1977    38.4 14880372      786.\n 7 Afghanistan  1982    39.9 12881816      978.\n 8 Afghanistan  1987    40.8 13867957      852.\n 9 Afghanistan  1992    41.7 16317921      649.\n10 Afghanistan  1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nExercise: Select the columns dep_delay and carrier from the flights data set. Select all the columns in the flights data set except year, month, and day.\n\n\nFiltering Rows\nThe filter function retains all rows of a data frame according to some specified condition(s).\n\ngapminder %&gt;%\n  filter(country == \"Spain\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Spain   Europe     1952    64.9 28549870     3834.\n 2 Spain   Europe     1957    66.7 29841614     4565.\n 3 Spain   Europe     1962    69.7 31158061     5694.\n 4 Spain   Europe     1967    71.4 32850275     7994.\n 5 Spain   Europe     1972    73.1 34513161    10639.\n 6 Spain   Europe     1977    74.4 36439000    13237.\n 7 Spain   Europe     1982    76.3 37983310    13926.\n 8 Spain   Europe     1987    76.9 38880702    15765.\n 9 Spain   Europe     1992    77.6 39549438    18603.\n10 Spain   Europe     1997    78.8 39855442    20445.\n11 Spain   Europe     2002    79.8 40152517    24835.\n12 Spain   Europe     2007    80.9 40448191    28821.\n\n\nNotice that to specify equality for a condition in filter one must use the double equals ==.\nHere are some other variations using filter:\n\ngapminder %&gt;%\n  filter(country == \"Spain\" | country == \"Portugal\")\n\n# A tibble: 24 × 6\n   country  continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Portugal Europe     1952    59.8  8526050     3068.\n 2 Portugal Europe     1957    61.5  8817650     3775.\n 3 Portugal Europe     1962    64.4  9019800     4728.\n 4 Portugal Europe     1967    66.6  9103000     6362.\n 5 Portugal Europe     1972    69.3  8970450     9022.\n 6 Portugal Europe     1977    70.4  9662600    10172.\n 7 Portugal Europe     1982    72.8  9859650    11754.\n 8 Portugal Europe     1987    74.1  9915289    13039.\n 9 Portugal Europe     1992    74.9  9927680    16207.\n10 Portugal Europe     1997    76.0 10156415    17641.\n# ℹ 14 more rows\n\n\n\ngapminder %&gt;%\n  filter(year &gt;= 1979)\n\n# A tibble: 852 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1982    39.9 12881816      978.\n 2 Afghanistan Asia       1987    40.8 13867957      852.\n 3 Afghanistan Asia       1992    41.7 16317921      649.\n 4 Afghanistan Asia       1997    41.8 22227415      635.\n 5 Afghanistan Asia       2002    42.1 25268405      727.\n 6 Afghanistan Asia       2007    43.8 31889923      975.\n 7 Albania     Europe     1982    70.4  2780097     3631.\n 8 Albania     Europe     1987    72    3075321     3739.\n 9 Albania     Europe     1992    71.6  3326498     2497.\n10 Albania     Europe     1997    73.0  3428038     3193.\n# ℹ 842 more rows\n\n\n\ngapminder %&gt;%\n  filter(year &gt;= 1979 & country == \"Spain\")\n\n# A tibble: 6 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Spain   Europe     1982    76.3 37983310    13926.\n2 Spain   Europe     1987    76.9 38880702    15765.\n3 Spain   Europe     1992    77.6 39549438    18603.\n4 Spain   Europe     1997    78.8 39855442    20445.\n5 Spain   Europe     2002    79.8 40152517    24835.\n6 Spain   Europe     2007    80.9 40448191    28821.\n\n\nExercise: Use filter to select those observations in the flights data that had a departure from JFK airport. Use filter to select those observations in the flights data that had a departure from JFK airport and the airline (carrier) is United Airlines (UA). Use filter to extract those observations where there was a departure delay that was an hour or more.\n\n\nGrouping\nThe group_by function allows us to group observations in a data frame by one or more grouping variables. The syntax for group_by is:\n\ngapminder %&gt;%\n  group_by(year)\n\n# A tibble: 1,704 × 6\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nor for more than one variable:\n\ngapminder %&gt;%\n  group_by(country,year)\n\n# A tibble: 1,704 × 6\n# Groups:   country, year [1,704]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nBy itself, group_by isn’t very useful. However, used with other functions it can be extremely useful. For example, we can combine group_by and summarise to create grouped summaries:\n\ngapminder %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_lifeEx = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year mean_lifeEx\n   &lt;int&gt;       &lt;dbl&gt;\n 1  1952        49.1\n 2  1957        51.5\n 3  1962        53.6\n 4  1967        55.7\n 5  1972        57.6\n 6  1977        59.6\n 7  1982        61.5\n 8  1987        63.2\n 9  1992        64.2\n10  1997        65.0\n11  2002        65.7\n12  2007        67.0\n\n\nHere is another example:\n\nflights %&gt;%\n  filter(dep_delay &gt;= 60) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(n_carrier = n())\n\n# A tibble: 16 × 2\n   carrier n_carrier\n   &lt;chr&gt;       &lt;int&gt;\n 1 9E           1991\n 2 AA           2034\n 3 AS             39\n 4 B6           4655\n 5 DL           2699\n 6 EV           6984\n 7 F9             75\n 8 FL            323\n 9 HA             11\n10 MQ           2037\n11 OO              4\n12 UA           3899\n13 US            779\n14 VX            365\n15 WN           1084\n16 YV             80\n\n\nThis tells us the number of times each carrier has a departure delay of at least an hour. We can add the arrange function to put these in order:\n\nflights %&gt;%\n  filter(dep_delay &gt;= 60) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(n_carrier = n()) %&gt;%\n  arrange(n_carrier)\n\n# A tibble: 16 × 2\n   carrier n_carrier\n   &lt;chr&gt;       &lt;int&gt;\n 1 OO              4\n 2 HA             11\n 3 AS             39\n 4 F9             75\n 5 YV             80\n 6 FL            323\n 7 VX            365\n 8 US            779\n 9 WN           1084\n10 9E           1991\n11 AA           2034\n12 MQ           2037\n13 DL           2699\n14 UA           3899\n15 B6           4655\n16 EV           6984\n\n\nor if we want the opposite order:\n\nflights %&gt;%\n  filter(dep_delay &gt;= 60) %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(n_carrier = n()) %&gt;%\n  arrange(desc(n_carrier))\n\n# A tibble: 16 × 2\n   carrier n_carrier\n   &lt;chr&gt;       &lt;int&gt;\n 1 EV           6984\n 2 B6           4655\n 3 UA           3899\n 4 DL           2699\n 5 MQ           2037\n 6 AA           2034\n 7 9E           1991\n 8 WN           1084\n 9 US            779\n10 VX            365\n11 FL            323\n12 YV             80\n13 F9             75\n14 AS             39\n15 HA             11\n16 OO              4\n\n\nExercise: Use group_by and summarise to compute the average departure delay by carrier. Use group_by and summarise to find the number of times each airport has had departure delays of an hour or more, use arrange to place the airports with the most departure delays at the top.\nWe can group by more than one variable:\n\ngapminder %&gt;%\n  group_by(year,continent) %&gt;%\n  summarise(continent_count = n())\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 60 × 3\n# Groups:   year [12]\n    year continent continent_count\n   &lt;int&gt; &lt;fct&gt;               &lt;int&gt;\n 1  1952 Africa                 52\n 2  1952 Americas               25\n 3  1952 Asia                   33\n 4  1952 Europe                 30\n 5  1952 Oceania                 2\n 6  1957 Africa                 52\n 7  1957 Americas               25\n 8  1957 Asia                   33\n 9  1957 Europe                 30\n10  1957 Oceania                 2\n# ℹ 50 more rows\n\n\nand also summarise more than one variable after grouping.\n\ngapminder %&gt;%\n  group_by(year,continent) %&gt;%\n  summarise(continent_count = n(),\n            mean_lifeExp = mean(lifeExp))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 60 × 4\n# Groups:   year [12]\n    year continent continent_count mean_lifeExp\n   &lt;int&gt; &lt;fct&gt;               &lt;int&gt;        &lt;dbl&gt;\n 1  1952 Africa                 52         39.1\n 2  1952 Americas               25         53.3\n 3  1952 Asia                   33         46.3\n 4  1952 Europe                 30         64.4\n 5  1952 Oceania                 2         69.3\n 6  1957 Africa                 52         41.3\n 7  1957 Americas               25         56.0\n 8  1957 Asia                   33         49.3\n 9  1957 Europe                 30         66.7\n10  1957 Oceania                 2         70.3\n# ℹ 50 more rows\n\n\nWe will spend a lot of time later talking about plots and visualizations. Just to see what is possible, let’s make a plot using the results of a grouped summary:\n\na_grouped_summary &lt;- gapminder %&gt;%\n  group_by(year,continent) %&gt;%\n  summarise(continent_count = n(),\n            mean_lifeExp = mean(lifeExp))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\na_grouped_summary %&gt;%\n  ggplot(aes(x=year,y=mean_lifeExp,color=continent)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nFigure 1: A plot made using data obtained from a grouped summary.\n\n\n\n\nExercise: Modify the code in the last code chunk to make a plot of the average departure delay by airport per each day. That is, the x-axis should be the day, the y-axis should be the average departure delay and the colors should distinguish the different airports. Hint: When you compute the mean, you will have to add an extra argument, na.rm=TRUE because there are missing values in the data.\n\n\nReshaping\nTypically, we want our data to be in tidy format. This means\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFigure 2 shows the rules visually.\n\n\n\n\n\nFigure 2: The following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells. Illustration credit: https://github.com/hadley/r4ds/blob/main/images/tidy-1.png\n\n\n\n\nOften, getting data into tidy format requires reshaping the data. But there maybe reasons to reshape the data from tidy format to another format. The pivot_longer and pivot_wider functions facilitate these types of reshaping.\nFor example, suppose that for some reason we want to view the life expectancy for each country in the Gapminder data so that the values for each year are in their own column. This can be achieved with pivot_wider:\n\ngapminder %&gt;%\n  select(country,year,lifeExp) %&gt;%\n  pivot_wider(names_from = \"year\",values_from = lifeExp)\n\n# A tibble: 142 × 13\n   country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997`\n   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Afghan…   28.8   30.3   32.0   34.0   36.1   38.4   39.9   40.8   41.7   41.8\n 2 Albania   55.2   59.3   64.8   66.2   67.7   68.9   70.4   72     71.6   73.0\n 3 Algeria   43.1   45.7   48.3   51.4   54.5   58.0   61.4   65.8   67.7   69.2\n 4 Angola    30.0   32.0   34     36.0   37.9   39.5   39.9   39.9   40.6   41.0\n 5 Argent…   62.5   64.4   65.1   65.6   67.1   68.5   69.9   70.8   71.9   73.3\n 6 Austra…   69.1   70.3   70.9   71.1   71.9   73.5   74.7   76.3   77.6   78.8\n 7 Austria   66.8   67.5   69.5   70.1   70.6   72.2   73.2   74.9   76.0   77.5\n 8 Bahrain   50.9   53.8   56.9   59.9   63.3   65.6   69.1   70.8   72.6   73.9\n 9 Bangla…   37.5   39.3   41.2   43.5   45.3   46.9   50.0   52.8   56.0   59.4\n10 Belgium   68     69.2   70.2   70.9   71.4   72.8   73.9   75.4   76.5   77.5\n# ℹ 132 more rows\n# ℹ 2 more variables: `2002` &lt;dbl&gt;, `2007` &lt;dbl&gt;\n\n\nAs an application, we can use such data to make a table like the following one:\n\ngapminder %&gt;%\n  select(country,year,lifeExp) %&gt;%\n  pivot_wider(names_from = \"year\",values_from = lifeExp) %&gt;%\n  select(country,`1952`,`2007`) %&gt;%\n  filter(country %in% c(\"Spain\",\"Portugal\")) %&gt;%\n  kable()\n\n\n\n\ncountry\n1952\n2007\n\n\n\n\nPortugal\n59.82\n78.098\n\n\nSpain\n64.94\n80.941\n\n\n\n\n\n\n\nExercise: Use pivot_wider to create a data frame starting with the Gapminder data that has the population values for each country but with each year as its own column.\nExercise: Explain what the following command does:\n\nflights %&gt;% \n  pivot_wider(names_from = origin,values_from = distance)\n\nThe billboard data set records the billboard rank of songs in the year 2000:\n\nbillboard %&gt;%\n  head() %&gt;%\n  kable()\n\n\n\n\nartist\ntrack\ndate.entered\nwk1\nwk2\nwk3\nwk4\nwk5\nwk6\nwk7\nwk8\nwk9\nwk10\nwk11\nwk12\nwk13\nwk14\nwk15\nwk16\nwk17\nwk18\nwk19\nwk20\nwk21\nwk22\nwk23\nwk24\nwk25\nwk26\nwk27\nwk28\nwk29\nwk30\nwk31\nwk32\nwk33\nwk34\nwk35\nwk36\nwk37\nwk38\nwk39\nwk40\nwk41\nwk42\nwk43\nwk44\nwk45\nwk46\nwk47\nwk48\nwk49\nwk50\nwk51\nwk52\nwk53\nwk54\nwk55\nwk56\nwk57\nwk58\nwk59\nwk60\nwk61\nwk62\nwk63\nwk64\nwk65\nwk66\nwk67\nwk68\nwk69\nwk70\nwk71\nwk72\nwk73\nwk74\nwk75\nwk76\n\n\n\n\n2 Pac\nBaby Don't Cry (Keep...\n2000-02-26\n87\n82\n72\n77\n87\n94\n99\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2Ge+her\nThe Hardest Part Of ...\n2000-09-02\n91\n87\n92\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3 Doors Down\nKryptonite\n2000-04-08\n81\n70\n68\n67\n66\n57\n54\n53\n51\n51\n51\n51\n47\n44\n38\n28\n22\n18\n18\n14\n12\n7\n6\n6\n6\n5\n5\n4\n4\n4\n4\n3\n3\n3\n4\n5\n5\n9\n9\n15\n14\n13\n14\n16\n17\n21\n22\n24\n28\n33\n42\n42\n49\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3 Doors Down\nLoser\n2000-10-21\n76\n76\n72\n69\n67\n65\n55\n59\n62\n61\n61\n59\n61\n66\n72\n76\n75\n67\n73\n70\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n504 Boyz\nWobble Wobble\n2000-04-15\n57\n34\n25\n17\n17\n31\n36\n49\n53\n57\n64\n70\n75\n76\n78\n85\n92\n96\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n98^0\nGive Me Just One Nig...\n2000-08-19\n51\n39\n34\n26\n26\n19\n2\n2\n3\n6\n7\n22\n29\n36\n47\n67\n66\n84\n93\n94\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each. Here, the column names are one variable (the week) and the cell values are another (the rank).\nTo tidy this data, we’ll use pivot_longer():\n\nbillboard %&gt;%\n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n# A tibble: 24,092 × 5\n   artist track                   date.entered week   rank\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n 9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n# ℹ 24,082 more rows\n\n\nExercise: Consider the diamonds data set from the ggplot2 package which is part of the tidyverse. The first few rows of the diamonds data looks as follows:\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\n\nApply the pivot_longer function to the diamonds data to combine the x, y, and z columns into a single column.\nTo see that pivot_longer and pivot_wider are inverses of one another, examine the following code:\n\ngapminder %&gt;%\n  select(country,year,lifeExp) %&gt;%\n  pivot_wider(names_from = \"year\",values_from = lifeExp) %&gt;%\n  pivot_longer(cols=-c(\"country\"),names_to = \"year\", values_to = \"lifeExp\")\n\n# A tibble: 1,704 × 3\n   country     year  lifeExp\n   &lt;fct&gt;       &lt;chr&gt;   &lt;dbl&gt;\n 1 Afghanistan 1952     28.8\n 2 Afghanistan 1957     30.3\n 3 Afghanistan 1962     32.0\n 4 Afghanistan 1967     34.0\n 5 Afghanistan 1972     36.1\n 6 Afghanistan 1977     38.4\n 7 Afghanistan 1982     39.9\n 8 Afghanistan 1987     40.8\n 9 Afghanistan 1992     41.7\n10 Afghanistan 1997     41.8\n# ℹ 1,694 more rows\n\n\nPivoting is covered in more detail in Chapter 6 of (Wickham, Çetinkaya-Rundel, and Grolemund 2023)."
  },
  {
    "objectID": "lesson03/index.html#footnotes",
    "href": "lesson03/index.html#footnotes",
    "title": "Lesson 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe song will be included as long as it was in the top 100 at some point in 2000, and is tracked for up to 72 weeks after it appears.↩︎"
  },
  {
    "objectID": "lesson04/index.html",
    "href": "lesson04/index.html",
    "title": "Lesson 4",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nChoose an appropriate visualization for exploring and summarizing a tabular data set.\nUse methods from the ggplot2 package to create an appropriate visualization for exploring and summarizing a tabular data set.\nApply steps from exploratory data analysis to generate or refine questions about a data set."
  },
  {
    "objectID": "lesson04/index.html#learning-objectives",
    "href": "lesson04/index.html#learning-objectives",
    "title": "Lesson 4",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nChoose an appropriate visualization for exploring and summarizing a tabular data set.\nUse methods from the ggplot2 package to create an appropriate visualization for exploring and summarizing a tabular data set.\nApply steps from exploratory data analysis to generate or refine questions about a data set."
  },
  {
    "objectID": "lesson04/index.html#readings-etc.",
    "href": "lesson04/index.html#readings-etc.",
    "title": "Lesson 4",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapters 2 and 11 from R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View book online.\nChapter 4 from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online."
  },
  {
    "objectID": "lesson04/index.html#overview",
    "href": "lesson04/index.html#overview",
    "title": "Lesson 4",
    "section": "Overview",
    "text": "Overview\nAn exploratory data analysis (EDA) is any initial investigation of a data set or data sets with the goal to simply see what is in the data and what questions one may ask from the data.\nImportant: Every statistical or machine learning analysis should start with an EDA.\nYour goal during EDA is to develop an understanding of your data. Two major themes for EDA are:\n\nTo explore what type of variation occurs within variables.\nTo explore what type of covariation occurs between variables.\n\nThe two most important tools for EDA are\n\nNumerical data summaries.\nVisual data summaries.\n\nIn this lesson, we will first build our toolkit for EDA with a major focus on gaining tools and skills for data visualization. Then, we will work through some EDA case studies."
  },
  {
    "objectID": "lesson04/index.html#introduction-to-data-visualization",
    "href": "lesson04/index.html#introduction-to-data-visualization",
    "title": "Lesson 4",
    "section": "Introduction to Data Visualization",
    "text": "Introduction to Data Visualization\nMost people process visual information quickly and easily. Few people can learn much at all by scrolling through a spreadsheet of raw data. Thus, it is important for a data scientist to develop excellent skills in data visualization. By data visualization, we mean the process of representing data via a visual format.\nIn this course, we will utilize the ggplot2 package (and maybe some packages that extend ggplot2) for data visualization (Wickham 2016). There are many excellent resources on data visualization. Two that are highly recommended and available for free online are Data Visualization a Practical Introduction and Fundamentals of Data Visualization (Healy 2018; Wilke 2019). View Data Visualization a Practical Introduction. View Fundamentals of Data Visualization. Not only are these two books free, they are also recent and utilize R and ggplot2.\nData visualizations come with a risk of confusing people. Further, visualizations might not work well for everyone. So, it is very important to choose visualizations that are as clear, accessible, and clean as possible. For example, you should:\n\nAvoid the use of colors that are not easy to distinguish.\nUse appropriate scales.\nUse alt-text and figure captions.\n\nThere are other practices and techniques for making effective and accessible visualizations that we will discuss later.\n\n  The ggplot2 Package\nThe ggplot2 package implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\nFigure 1 shows the typical structure of the grammar of graphics as implemented in the ggplot2 package1.\n\n\n\n\n\nFigure 1: The basic ggplot2 syntax. Figure source (Timbers, Campbell, and Lee 2022).\n\n\n\n\nLet’s see what happens when we run the commands from Figure 1.\n\nggplot(ten_lang, aes(x = language, y = mother_tongue)) +\n  geom_bar(stat = \"identity\")\n\n# the following code is equivalent\n# ten_lang %&gt;% \n#   ggplot(aes(x = language, y = mother_tongue)) +\n#   geom_bar(stat = \"identity\")\n\n\n\n\nFigure 2: The bar plot resulting from running code explained in Figure 1\n\n\n\n\nBefore we dive further into the details of using ggplot2 and see more examples, let’s think about what kinds of plots or graphs we might want to make.\n\n\nTypes of Plots\nThere are many different types of plots one can make so there isn’t necessarily a unique choice to be made when deciding how to visualize data. However, there are a few considerations that will have a strong influence on what type of plot(s) you might create to visualize some data.\nAmong the most important considerations is, the type of variable(s) involved in the parts of your data that you want to display visually. Table 1 lists the most common variable types.\n\n\nTable 1: Types of variables encountered in typical data visualization scenarios, table from (Wilke 2019).\n\n\n\n\n\n\n\n\nType of variable\nExamples\nAppropriate scale\nDescription\n\n\n\n\nquantitative/numerical continuous\n1.3, 5.7, 83, 1.5x10-2\ncontinuous\nArbitrary numerical values. These can be integers, rational numbers, or real numbers.\n\n\nquantitative/numerical discrete\n1, 2, 3, 4\ndiscrete\nNumbers in discrete units. These are most commonly but not necessarily integers. For example, the numbers 0.5, 1.0, 1.5 could also be treated as discrete if intermediate values cannot exist in the given dataset.\n\n\nqualitative/categorical unordered\ndog, cat, fish\ndiscrete\nCategories without order. These are discrete and unique categories that have no inherent order. These variables are also called factors.\n\n\nqualitative/categorical ordered\ngood, fair, poor\ndiscrete\nCategories with order. These are discrete and unique categories with an order. For example, “fair” always lies between “good” and “poor”. These variables are also called ordered factors.\n\n\ndate or time\nJan. 5 2018, 8:03am\ncontinuous or discrete\nSpecific days and/or times. Also generic dates, such as July 4 or Dec. 25 (without year).\n\n\ntext\nThe quick brown fox jumps over the lazy dog.\nnone, or discrete\nFree-form text. Can be treated as categorical if needed.\n\n\n\n\nThe reason why data types such as those listed in Table 1 influence the type of plot(s) used to display data is because the data type of a variable determines what kind of aesthetics can be used in a plot.\nBy an aesthetic, we mean a visual element that can be used to describe aspects of a given graphic. Figure 3 shows some common aesthetics for plots or graphs used in data visualization.\n\n\n\n\n\nFigure 3: Common aesthetics\n\n\n\n\nReturning to Figure 1, we notice that one of the arguments that must be given to ggplot is aes which is short for aesthetic.\nFigure 4 shows several of the most common types of plots for a single or pair of variables. These plot types are:\n\nscatter plots visualize the relationship between two quantitative variables,\nline plots visualize trends with respect to an independent, ordered quantity (e.g., time),\nbar plots visualize comparisons of amounts, and\nhistograms visualize the distribution of one quantitative variable (i.e., all its possible values and how often they occur)\n\n\n\n\n\n\nFigure 4: Examples of scatter, line and bar plots, as well as histograms.\n\n\n\n\n\n\nTips for Good Visualizations\nWe will soon learn how to make these types of plots and more with ggplot2. Before that, here is a list of tips for making sure that our visualizations are good:\n\nMake sure the visualization answers the question you have asked most simply and plainly as possible.\nUse legends and labels so that your visualization is understandable without reading the surrounding text.\nEnsure the text, symbols, lines, etc., on your visualization are big enough to be easily read.\nEnsure the data are clearly visible; don’t hide the shape/distribution of the data behind other objects (e.g., a bar).\nMake sure to use color schemes that are easily visible by those with colorblindness (a surprisingly large fraction of the overall population—from about 1% to 10%, depending on sex and ancestry (Deeb 2005)). For example, ColorBrewer and the RColorBrewer R package (Neuwirth 2014) provide the ability to pick such color schemes, and you can check your visualizations after you have created them by uploading to online tools such as a color blindness simulator.\nRedundancy can be helpful; sometimes conveying the same message in multiple ways reinforces it for the audience.\nUse colors sparingly. Too many different colors can be distracting, create false patterns, and detract from the message.\nBe wary of overplotting. Overplotting is when marks that represent the data overlap, and is problematic as it prevents you from seeing how many data points are represented in areas of the visualization where this occurs. If your plot has too many dots or lines and starts to look like a mess, you need to do something different.\nOnly make the plot area (where the dots, lines, bars are) as big as needed. Simple plots can be made small.\nDon’t adjust the axes to zoom in on small differences. If the difference is small, show that it’s small!\n\nIf you’re not sure what plot to make for your data, the data-to-viz website can be very helpful. View the data-to-viz site. Once you know what type of plot to make, the R Graph Gallery website is helpful for finding the code to make the plot with ggplot2. View the R Graph Gallery"
  },
  {
    "objectID": "lesson04/index.html#preparation-for-the-next-lesson",
    "href": "lesson04/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 4",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nChapter 26 from R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View book online."
  },
  {
    "objectID": "lesson04/index.html#references",
    "href": "lesson04/index.html#references",
    "title": "Lesson 4",
    "section": "References",
    "text": "References\n\n\nDeeb, Sameer. 2005. “The Molecular Basis of Variation in Human Color Vision.” Clinical Genetics 67: 369–77.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nNeuwirth, Erich. 2014. RColorBrewer: ColorBrewer Palettes. https://cran.r-project.org/web/packages/RColorBrewer/index.html.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".\n\n\nWilke, Claus O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-20\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n canlang      * 0.0.1   2023-09-13 [1] Github (ttimbers/canlang@1a54305)\n cowplot      * 1.1.1   2020-12-30 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n gapminder    * 1.0.0   2023-03-10 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n ggthemes     * 4.2.4   2021-01-20 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magick       * 2.7.5   2023-08-07 [1] CRAN (R 4.3.0)\n patchwork    * 1.1.3   2023-08-14 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n RColorBrewer * 1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n readxl       * 1.4.3   2023-07-06 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson04/index.html#eda-case-studies",
    "href": "lesson04/index.html#eda-case-studies",
    "title": "Lesson 4",
    "section": "EDA Case Studies",
    "text": "EDA Case Studies\nIn this section, we will work through a couple of short EDA case studies. If you want to see and EDA live coded by a master data scientist, check out the following video:\n\nExploratory data analysis worked example video by Hadley Wickham, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nEDA Case Study 1:\nLet’s work through an EDA together using the data set on tornados that we loaded earlier in this lesson. To get additional practice in using all the tools we have developed so far, let’s create a new RStudio project and Quarto notebook and go through the EDA together.\n\n\nEDA Case Study 2:\nIn one of her blog posts, the data scientist Julia Silge examines the droughts in the United States, view the blog post. As a seasoned data scientist, Silge begins all of her analyses with an EDA. Let’s reproduce and possibly extend the EDA that Silge conducts as part of her analysis. Again, let’s create a new RStudio project and Quarto notebook and go through the EDA together."
  },
  {
    "objectID": "lesson04/index.html#footnotes",
    "href": "lesson04/index.html#footnotes",
    "title": "Lesson 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use the pipe operator %&gt;% (or even |&gt;) to input a data frame into the first argument for the ggplot function.↩︎\nIf you copy and paste the plot commands in this notebook into your own R session you may get plots with a slightly difference appearance. The reason for this is that a plot theme has been set for this notebook using the theme_set(theme_minimal(base_size=12)) command. You can learn more about ggplot themes here.↩︎"
  },
  {
    "objectID": "lesson04/index.html#using-the-ggplot2-package",
    "href": "lesson04/index.html#using-the-ggplot2-package",
    "title": "Lesson 4",
    "section": "  Using the ggplot2 Package",
    "text": "Using the ggplot2 Package\nIn this section, we will go through many use cases for ggplot2. For the examples and exercises in this section, we will work with three data sets:\n\nThe mpg data set in the ggplot2 package. The first few rows for mpg are shown in Table 2.\nThe diamonds data set in the ggplot2 package. The first few rows for diamonds are shown in Table 3.\nThe tornados data set from the Tidy Tuesday data repository. The first few rows for this data are shown in Table 4.\n\nTo access the mpg and diamonds data sets, we just need to load the ggplot2 package. To access the tornados data, we need to load the .csv file which we do with the following code:\n\ntornados &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-16/tornados.csv')\n\nLet’s look at the first few rows for each of the three data sets:\n\nmpg\n\n\n\n\n\nTable 2: Table showing first few rows of the mpg data set.\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\n\n\n\n\n\n\n\ndiamonds\n\n\n\n\n\nTable 3: Table showing first few rows of the diamonds data set.\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\n\n\n\ntornados\n\n\n\n\n\nTable 4: Table showing first few rows of the tornados data set.\n\n\nom\nyr\nmo\ndy\ndate\ntime\ntz\ndatetime_utc\nst\nstf\nmag\ninj\nfat\nloss\nslat\nslon\nelat\nelon\nlen\nwid\nns\nsn\nf1\nf2\nf3\nf4\nfc\n\n\n\n\n192\n1950\n10\n1\n1950-10-01\n21:00:00\nAmerica/Chicago\n1950-10-02 03:00:00\nOK\n40\n1\n0\n0\n5e+04\n36.73\n-102.52\n36.88\n-102.30\n15.8\n10\n1\n1\n25\n0\n0\n0\nFALSE\n\n\n193\n1950\n10\n9\n1950-10-09\n02:15:00\nAmerica/Chicago\n1950-10-09 08:15:00\nNC\n37\n3\n3\n0\n5e+05\n34.17\n-78.60\n0.00\n0.00\n2.0\n880\n1\n1\n47\n0\n0\n0\nFALSE\n\n\n195\n1950\n11\n20\n1950-11-20\n02:20:00\nAmerica/Chicago\n1950-11-20 08:20:00\nKY\n21\n2\n0\n0\n5e+05\n37.37\n-87.20\n0.00\n0.00\n0.1\n10\n1\n1\n177\n0\n0\n0\nFALSE\n\n\n196\n1950\n11\n20\n1950-11-20\n04:00:00\nAmerica/Chicago\n1950-11-20 10:00:00\nKY\n21\n1\n0\n0\n5e+05\n38.20\n-84.50\n0.00\n0.00\n0.1\n10\n1\n1\n209\n0\n0\n0\nFALSE\n\n\n197\n1950\n11\n20\n1950-11-20\n07:30:00\nAmerica/Chicago\n1950-11-20 13:30:00\nMS\n28\n1\n3\n0\n5e+04\n32.42\n-89.13\n0.00\n0.00\n2.0\n37\n1\n1\n101\n0\n0\n0\nFALSE\n\n\n194\n1950\n11\n4\n1950-11-04\n17:00:00\nAmerica/Chicago\n1950-11-04 23:00:00\nPA\n42\n3\n1\n0\n5e+05\n40.20\n-76.12\n40.40\n-75.93\n15.9\n100\n1\n1\n71\n11\n0\n0\nFALSE\n\n\n\n\n\n\n\n\nExercise: For each variables in each of the three data sets, determine the appropriate data type.\n\nBasic ggplot2 Usage\n\nNumerical Data\nHistograms are a common way to visualize a single numerical variable. Here is how to make a basic histogram with ggplot22:\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNotice that we get a message about the number of “bins” for our histogram.\nQuestion: What is meant by the number of bins for a histogram?\nWe can change either the number of bins or the width of bins for a histogram created with ggplot2. For example,\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(bins=22)\n\n\n\n\nor\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\nThe aesthetic value fill determines the shading of a histogram (and also other graphical objects as we will see later).\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(fill=\"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe aesthetic value color determines the boundary color of a histogram (and also other graphical objects as we will see later).\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(color=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHere is a nicer version of our histogram where we’ve included better labels for the axes:\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(fill=\"purple\",color=\"white\",bins = 25) + \n  labs(x=\"Highway gas milage (mpg)\",y=\"Count\")\n\n\n\n\nExercise: Create a nice histogram for the carat variable in the diamonds data set.\nBox plots are an alternative way to visualize numerical variables. Here is a basic box plot created using using ggplot2:\n\nmpg %&gt;%\n  ggplot(aes(y=hwy)) + \n  geom_boxplot()\n\n\n\n\nHere is an nicer version that also adds the data points over the box plot:\n\nmpg %&gt;%\n  ggplot(aes(y=hwy,x=rep(1,nrow(mpg)))) + \n  geom_boxplot(outlier.shape = NA,color=\"orange\") +\n  geom_jitter(width = 0.2,alpha=0.5,color=\"darkgreen\") + \n  labs(x=\"\",y=\"Highway gas mileage (mpg)\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\nExercise: Compare and contrast histograms and box plots.\nExercise: Create a nice box plot for the carat variable in the diamonds data set.\nDensity plots are yet another way to visualize numerical variables. They are basically smoothed versions of histograms but with the y-axis scaled so that the total area under the curve is 1.\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_density()\n\n\n\n\nExercise: What are the main aesthetics used in a density plot?\nSince density plots involve lines or more precisely, curves we can adjust the line width. For example,\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_density(linewidth=2)\n\n\n\n\nAdding a so-called rug lets us see where the observed values in our data fall:\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_density(linewidth=2) + \n  geom_rug()\n\n\n\n\nBy normalizing our data, we can plot a density curve over the data histogram. For example,\n\nmpg %&gt;%\n  ggplot(aes(x=hwy)) + \n  geom_histogram(aes(y = after_stat(density)),\n                 fill=\"purple\",color=\"white\",bins = 25) + \n  geom_density(linewidth=2,color=\"gold\") + \n  geom_rug()\n\n\n\n\nExercise: Create a nice density plot for the carat variable in the diamonds data set.\n\n\nCategorical Data\nPerhaps the most common plot type for a single categorical variable is a bar plot. Here is how to create a basic bar plot using ggplot2:\n\ndiamonds %&gt;%\n  ggplot(aes(cut)) + \n  geom_bar()\n\n\n\n\nNote that a bar plot is just a visual representation of a frequency table for the variable:\n\ndiamonds %&gt;%\n  pull(cut) %&gt;%\n  table()\n\n.\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551 \n\n\nSimilar to what we saw with histograms, the color aesthetic changes the boundary of our bars in bar plots:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,color=cut)) + \n  geom_bar()\n\n\n\n\nwhile the fill aesthetic changes the shading of our bars:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar()\n\n\n\n\nThe next plot demonstrates how to modify the color palette for a categorical variable:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar(color=\"black\") + \n  scale_fill_brewer(palette=\"Spectral\") + \n  labs(x = \"Cut of diamond\",y=\"Count\", fill=\"Cut of diamond\") \n\n\n\n\nWe can also manually change the ordering of our variables:\n\ndiamonds %&gt;%\n  mutate(cut = factor(cut,levels=c(\"Fair\",\"Good\",\"Ideal\",\"Premium\",\"Very Good\"))) %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar(color=\"black\") + \n  scale_fill_brewer(palette=\"Spectral\") + \n  labs(x = \"Cut of diamond\",y=\"Count\", fill=\"Cut of diamond\") \n\n\n\n\nExercise: Create a nice bar plot for the class variable in the mpg data set.\nNote that the ggthemes package contains functions like scale_color_colorblind and scale_fill_colorblind that make it easy to use colorblind-friendly color palettes. For example,\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar(color=\"black\") + \n  scale_fill_colorblind() + \n  labs(x = \"Cut of diamond\",y=\"Count\", fill=\"Cut of diamond\")\n\n\n\n\nThe scales package makes it easy to use certain common scales or units for the axes. For example, if we wanted scientific notation for the y-axis in the previous plot, we could do as follows:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=cut)) + \n  geom_bar(color=\"black\") + \n  scale_fill_colorblind() + \n  scale_y_continuous(labels = scales::label_scientific()) + \n  labs(x = \"Cut of diamond\",y=\"Count\", fill=\"Cut of diamond\")\n\n\n\n\n\n\nVisualizing Relationships\nTo visualize a relationship we need to have at least two variables mapped to aesthetics of a plot. We will start by looking at examples for visualizing the relationship between exactly two variables. The most common ways to do this include:\n\nScatter plots for visualizing the relationship between two numerical variables.\nSide-by-side box plots for visualizing the relationship between a numerical variable and a categorical variable.\nStacked bar plots for visualizing the relationship between two categorical variables.\n\nHere is an example of a scatter plot:\n\ndiamonds %&gt;%\n  ggplot(aes(x=carat,y=price)) + \n  geom_point()\n\n\n\n\nHere is an example of a side-by-side box plot:\n\ndiamonds %&gt;%\n  ggplot(aes(x=clarity,y=price)) + \n  geom_boxplot()\n\n\n\n\nHere is an examplep of a stacked bar plot:\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=color)) + \n  geom_bar()\n\n\n\n\nQuestion: How would you interpret the information provided by each of the last three plots?\nExercise: Use appropriate variables from the mpg data set to make a scatter plot, a side-by-side box plot, and a stacked bar plot.\nIn some cases it is helpful to apply some kind of transformation to variables because this can help to illuminate the nature of the relationship between variables. For example, here are the scatter plot and side-by-side bar plot of the data from before but now with a \\(\\log_{10}\\) transform applied to the price:\n\ndiamonds %&gt;%\n  ggplot(aes(x=carat,y=log10(price))) + \n  geom_point()\n\n\n\n\nNotice that in the last scatter plot for the diamonds data, there are many points that overlay one another. A way to address this is using the alpha argument like\n\ndiamonds %&gt;%\n  ggplot(aes(x=carat,y=log10(price))) + \n  geom_point(alpha=0.1)\n\n\n\n\nAnother way to examine the relationship between two categorical variables is with a relative frequency plot such as\n\ndiamonds %&gt;%\n  ggplot(aes(x=cut,fill=color)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nQuestion: What is the interpretation of the relative frequency plot?\nQuestion: Can you think of some ways in which we can improve any of the last few plots?\nExercise: Try to determine if there are any useful transformation you can find for the variables in the plots you made with the mpg data.\nThe ggpairs function from the GGAlly package can sometimes be a good way to view the relationship between many paris of variables together.\n\ndiamonds %&gt;%\n  select(-c(x,y,z)) %&gt;%\n  GGally::ggpairs()\n\n\n\n\nNote that ggpairs is not very useful for a data set with a lot of variables or a data set that has a categorical variable with a large number of levels.\nIf we want to move beyond just two variables at a time, we will have to use other aesthetics like color, shape, size, etc. to incorporate the additional variables. Let’s see some examples:\n\nmpg %&gt;%\n  ggplot(aes(x=displ,y=cty,color=drv)) + \n  geom_point()\n\n\n\n\nor\n\nmpg %&gt;%\n  ggplot(aes(x=displ,y=cty,color=as.character(cyl),shape=drv)) + \n  geom_point()\n\n\n\n\nExercise: Try some techniques to visualize more than two variables at a time for the diamonds data set.\nWarning: You should avoid trying to cram too much into a single visualization. Just because you can include many variables does not necessarily mean that doing so will result in a better plot.\n\n\n\nSummary So Far\nWe have seen some basic techniques for data visualization and learned how to use the functionality of ggplot2 for creating high quality visualizations. While we have only scratched the surface in terms of what one can do using ggplot2, you should have a solid grounding for learning more. As we move through the course, we will continue to develop our tools and skills in data visualization. Further, you are highly encouraged to explore more on your own using some of the recommended resources and as part of your semester project.\nOur next goal is to look at some case studies in exploratory data analysis where visualizations will play an important role. However, before we get into that you should see how you can use some of the features of Quarto to not only create a high quality plot but also how to include it as a figure in a report or presentation document.\n\n\nFigures in Quarto\nWhen we create figures in a Quarto notebook, we can add additional information such as a label, figure caption, alt text, etc. The following is an example of how to do this:\n#| echo: false\n#| warning: false\n#| message: false\n#| label: fig-quarto-figs\n#| fig-cap: Scatter plot of city gas mileage versus the engine size of some sample vehicles. Data is from the `mpg` data set in the `ggplot2` package [@wickham2016].\n#| fig-alt: |\n#|   A scatterplot of city gas mileage vs. engine size of vehicles, with a\n#|   best fit line of the relationship between these two variables \n#|   overlaid. The plot displays a negative, non-linear, and relatively \n#|   strong relationship between these two variables.\n\nmpg %&gt;%\n  ggplot(aes(x=displ,y=cty)) + \n  geom_point(aes(color=as.character(cyl),shape=drv),size=2) + \n  labs(\n    title = \"Engine size\",\n    subtitle = \"Engine size versus gas mileage for different vehicles\",\n    x = \"Engine displacement (litres)\",\n    y = \"City gas mileage (mpg)\",\n    color = \"Number of Cylinders\",\n    shape = \"Drive type\"\n  ) +\n  scale_color_colorblind()\nFigure 5 is what the last code chunk produces.\n\n\n\n\n\nFigure 5: Scatter plot of city gas mileage versus the engine size of some sample vehicles. Data is from the mpg data set in the ggplot2 package (Wickham 2016)."
  },
  {
    "objectID": "lesson04/index.html#figures-in-quarto",
    "href": "lesson04/index.html#figures-in-quarto",
    "title": "Lesson 4",
    "section": "Figures in Quarto",
    "text": "Figures in Quarto\nWhen we create figures in a Quarto notebook, we can add additional information such as a label, figure caption, alt text, etc. The following is an example of how to do this:\n#| echo: false\n#| warning: false\n#| message: false\n#| label: fig-quarto-figs\n#| fig-cap: Scatter plot of city gas mileage versus the engine size of some sample vehicles. Data is from the `mpg` data set in the `ggplot2` package [@wickham2016].\n#| fig-alt: |\n#|   A scatterplot of city gas mileage vs. engine size of vehicles, with a\n#|   best fit line of the relationship between these two variables \n#|   overlaid. The plot displays a negative, non-linear, and relatively \n#|   strong relationship between these two variables.\n\nmpg %&gt;%\n  ggplot(aes(x=displ,y=cty)) + \n  geom_point(aes(color=as.character(cyl),shape=drv),size=2) + \n  labs(\n    title = \"Engine size\",\n    subtitle = \"Engine size versus gas mileage for different vehicles\",\n    x = \"Engine displacement (litres)\",\n    y = \"City gas mileage (mpg)\",\n    color = \"Number of Cylinders\",\n    shape = \"Drive type\"\n  ) +\n  scale_color_colorblind()\nFigure 5 is what the last code chunk produces.\n\n\n\n\n\nFigure 5: Scatter plot of city gas mileage versus the engine size of some sample vehicles. Data is from the mpg data set in the ggplot2 package (Wickham 2016)."
  },
  {
    "objectID": "lesson05/index.html",
    "href": "lesson05/index.html",
    "title": "Lesson 5",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nWrite functions in R.\nUse map and other such functions from purrr for iteration."
  },
  {
    "objectID": "lesson05/index.html#learning-objectives",
    "href": "lesson05/index.html#learning-objectives",
    "title": "Lesson 5",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nWrite functions in R.\nUse map and other such functions from purrr for iteration."
  },
  {
    "objectID": "lesson05/index.html#readings-etc.",
    "href": "lesson05/index.html#readings-etc.",
    "title": "Lesson 5",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nChapter 26 from R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View Chapter 26.\nChapter 27 from R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). View Chapter 27."
  },
  {
    "objectID": "lesson05/index.html#overview",
    "href": "lesson05/index.html#overview",
    "title": "Lesson 5",
    "section": "Overview",
    "text": "Overview\nComputer programming is the art and science of instructing computers to perform specific tasks or solve problems. It’s like giving step-by-step directions to a computer to execute. Programmers use programming languages, which are like the computer’s communication tools, to write code. This code is a set of instructions that tells the computer what to do. In the world of data science, programming is an essential skill because it allows you to manipulate, analyze, and visualize data efficiently. Learning to code empowers you to create software, build data models, and automate repetitive tasks, making it a fundamental skill for anyone aspiring to excel in the field of data science.\nTwo of the most important programming concepts are iteration and functions. Both iteration and functions save us time, iterations by allowing us to perform repetitive tasks without repeating a command, and functions by allowing us to generalize commands/code and reuse code. In data science, iteration and functions are critical for tasks like data cleaning, analysis, and visualization. They enable us to process and analyze large datasets efficiently while maintaining clean, organized, and readable code.\nIn this lesson, we will learn how to implement functions and certain types of iteration in R."
  },
  {
    "objectID": "lesson05/index.html#functions-in-r",
    "href": "lesson05/index.html#functions-in-r",
    "title": "Lesson 5",
    "section": "  Functions in R",
    "text": "Functions in R\nA function1 is a self-contained block of code that performs a specific task or set of tasks. Functions are fundamental to programming and are used to encapsulate and modularize code, making it more organized, reusable, and easier to maintain. Some reasons to write functions include:\n\nReusability: Functions in R allow you to encapsulate logic and reuse it throughout your code. This promotes code modularity, reduces redundancy, and simplifies maintenance.\nAbstraction: Functions provide a level of abstraction, allowing you to focus on what a piece of code does rather than how it does it. This enhances code readability and comprehension.\nScoping: Lexical scoping in R enables functions to maintain their own environments, which is useful for creating closures and encapsulating state.\nExtensibility: R’s extensibility through user-defined functions enables you to tailor your code to specific tasks or domains, making it a versatile language for data analysis and statistics.\nError Handling: Functions in R can include error-handling mechanisms to gracefully handle unexpected situations, enhancing the robustness of your code.\nTesting: Functions make it easier to write unit tests for your code, ensuring its correctness and reliability in data analysis pipelines.\nFunctional Programming: R supports functional programming paradigms, allowing you to use higher-order functions like apply, lapply, and sapply, which simplify data manipulation and analysis tasks. Further, the packages purrr and furrr enhance the use of functional programming in R.\n\n\nA good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code).\n\nHere are the key components and characteristics of functions in R:\n\nFunction Name: Every function in R has a name that uniquely identifies it. Function names can consist of letters, numbers, dots, and underscores but should not start with a number or contain spaces.\nParameters (Arguments): Functions can accept zero or more parameters, also known as arguments. Parameters are values that you pass to the function, and the function can use these values to perform its tasks. Parameters are enclosed in parentheses following the function name.\nFunction Body: The function body contains the actual code that defines what the function does. It consists of a sequence of R expressions and statements that are executed when the function is called. The function body is enclosed in curly braces {}.\nReturn Value: Functions in R can return a value, which is the result of their computations. You specify the return value using the return() keyword. If a function doesn’t explicitly use return(), it returns the value of the last evaluated expression by default.\n\nHere’s a basic example of a function in R that adds two numbers:\n\nadd_numbers &lt;- function(a, b) {\n  result &lt;- a + b\n  return(result)\n}\n\nIn this example:\n\nadd_numbers is the function name.\n(a, b) are the function’s parameters.\nThe function body calculates the sum of a and b and assigns it to the variable result.\nThe return(result) statement specifies that the function should return the value stored in result.\n\nYou can call this function with specific values for a and b to get the result:\n\n(result &lt;- add_numbers(3, 5))\n\n[1] 8\n\n\nExercise: Write an R function named mult_numbers that inputs two numbers and returns their product.\nLet’s examine some more examples of writing and working with functions together in an R session."
  },
  {
    "objectID": "lesson05/index.html#iteration-with-purrr",
    "href": "lesson05/index.html#iteration-with-purrr",
    "title": "Lesson 5",
    "section": "  Iteration with purrr",
    "text": "Iteration with purrr\nFunctional programming, as implemented in R using packages like purrr, offers several advantages that make it particularly useful in data science:\n\nReadability and Maintainability: Functional programming encourages writing code that is concise and easier to understand. It emphasizes the use of pure functions, which take inputs and produce outputs without modifying external state. This makes code more predictable and less prone to bugs, making it easier to maintain, especially in data science projects where the logic can become complex.\nModularity: Functional programming promotes modularity by breaking down complex operations into smaller, reusable functions. In data science, where you often perform similar data manipulations or analyses on different datasets, this modularity can save time and effort by allowing you to reuse code snippets efficiently.\nParallelization: Functional programming encourages the use of pure functions that don’t rely on external state or side effects. This property makes it easier to parallelize code, taking advantage of multi-core processors for faster data processing, a significant benefit in data science, where working with large datasets can be time-consuming. The future and furrr packages implement parallel processing versions of the map family of functions.\nError Handling: Functional programming encourages a more structured approach to error handling, leading to more robust and predictable error handling in data analysis pipelines.\nFunctional Pipelines: Functional programming promotes the use of pipelines, where data flows through a series of transformations. This aligns well with the typical data preprocessing and analysis steps in data science. Libraries like purrr in R make it easy to build and read such pipelines, enhancing code clarity.\nData Transformation: In data science, you often need to perform various transformations on data, such as mapping, filtering, and summarizing. Functional programming’s emphasis on these operations aligns well with the typical tasks in data cleaning and preprocessing.\nFunctional Libraries: R has a rich ecosystem of packages like purrr, dplyr, and tidyr that are designed with functional programming principles in mind. These packages provide powerful tools for data manipulation and analysis while promoting clean, readable code.\n\nIn this class, we will emphasize the use of the map family of functions from the purrr package which is part of tidyverse. The map family of functions have the following structure:\nmap(.x,.f)\nwhere .x is some data object in R and .f is an R function. The map functions evaluate the function .f on each unit of the data in object .x and then return some other type of R object. Figure 1 illustrates the basic functioning of map-type functions.\n\n\n\n\n\nFigure 1: Visual illustration of how map works. Figure taken from (Wickham 2019).\n\n\n\n\nLet’s see a very simple example of using purrr. Suppose we have a vector of numbers and we want to square each of them. One way to do this would is as follows:\n\nmy_sqr &lt;- function(x){\n  return(x^2)\n}\n\nnums_to_square &lt;- c(-1,4,6)\n\nmap(nums_to_square,my_sqr)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 16\n\n[[3]]\n[1] 36\n\n\nNote that map returns a list. What if we just wanted to return another vector? Since my_sqr returns numerical values, we could replace map with map_dbl like\n\nmap_dbl(nums_to_square,my_sqr)\n\n[1]  1 16 36\n\n\nNow the result is a vector. There is a whoe family of different kinds of map-like functions. For example, there are map-type functions that return data frames and we will find these to be very useful in our later work.\nIt’s a good idea to examine the documentation for purrr, view the documentation.\nExercise: Write a function called times_two that inputs a number and returns double its value. Use map_dbl to multiply all the numbers from 1 to 6 by 2.\nLet’s examine some more examples of using purrr methods together in an R session."
  },
  {
    "objectID": "lesson05/index.html#preparation-for-the-next-lesson",
    "href": "lesson05/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 5",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nChapter 10 on statistical inference from Data Science A First Introduction (Timbers, Campbell, and Lee 2022)."
  },
  {
    "objectID": "lesson05/index.html#references",
    "href": "lesson05/index.html#references",
    "title": "Lesson 5",
    "section": "References",
    "text": "References\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley. 2019. Advanced r. CRC press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-04\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n dplyr       * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson06/index.html",
    "href": "lesson06/index.html",
    "title": "Lesson 6",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nIdentify and describe some common probability distributions.\nState Bayes’ theorem.\nUnderstand the concepts of probability mass functions (PMFs), probability density functions (PDFs), and cumulative distribution functions (CDFs).\nWork with various distributions in R."
  },
  {
    "objectID": "lesson06/index.html#learning-objectives",
    "href": "lesson06/index.html#learning-objectives",
    "title": "Lesson 6",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nIdentify and describe some common probability distributions.\nState Bayes’ theorem.\nUnderstand the concepts of probability mass functions (PMFs), probability density functions (PDFs), and cumulative distribution functions (CDFs).\nWork with various distributions in R."
  },
  {
    "objectID": "lesson06/index.html#readings-etc.",
    "href": "lesson06/index.html#readings-etc.",
    "title": "Lesson 6",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:"
  },
  {
    "objectID": "lesson06/index.html#preparation-for-the-next-lesson",
    "href": "lesson06/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 6",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nChapter 5 on classification and Chapter 7 on regression from Data Science A First Introduction (Timbers, Campbell, and Lee 2022)."
  },
  {
    "objectID": "lesson06/index.html#references",
    "href": "lesson06/index.html#references",
    "title": "Lesson 6",
    "section": "References",
    "text": "References\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-16\n pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n sessioninfo * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson06/index.html#overview",
    "href": "lesson06/index.html#overview",
    "title": "Lesson 6",
    "section": "Overview",
    "text": "Overview\nProbability and random variables play a fundamental role in data science and machine learning by providing the theoretical foundation for understanding uncertainty and variability in data. Later in the course, we will study inference and predictive modeling and it will be helpful to have the basic vocabulary of probability and random variables at our disposal to help describe methods for inference and predictive modeling.\nInformally:\n\nProbability is the mathematical framework for quantifying uncertainty. In data science, it allows us to express how likely different outcomes are, providing a way to model the inherent variability in data. Probability is used to formulate hypotheses, make predictions, and assess the likelihood of various events occurring. Understanding probability is crucial when dealing with tasks such as statistical inference, hypothesis testing, machine learning, and Bayesian modeling.\nRandom variables are a key concept that extends the idea of probability. A random variable is a variable whose value is subject to chance or randomness. In data science and machine learning, random variables are used to represent uncertain quantities that we wish to analyze. They can be discrete, like the outcome of a coin toss, or continuous, like the height of individuals in a population. Random variables help us describe and model data, and they serve as the basis for statistical models and machine learning algorithms.\n\nThere are other data science and mathematics courses in our curriculum that go into more detail on probability and random variables so our treatment in this course will be brief and informal."
  },
  {
    "objectID": "lesson05/index.html#footnotes",
    "href": "lesson05/index.html#footnotes",
    "title": "Lesson 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere we use the word function in the context of computer programming and note in the mathematical sense. While there are important relationships between mathematical and programming functions, there are also some distinctions. Mathematical functions can be implemented as progamming functions but many programming functions aren’t functions in the strict mathematical sense.↩︎"
  },
  {
    "objectID": "lesson06/index.html#introduction-to-probability",
    "href": "lesson06/index.html#introduction-to-probability",
    "title": "Lesson 6",
    "section": "Introduction to Probability",
    "text": "Introduction to Probability\nThe notion of probability starts with the concept of a random experiment, that is, a process with a well-defined but unpredictable outcome. A quintessential example is provided by tossing a coin. We know in advance that the outcome will be either heads or tails but it is difficult to determine with certainty the outcome of any given toss.\nA working definition for probability that is often used is\n\nFor an observation of a random experiment, the probability of a particular outcome is the proportion of times that outcome would occur in an indefinitely long sequence of like observations, under the same conditions.\n\nThis definition is why one typically says that the probability of tossing heads for a fair coin is 0.5 (or 50%).\nNote: According to our working definition a probability is always a number between 0 and 1 (inclusive).\nFor a random experiment, we define the sample space, often denoted by \\(S\\) to be the set of all possible outcomes. So, the sample space for the random experiment of tossing a coin one time would be \\(S = \\{\\text{heads},\\text{tails}\\}\\). An event is a subset of a sample space. We usually denote events by upper case latin letters like \\(A\\). For example, the event of tossing a heads would be \\(A = \\{\\text{heads}\\}\\).\nWhile it takes some time and effort to wrap one’s head around this, it is a fact that we can build up complicated events through logical operations. For example, we can define an event that is the event of tossing either heads or1 tails. Also, one can define an event that is the event of tossing heads and tails.\nQuestion: Suppose that \\(A\\) is the event of tossing either heads or tails in a single toss of a coin. Why should we say that the probability of this event is 1? Suppose that \\(B\\) is the event of tossing both heads and tails in a single toss of a coin. Why should we say that the probability of this event is 0?\nNotation: If \\(A\\) is an event, we denote by \\(P(A)\\) the probability that \\(A\\) has occurred. Given two events \\(A\\) and \\(B\\), we denote by \\(A \\cup B\\) the event that \\(A\\) or \\(B\\) occurred; we denote by \\(A \\cap B\\) the event that \\(A\\) and \\(B\\) occurred.\nA notion from probability that is very important in data science is that of conditional probability. A probability of an event \\(A\\), given that an event \\(B\\) occurred, is called a conditional probability and is denoted by \\(P(A | B)\\) and read as “the probability of \\(A\\) given \\(B\\)”. We define conditional probability mathematically by\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nIt is important to realize that in general \\(P(A|B) \\neq P(B|A)\\). For example, suppose that \\(A\\) is the event that I carry an umbrella on any given day and \\(B\\) is the event that it is raining on a given day. Then,\n\\[\nP(A|B) = \\text{the probability I will carry an umbrella given that it is raining}\n\\]\nwhile\n\\[\nP(B|A) = \\text{the probability it is raining given that I'm carrying an umbrella}\n\\]\nand there is no reason to believe that these probabilities should be the same.\nHowever, there is an important connection between \\(P(A|B)\\) and \\(P(B|A)\\) known as Bayes’ theorem which we state as\n\\[\nP(B|A) = \\frac{P(A|B)P(B)}{P(A)} = \\frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|B^{C})P(B^{C})}\n\\] whenever \\(A\\) is an event such that \\(P(A) \\neq 0\\) and where \\(B^{C}\\) denotes the complement of the event \\(B\\).\nLet’s examine a simple example. Suppose that we toss a fair coin three times and record the number of heads. Let\n\n\\(B =\\) the event that the first toss is heads, and\n\\(A =\\) the event that at least one toss is heads.\n\nThen,\n\n\\(P(B|A) =\\) the probability that the first toss is heads given that at least one toss is heads, and\n\\(P(A|B) =\\) the probability that at least one toss is heads given that the first toss is heads.\n\nIt is probably obvious that \\(P(A|B) = 1\\), and with just a little thought, one can reason out that \\(P(A) = \\frac{7}{8}\\) and \\(P(B) = \\frac{1}{2}\\). Then, by Bayes’ theorem we should have\n\\[\nP(B|A) = \\frac{1 \\cdot \\frac{1}{2}}{\\frac{7}{8}} = \\frac{4}{7}\n\\] It is often more convenient to work with numbers that it is to work with events from general sample spaces. Random variables are a mathematical tool for quantifying events.\nWe say that two events \\(A\\) and \\(B\\) are independent if \\(P(A|B) = P(A)\\). For example, suppose that we toss a fair coin three times and record the number of heads. Let \\(A\\) be the event that the first toss is heads and \\(B\\) be the event that the second toss is heads. Then, \\(A\\) and \\(B\\) are independent events. Furthermore, notice that \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\), and \\(P(A \\cap B) = \\frac{1}{4}\\). In general, whenever two events are independent, we have\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]"
  },
  {
    "objectID": "lesson06/index.html#introduction-to-random-variables",
    "href": "lesson06/index.html#introduction-to-random-variables",
    "title": "Lesson 6",
    "section": "Introduction to Random Variables",
    "text": "Introduction to Random Variables\nOur working definition of a random variable will be:\n\nFor a random phenomenon, a random variable is a function that assigns a numerical value to each point in the sample space.\n\nWe generally denote random variables by upper case latin letters near the end of the alphabet such as \\(X\\), \\(Y\\), etc. We use the corresponding lower case latin letters to denote that output values of a random variable. For example, if \\(X\\) is a radom variable and \\(A\\) is some event, then we could write \\(X(A) = x\\).\nConsider again, our example of tossing a coin three times. We can define a random variable by simply returning the number of heads in a given toss. If we denote this random variable by \\(X\\), then \\(X(\\{HHT\\}) = 2\\) while \\(X(\\{THT\\}) = 1\\).\n\nA probability distribution lists the possible outcomes for a random variable and their probabilities.\n\nFor example, if \\(X\\) is our random variable that returns the number of heads out of three tosses of a fair coin, then the probability distribution for \\(X\\) is provided by Table 1.\n\n\nTable 1: Probability for our example random variable that counts the number of heads out of three tosses of a fair coin.\n\n\nEvent\n\\(x\\)\nProbability\n\n\n\n\n\\(\\{TTT\\}\\)\n0\n\\(\\frac{1}{8}\\)\n\n\n\\(\\{HTT,THT,TTH\\}\\)\n1\n\\(\\frac{3}{8}\\)\n\n\n\\(\\{HHT,THH,HTH\\}\\)\n2\n\\(\\frac{3}{8}\\)\n\n\n\\(\\{HHH\\}\\)\n3\n\\(\\frac{1}{8}\\)\n\n\n\n\nIt is convenient to distinguish between two common types of random variables:\n\nA discrete random variable is one with output values from a discrete, countable set.\nA continuous random variable is one for which the output values form an interval in the set of all real numbers2. That is, continuous random variables have an infinite continuum of possible output values.\n\nOut coin tossing random variable provides an example of a discrete random variable. Discrete random variables are essentially described by a probability mass function (PMF) \\(p((x)\\) that generate the probabilities for the possible outcomes of a random variable. For example, the last two columns of Table 1 give a PMF for our example random variable that counts the number of heads out of three tosses of a fair coin.\nIn general, if \\(X\\) is a discrete random variable then,\n\n\\(0 \\leq p(x) \\leq 1\\) for all output values \\(x\\) for \\(X\\), and\n\\(\\sum_{x}p(x) = 1\\), where the sum is taken over all possible output values for \\(X\\).\n\n\nBinomial Distributions\nSuppose that \\(X\\) is a random variable that takes on values \\(0,1,\\ldots n\\) and let \\(\\pi\\) denote a real number in the interval \\([0,1]\\). Then we say that \\(X\\) is a binomial random variable if it’s PMF is given by\n\\[\np(x) = \\frac{n!}{(n-x)! x!}\\pi^{x}(1 - \\pi)^{n -x}\n\\]\nFor example, if we let \\(X\\) be the random variable that counts the number of heads out \\(n\\) tosses of a coin where the probability of landing heads in a single toss is \\(\\pi\\), then \\(X\\) is modeled by a binomial random variable.\nLater we will see how to work with binomial distributions in R.\n\n\nNormal Distributions\nContinuous random variables are described by a probability density function (PDF) \\(f(x)\\) where the total area under the curve \\(y=f(x)\\) and above the \\(x\\)-axis is 1. Probably the most well-known type of continuous random variable is the normal family of random variables which have a PDF of the form\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^{2}}}\n\\]\nwhere this time \\(\\pi\\) is the constant that is the ratio of a circle’s circumference to its diameter.\nLater we will see how to work with normal distributions in R.\n\n\nExpected Value and Variance\nIf \\(X\\) is a discrete random variable with PMF \\(p(x)\\), then the expected value of \\(X\\), denoted by \\(\\text{E}[X]\\) is defined by\n\\[\n\\text{E}[X] = \\sum_{x}xp(x)\n\\]\nwhere the sum is taken over all possible output values for \\(X\\). It is also possible to define the expected value for continuous random variables but for now we will be satisfied to simply also use \\(\\text{E}[X]\\) to denote the expected value for a continuous random variable.\nWe define the variance \\(\\text{Var}[X]\\) of a random variable \\(X\\) (continuous or discrete) to be the number\n\\[\n\\text{Var}[X] = \\text{E}[(X - \\text{E}[X])^2]\n\\]\nFor a discrete random variable \\(X\\) with PMF \\(p\\), we have that\n\\[\n\\text{Var}[X] = \\sum_{x}(x - \\mu)^2p(x)\n\\] where the sum is taken over all possible values of \\(x\\) and \\(\\mu = \\text{E}[X]\\).\nThe standard deviation of a random variable is defined to be the positive square root of its variance.\n\nProperties of Expected Value and Variance\nThere are some important properties that are helpful to know about expected value and variance. Namely,\n\nIf \\(X\\) and \\(Y\\) are random variables, then \\(\\text{E}[X + Y] = \\text{E}[X] + \\text{E}[Y]\\).\nIf \\(a\\) is a number and \\(X\\) is a random variable, then \\(\\text{E}[aX] = a\\text{E}[X]\\).\nIf \\(a\\) and \\(b\\) are numbers and \\(X\\) is a random variable, then \\(\\text{Var}[aX + b] = a^2\\text{Var}[X]\\).\n\n\nFor any random variable \\(X\\), the probability \\(P(X \\leq x)\\) is a function of \\(x\\) that returns the probability that \\(X\\) takes values less or equal to \\(x\\). Such a function is called the cumulative distribution function (CDF) for \\(X\\).\n\nTable 2 includes a column with the output values for the CDF for each value of \\(x\\) for the random variable that counts the number of heads out of three tosses of a fair coin.\n\n\nTable 2: PMF and CDF for our example random variable that counts the number of heads out of three tosses of a fair coin.\n\n\nEvent\n\\(x\\)\nProbability\nCDF\n\n\n\n\n\\(\\{TTT\\}\\)\n0\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{8}\\)\n\n\n\\(\\{HTT,THT,TTH\\}\\)\n1\n\\(\\frac{3}{8}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{HHT,THH,HTH\\}\\)\n2\n\\(\\frac{3}{8}\\)\n\\(\\frac{7}{8}\\)\n\n\n\\(\\{HHH\\}\\)\n3\n\\(\\frac{1}{8}\\)\n\\(1\\)\n\n\n\n\n\n\n\nExpected Value and Variance for Binomial and Normal R.V.s\nSuppose that \\(X\\) is a normal random variable with PDF\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^{2}}}\n\\]\nthen \\(\\text{E}[X] = \\mu\\) and \\(\\text{Var}[X] = \\sigma^{2}\\).\nNow, suppose that \\(X\\) is a binomial random variable with PMF\n\\[\np(x) = \\frac{n!}{(n-x)! x!}\\pi^{x}(1 - \\pi)^{n -x}\n\\]\nthen \\(\\text{E}[X] = n\\pi\\) and \\(\\text{Var}[X] = n\\pi(1-\\pi)\\).\n\n\nSome Common Distributions\nTable 3 lists some common distributions together with their type. The name of each distribution is a link to the Wikipedia article on that distribution.\n\n\nTable 3: Some common single-variable distributions and their type.\n\n\nName\nType\n\n\n\n\nGeometric\nDiscrete\n\n\nBinomial\nDiscrete\n\n\nHypergeometric\nDiscrete\n\n\nMultinomial\nDiscrete\n\n\nNegative binomial\nDiscrete\n\n\nPoisson\nDiscrete\n\n\nBeta\nContinuous\n\n\nGamma\nContinuous\n\n\nExponential\nContinuous\n\n\nNormal\nContinuous\n\n\nUniform\nContinuous"
  },
  {
    "objectID": "lesson06/index.html#footnotes",
    "href": "lesson06/index.html#footnotes",
    "title": "Lesson 6",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, “or” is used in the inclusive sense.↩︎\nThis is an imprecise and limited definition of continuous random variables but to make the definition precise requires mathematical machinery that is far outside the scope of this course.↩︎"
  },
  {
    "objectID": "lesson06/index.html#working-with-distributions-in-r",
    "href": "lesson06/index.html#working-with-distributions-in-r",
    "title": "Lesson 6",
    "section": "  Working with Distributions in R",
    "text": "Working with Distributions in R\nMany probability distributions are implemented in base R and many more can be access via various packages. We will focus on those distributions that are implemented in base R. Suppose that distname is the name associated with a distribution that is implemented in R. Then there are typically four R functions associated with that distribution:\n\nddistname implements the PMF or PDF for the distribution.\npdistname implements the CDF for the distribution.\nqdistname implements the inverse CDF for the distribution.\nrdistname implements sampling from the distribution.\n\nLet’s work through some examples together in an RStudio project. You can access the relevant code on either the course learning management system or via this GitHub repo."
  },
  {
    "objectID": "lesson07/index.html",
    "href": "lesson07/index.html",
    "title": "Lesson 7",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine supervised machine learning in general terms.\nDistinguish between regression and classification problems.\nUnderstand the concept of error and the significance of test versus training error.\nAppreciate the trade-off between model flexibility and interpretability, and between bias and variance.\nDescribe the nearest neighbors, linear regression, and neural networks models for supervised machine learning."
  },
  {
    "objectID": "lesson07/index.html#learning-objectives",
    "href": "lesson07/index.html#learning-objectives",
    "title": "Lesson 7",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine supervised machine learning in general terms.\nDistinguish between regression and classification problems.\nUnderstand the concept of error and the significance of test versus training error.\nAppreciate the trade-off between model flexibility and interpretability, and between bias and variance.\nDescribe the nearest neighbors, linear regression, and neural networks models for supervised machine learning."
  },
  {
    "objectID": "lesson07/index.html#readings-etc.",
    "href": "lesson07/index.html#readings-etc.",
    "title": "Lesson 7",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapters 5, 6, 7, & 8 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nRead Chapter 2 of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nThe following two video lectures are also recommended:\n\n\nMotivating problems for machine (statistical) learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSupervised and unsupervised learning. Watch video on YouTube."
  },
  {
    "objectID": "lesson07/index.html#overview",
    "href": "lesson07/index.html#overview",
    "title": "Lesson 7",
    "section": "Overview",
    "text": "Overview\nThis lesson is essentially about modeling data. Specifically, we are interested to model data that is paired or labeled, where we view one or more variables as predictors and one variable as a corresponding response or label. Let’s consider a motivating example. The penguins data set available through either the modeldata package or the palmerpenguins package records observations on 344 penguins from three species of penguins collected from three islands in the Palmer Archipelago, Antarctica. Table 1 shows the first 10 rows of the data set.\n\n\n\n\n\nTable 1: The penguins data set records observations on 344 penguins from three species of penguins collected from three islands in the Palmer Archipelago, Antarctica.\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n\n\nAdelie\nTorgersen\n34.1\n18.1\n193\n3475\nNA\n\n\nAdelie\nTorgersen\n42.0\n20.2\n190\n4250\nNA\n\n\n\n\n\n\n\n\nOne problem we might be interested in is to model the body mass of a penguin based on the three other body measurements and its sex and species. Here body mass would be our response variable and bill length, bill depth, flipper length, species, and sex would be our predictor variables. In this case, since the response variable is numerical, we say that we have a regression problem. Figure 1 illustrates this regression problem.\n\n\n\n\n\nFigure 1: A regression problem for the penguins data set.\n\n\n\n\nInstead, we might be interested to model the sex of a penguin based on its physical measurements and its species. In this case, since the response is categorical, we say that we have a classification problem. Figure 2 illustrates this classification problem.\n\n\n\n\n\nFigure 2: A classification problem for the penguins data set.\n\n\n\n\nOur approach to modeling data will be via supervised machine learning, also known as statistical learning that uses data to build mathematical models to model and gain insight from data. The methods of machine learning are currently very popular in data science for the role they play in predictive modeling but are also commonly used for inferential purposes. Machine learning is also a currently prominent approach to developing artificial intelligence technologies. Figure 3 illustrates the relationship between AI, machine learning, and deep learning. Of course, what one means by a “useful insight” is highly dependent on the domain of specialization or area of application. Thus, machine learning is an inherently interdisciplinary field that intersects with many disciplines such as computer science, data science, mathematics and statistics and a variety of other fields.\n\n\n\nFigure 3: Illustration credit: https://vas3k.com/blog/machine_learning/\n\n\nSupervised machine learning builds models to predict response values based on corresponding predictor values by using example data that comes as a source and target pair. Figure 4 illustrates the supervised learning paradigm.\n\n\n\nFigure 4: Supervised machine learning builds models to predict response values based on corresponding predictor values by using example data that comes as a source and target pair. Illustration credit: https://www.javatpoint.com\n\n\nIn this lesson, we will learn the basic concepts of supervised machine learning. We will use the nearest neighbors algorithm to facilitate our understanding of the basic principles of supervised learning. This algorithm is a simple but useful machine learning algorithm. In future lessons, we will learn about additional commonly used supervised learning methods."
  },
  {
    "objectID": "lesson07/index.html#introduction-to-machine-statistical-learning",
    "href": "lesson07/index.html#introduction-to-machine-statistical-learning",
    "title": "Lesson 7",
    "section": "Introduction to Machine (Statistical) Learning",
    "text": "Introduction to Machine (Statistical) Learning\nMachine learning or statistical learning generally refers to methods or tools that seek to derive insight or understanding from data by using models. Here by model we mean a mathematical or computational representation of some part of the real world. In machine learning, we fit or learn a model or class of models to data. The goal of fitting models is usually one of the following:\n\nPrediction - using what is known or has been observed to make informed (hopefully accurate) claims about what we want to know or has yet to be observed.\nInference - using a sample to make informed (hopefully accurate) claims about a larger population. For example, we might want to know which predictors are associated with a response, or what is the relationship between the response and each predictor.\n\nFor an example of prediction, suppose that we are advertising experts working with a customer that sales video games. Our customer cannot directly control their sales but they can directly control their marketing by deciding how much to invest in advertising. Say for example that our customer has three ways to advertise: via YouTube, via podcasts, or via Spotify. We can use our past knowledge about how much our customer has spent i.e., their advertising budget and the corresponding sales to make predictions using a model about how sales will be in the future depending on how the company changes its advertising in each of the three media.\nFor an example of inference, suppose we want to know if caffeine consumption is associated with exam performance for students at the University of Scranton. We could collect data for a number of students on how much caffeine they’ve had before and exam and then record the corresponding exam performance. Here we probably aren’t interested to predict what grade someone will get based on consuming a certain amount of caffeine but rather we are interested in whether or not there is an association between caffeine consumption and exam performance. We could use a model to make inferences about this association.\nWhile this course will only look at supervised learning, it’s worth taking a moment to point out that there are actually two prominent broad classes of machine learning models:\n\nSupervised - In supervised learning, data comes in pairs \\((y_{i},{\\bf x}_{i})\\) where we view \\({\\bf x}_{i}\\) (which may be a vector) as a predictor and \\(y_{i}\\) as a response. Often, We the predictors are something we can influence directly like the advertising budget from our earlier example while the response is something we don’t have direct control over like the sales from our example. Thus, there is an assumed functional relationship between predictors and the response of the form\n\n\\[\ny = f({\\bf x}) + \\epsilon\n\\]\nwhere we think of \\(f({\\bf x})\\) as the mean value for \\(y\\) viewed as a random variable and \\(\\epsilon\\) as containing the variance of \\(y\\) so that \\(E[\\epsilon] = 0\\).\nWe note that \\(y\\) may be numerical in which case we have a regression problem or it may be categorical in which case we have a classification problem.\n\nUnsupervised - In unsupervised learning, there is no response variable. Some common unsupervised problems include clustering and density estimation. Both of these essentially seek to discover a pattern in the data.\n\nFigure 5 illustrates the distinction between supervised and unsupervised learning models.\n\n\n\nFigure 5: Illustration credit: https://vas3k.com/blog/machine_learning/\n\n\nIn this course, we will focus on supervised learning and save discussions on unsupervised learning for future courses.\n\nFitting Supervised Models\nFitting a supervised learning model typically amounts to estimating the function \\(f\\) in the assumed relationship\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] between the predictor and response variables. When we estimate \\(f\\) we denote the estimate by \\(\\hat{f}\\). Then, we can use \\(\\hat{f}\\) to predict the response for each predictor value \\({\\bf x}\\) by computing\n\\[\n\\hat{y} = \\hat{f}({\\bf x})\n\\]\nHow do we estimate a function \\(f\\)? In machine learning, we use the data together with some algorithm to construct \\(\\hat{f}\\). The general steps are:\n\nSpecify a class of functions from which to choose \\(\\hat{f}\\).\nSpecify a loss function that measures how well a given \\(\\hat{f}\\) fits the data. That is, the loss functions is a quantitative comparison between the observed response values and predicted response values. The loss function is often denoted by \\(L(y,\\hat{y}) = L(y,\\hat{f}({\\bf x}))\\).\nFind the \\(\\hat{f}\\) that minimizes the loss function. Note that this involves solving an optimization problem.\n\n\nRegression\nLet’s consider an illustrative example where \\({\\bf x}\\) represents the years of education of some individuals and \\(y\\) is the income they earn in their profession. Thus, both variables are numerical so we are dealing with a regression problem. We are assuming that there is a true but unknown functional relationship between the years of education and the income they earn.\nThe left panel of Figure 6 shows a scatter plot of our education versus income data while the right panel shows the data again but with a curve corresponding to the graph of a function \\(\\hat{f}\\) that passes through the data.\n\n\n\nFigure 6: Illustration of supervised learning through a regression problem. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nHow did we come up with the function \\(\\hat{f}\\)? Basically, we minimized the residual error between our predicted and observed response. That is, for each response value \\({\\bf x}\\) we minimized how far \\(y=f({\\bf x})\\) can be from \\(\\hat{y}=\\hat{f}({\\bf x})\\). There are three important points that need to be addressed before we can implement regression in a practical situation.\n\nThe set of all functions is too large to work with in practice so we must make some choices that allow us to narrow down the class of functions from which \\(\\hat{f}\\) will be taken. For example, we could restrict to only linear functions, or only quadratic functions, or only polynomial functions. These classes of functions are easy to describe because these types of functions are uniquely described by a finite number of parameters. However, sometimes data can not be modeled well by, e.g., polynomials so more sophisticated non-parametric ways of describing classes of functions have been developed that allow for more flexible modeling.\nWe must decide on how we will define and measure error. That is, we must specify an appropriate loss function. For regression problems, a typical way to measure error is the squared-error. Referring back to the right side of Figure 6, we define the \\(i\\)-th residual \\(r_{i}\\) to be the vertical (signed) distance between the observed response value \\(y_{i}\\) and the corresponding predicted value \\(\\hat{y}_{i} = \\hat{f}({\\bf x}_{i})\\). That is,\n\n\\[\nr_{i} = y_{i} - \\hat{y}_{i}\n\\] Then the squared error (SE) is\n\\[\n\\text{SE} = \\sum_{i=1}^{n}r_{i}^{2} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2} = \\sum_{i=1}^{n}(y_{i} - \\hat{f}({\\bf{x}_{i}}))^{2}\n\\]\nIn this case, we take \\(\\hat{f}\\) to be the function from some specified class of functions such that it minimizes the corresponding SE.\nImportant Point: A main component of many if not most supervised machine learning problems is solving some kind of optimization problem. Usually when one speaks of a machine learning algorithm (or learning algorithm), what they are actually referring to is some algorithm that is used to solve an appropriate optimization problem.\n\nWe have to distinguish between reducible error and irreducible error. No machine learning model will ever be perfect. Suppose that we have an estimate \\(\\hat{f}\\) that yields a prediction \\(\\hat{y} = \\hat{f}({\\bf x})\\). Since in reality the response is a random variable\n\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] we have\n\\[\n\\begin{align*}\n\\text{E}[(y - \\hat{y})^{2}] &= \\text{E}[(f({\\bf x}) + \\epsilon - \\hat{f}({\\bf x}))^2] \\\\\n&= \\text{E}[((f({\\bf x}) - \\hat{f}({\\bf x})) + \\epsilon)^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2 - 2\\epsilon (f({\\bf x}) - \\hat{f}({\\bf x})) + \\epsilon^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2] - 2(f({\\bf x}) - \\hat{f}({\\bf x}))\\text{E}[\\epsilon] + \\text{E}[(\\epsilon - 0)^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2] + \\text{Var}[\\epsilon]\n\\end{align*}\n\\]\nBy choosing a good enough family of functions or a good enough learning algorithm we can reduce \\(\\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2]\\) as much as we want. This corresponds to the reducible error. However, we have no control over \\(\\text{Var}[\\epsilon]\\) and this corresponds to the irreducible error.\n\n\nClassification\nFor classification problems in supervised machine learning, the response variable is categorical. Figure 7 illustrates this, showing a scatter plot of data where coloring is used to distinguish the data points as belonging to one of two different classes.\n\n\n\nFigure 7: Illustration of a classification problem in which the response variable is a binary categorical variable. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nFor classification problems, our goal is still to estimate a functional relationship of the form \\(y = f({\\bf x}) + \\epsilon\\). However, we can no longer measure error using the squared error because the response values are not numerical. A common method for measuring error in classification problems is classification error (CE) defined by\n\\[\n\\text{CE} = \\sum_{i=1}^{n}I(y_{i} \\neq \\hat{y}_{i})\n\\]\nwhere \\(I\\) is the indicator function that is equal to 1 whenever \\(y_{i} \\neq \\hat{y}_{i}\\) and equal to 0 whenever \\(y_{i} = \\hat{y}_{i}\\). Essentially, CE counts the number of misclassifications.\nSimilar to regression, fitting a classification model involves finding a function \\(\\hat{f}\\) from some specified class of functions such that the corresponding CE is minimized.\nNote that it is possible to convert a regression problem to a classification problem by binning or discretizing the response variable in some way.\n\n\n\nComplexity Vs. Interpretability\nAnother issue that needs to be taken into account when fitting models is the tradeoff between how easy it is to interpret a model versus the maximum degree of accuracy for the model. Figure 8 illustrates this through a representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity.\n\n\n\nFigure 8: A representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity. Figure from (Tibshirani, James, and Trevor 2017).\n\n\n\n\nTraining Error Vs. Test Error\nWhen we fit a model to data, say by minimizing the error the resulting estimate function we get depends on the data used to fit the model. We refer to this data as the training data and the corresponding error as the training error. By choosing a sufficiently flexible set of functions from which to fit to the data, we can make the training error as small as we want. This might seem like a great thing, but there is a major problem with it.\nSuppose we want to use a model to make predictions about future unseen values of our predictor \\({\\bf x}\\). If a model is fit too well to the training data, then in general it tends not to be very good at making accurate predictions for future values. One says that models that are overfit to the training data are poor at generalization.\nHow do we build models that generalize well and avoid overfitting? A common approach is to separate data into a training set that is used to fit a model and a test set which is used to assess how well the models generalizes to unseen data via the test error. Figure 9 shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data.\n\n\n\nFigure 9: The left panel shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nWhile the training/test set approach to fitting accurate models while avoiding overfitting is very good in principle, there are some practical limitations. For example,\n\nHow do we know the training data is sufficiently representative?\nWhat if we don’t have a sufficiently large data set to split into a training and a test set?\nHow do we know what the minimum possible test error is?\n\nWe will spend a lot of time later talking more about these issues and ways to deal with them.\n\n\nThe Bias-Variance Trade-Off\nReferring back to Figure 9, notice the distinct U-shape in the curve for the test error. This is more than just a curiosity, it is the result of another type of trade-off known as the bias-variance trade-off.\nLet’s try to get a sense for this starting with some intuition. Suppose we having a regression problem with a single predictor. If we restrict to the class of linear functions, that is functions with graph that is a straight line in the plane, then any such function is uniquely specified by two parameters, the slope and intercept. Intuitively, such as model is highly biased because it’s going to make very rigid predictions. However, linear functions have low variance in the sense than models fit to similar data will have very similar slope and intercept values. On the other hand, a cubic polynomial being described uniquely by four parameters is much less biased than a linear function but will have higher variance.\nIt is outside the scope of this course, but it can be shown that the expected squared error for an observed value \\({\\bf x}_{0}\\) can be decomposed as follows:\n\\[\n\\text{E}[(y_{0} - \\hat{f}({\\bf x}))^2] = \\text{Var}(\\hat{f}({\\bf x}_{0})) + [\\text{Bias}(\\hat{f}({\\bf x}_{0}))]^2 + \\text{Var}(\\epsilon)\n\\]\nWe refer to the first two terms as\n\nthe variance of \\(\\hat{f}({\\bf x}_{0})\\)\nthe squared bias of \\(\\hat{f}({\\bf x}_{0})\\)\n\nFigure 10 illustrates this formula.\n\n\n\nFigure 10: Squared bias (blue curve), variance (orange curve), Var(ε) (dashed line), and test MSE (red curve). The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nImportant: What you should keep in mind as we proceed through the course is the following:\n\nSimple models tend to have high bias but much lower variance.\nComplex models tend to have lower bias but much higher variance.\n\nAnytime you choose a particular modeling approach for a specific application or data set, you should take into account the bias-variance trade-off."
  },
  {
    "objectID": "lesson07/index.html#some-data-sources",
    "href": "lesson07/index.html#some-data-sources",
    "title": "Lesson 7",
    "section": "Some Data Sources",
    "text": "Some Data Sources\nOne thing students often struggle with is finding and picking a good data set for their projects. Appendix B of the online textbook contains a very helpful list of data sources (Alexander, n.d.). View appendix B. Two other very interesting and useful sources of data are the Tidy Tuesday data repositories and Kaggle. There are also many R packages that either include data or that can be used to download data. The ROpenSci project is a good resource for finding R packages that can be used to obtain data, view the project."
  },
  {
    "objectID": "lesson07/index.html#preparation-for-the-next-lesson",
    "href": "lesson07/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 7",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead Chapter 8 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nYou may also want to read section 3.1 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nWatch the corresponding video lecture on regression. View on YouTube."
  },
  {
    "objectID": "lesson07/index.html#references",
    "href": "lesson07/index.html#references",
    "title": "Lesson 7",
    "section": "References",
    "text": "References\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-30\n pandoc   3.1.9 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n ggthemes     * 4.2.4   2021-01-20 [1] CRAN (R 4.3.0)\n infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson07/index.html#nearest-neighbors",
    "href": "lesson07/index.html#nearest-neighbors",
    "title": "Lesson 7",
    "section": "Nearest Neighbors",
    "text": "Nearest Neighbors\nThe nearest neighbors approach to supervised machine learning is a very simple and intuitive approach to modeling. For example, suppose we have a data set consisting of a single predictor \\({\\bf x}\\) and a response \\(y\\). The nearest neighbors approach to regression is to predict the response for a new value of \\({\\bf x}\\) by averaging the responses for the \\(k\\) nearest values of \\({\\bf x}\\) in the training data. Nearest neighbors can also be used for classification problems. In this case, we predict the class of a new value of \\({\\bf x}\\) by taking a majority vote of the classes of the \\(k\\) nearest values of \\({\\bf x}\\) in the training data.\nLet’s proceed to an online interactive demo for nearest neighbors applied to a classification problem. View the demo."
  },
  {
    "objectID": "lesson09/index.html",
    "href": "lesson09/index.html",
    "title": "Lesson 9",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe real-world examples of questions that can be answered with statistical inference.\nDefine common population parameters (e.g., mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.\nDefine the following statistical sampling terms: population, sample, population parameter, point estimate, and sampling distribution.\nDefine bootstrapping.\nUse R to create a bootstrap distribution to approximate a sampling distribution."
  },
  {
    "objectID": "lesson09/index.html#learning-objectives",
    "href": "lesson09/index.html#learning-objectives",
    "title": "Lesson 9",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe real-world examples of questions that can be answered with statistical inference.\nDefine common population parameters (e.g., mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.\nDefine the following statistical sampling terms: population, sample, population parameter, point estimate, and sampling distribution.\nDefine bootstrapping.\nUse R to create a bootstrap distribution to approximate a sampling distribution."
  },
  {
    "objectID": "lesson09/index.html#readings-etc.",
    "href": "lesson09/index.html#readings-etc.",
    "title": "Lesson 9",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapter 10 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nYou might also want to look at chapters 8 & 9 from Statistical Inference via Data Science."
  },
  {
    "objectID": "lesson09/index.html#motivation-for-linear-regression",
    "href": "lesson09/index.html#motivation-for-linear-regression",
    "title": "Lesson 9",
    "section": "Motivation for Linear Regression",
    "text": "Motivation for Linear Regression\nRecall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] and then we seek to find a function \\(\\hat{f}\\) from a predetermined class of functions that does a good job in approximating \\(f\\). Let’s study this problem in more detail but in a very simplest setting. Specifically, we will assume that \\({\\bf x}\\) and \\(y\\) are both single numerical variables and that \\(f\\) is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers \\(\\beta_{0}\\) and \\(\\beta_{1}\\) such that\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\] for all values of \\(x\\) and \\(y\\). Recall that we are assuming that \\(\\text{E}[\\epsilon] = 0\\) so \\(\\epsilon\\) is a random variable with expected value (or mean) equal to zero.\nIf we restrict ourselves to the class of single-variable linear functions, then finding an approximation to \\(f(x) = \\beta_{0} + \\beta_{1} x\\) is equivalent to finding values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) so that\n\\[\n\\hat{f}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x \\approx f(x) = \\beta_{0} + \\beta_{1} x\n\\] Thus, this would be a parametric model since any candidate approximating function is uniquely specified by specifying the values for the parameters \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\).\nFigure 1 shows the plot of data that has been generated by a relationship of the form \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\). You should examine the code used to create or simulate the data in this example and see how it relates to the expression \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\).\n\n\nCode\nset.seed(1287)\nN &lt;- 25\nx &lt;- rnorm(N,mean=72,sd=12)\ny &lt;- 1.2 + 0.75 * x + rnorm(N,sd=2)\nxy_data &lt;- tibble(x=x,y=y)\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point()\n\n\n\n\n\nFigure 1: A data set with two numerical variables \\(x\\) and \\(y\\) generated by an underlying linear function so that \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\).\n\n\n\n\nFrom a (supervised) machine learning perspective, fitting a line to such data means “learning” the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) from the data. How do we learn \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\)? Figure 2 shows the same data as in Figure 1 but where we have added a best fit line as well as a single residual value.\n\n\nCode\nfitted_linear_model &lt;- lm(y ~ x, data=xy_data) %&gt;%\n  augment()\n\na_point &lt;- fitted_linear_model[1,1:3] %&gt;% as.numeric()\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) + \n  geom_point(data=NULL,aes(x=a_point[2],y=a_point[3]),color=\"purple\",size=3) + \n  geom_segment(aes(x = a_point[2], y = a_point[1], xend = a_point[2], yend = a_point[3]), \n               data = NULL,\n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 2: The same data as shown in Figure 1 but with a best fit line as well as a single residual also shown.\n\n\n\n\nFigure 3 shows the same data as in Figure 1 but where we have added a best fit line as well as all the residual values. One way to learn the values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) is to minimize the squared error for the residuals.\n\n\nCode\nfitted_linear_model %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) +\n  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), \n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 3: The same data as shown in Figure 1 but with a best fit line as well as all residuals also shown.\n\n\n\n\nNotice that we can write the squared error for the residuals as a function of two variables \\(L(\\beta_{0},\\beta_{1})\\) defined by\n\\[\nL(\\beta_{0},\\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2\n\\]\nThen, in order to minimize this function we need to find the critical values for the function \\(L\\) by computing partial derivatives and solving\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_{0}} &= 0 \\\\\n\\frac{\\partial L}{\\partial \\beta_{1}} &= 0\n\\end{align*}\n\\]\nHowever, there is an alternative approach that uses tools from linear algebra such as matrices and we will examine this approach for a few reasons:\n\nIt motivates the use of linear algebra and matrices in machine learning.\nIt helps provide a geometric perspective to machine learning.\nIt generalizes well to the situation when we have more than one predictor variable.\n\nThe next section treats the linear algebra tools we will use for linear regression and the section after that applies linear algebra to do linear regression.\nNote: Linear regression can and often is used even in situations where we do not know a priori that \\(f\\) in the relation \\(y = f(x) + \\epsilon\\) is linear.\nQuestion: What do you think some of the pros and cons of using linear regression for supervised learning even if the function \\(f\\) in the relationship \\(y = f(x) + \\epsilon\\) might not be linear?"
  },
  {
    "objectID": "lesson09/index.html#simple-linear-regression",
    "href": "lesson09/index.html#simple-linear-regression",
    "title": "Lesson 9",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nReturn to the data shown in Figure 1 and Figure 2. Fitting a line to this data means finding values for the intercept \\(\\hat{\\beta}_{0}\\) and slope \\(\\hat{\\beta}_{1}\\) so that for each data point \\((x_{i},y_{i})\\) we have that\n\\[\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i} = \\hat{\\beta}_{0} \\cdot 1 + \\hat{\\beta}_{1} x_{i}\n\\]\nis as close as possible to \\(y_{i}\\). We can rewrite the last expression as a matrix vector product:\n\\[\nX {\\bf \\beta} =  \\left[\\begin{array}{cc} 1 & x_{1} \\\\ 1 & x_{2} \\\\ \\vdots & \\vdots \\\\ 1 & x_{n} \\end{array}\\right]\\left[\\begin{array}{c} \\beta_{0} \\\\ \\beta_{1} \\end{array}\\right]\n\\]\nIn order to account for the intercept coefficient, we have to add the column of ones. We call the matrix \\(X\\) so formed the data matrix and the vector \\({\\bf \\beta}\\) the parameter vector. Thus, we can write the linear regression problem as a linear least squares problem to minimize the residual \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\). Minimizing this is the 2-norm is the same as minimizing the squared error. Let’s see this worked out in a computational example.\n\nFitting Linear Models with R\nThere are many functions in base R and other packages that can be used to fit not only linear models but a variety of many different types of models. In base R, we have a function lm that can be used to fit a linear regression model. Let’s examine the documentation for lm. We see that the first argument for lm is a formula. Many modeling functions in base R and other packages accept utilize a formula to represent the model specification. The easiest way to understand this is to see some examples.\nLet’s compare what R does when we use the function lm to fit a linear model with our earlier approach of using \\(QR\\) factorization.\n\nlm_fit &lt;- lm(y ~ x,data=xy_data)\n\ncoefficients(lm_fit)\n\n(Intercept)           x \n   1.774549    0.742658 \n\n\nUnder the hood, the lm function in R is using the \\(QR\\) factorization to compute the slope and intercept for the line in figures like Figure 2 and Figure 3.\nWe can also make predictions on new data with lm models:\n\npredict(lm_fit,newdata=tibble(x=67))\n\n       1 \n51.53264"
  },
  {
    "objectID": "lesson09/index.html#multiple-linear-regression",
    "href": "lesson09/index.html#multiple-linear-regression",
    "title": "Lesson 9",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSuppose that we have data of the form \\((y_{i},{\\bf x}_{i}) = (y_{i},x_{i1},x_{i2},\\ldots , x_{ip})\\) so that there are \\(p\\) predictor variables. The multiple linear regression model takes the form\n\\[\ny = \\beta_{0} + \\beta_{1}{\\bf x}_{1} + \\beta_{2}{\\bf x}_{2} + \\cdots + \\beta_{p}{\\bf x}_{p} + \\epsilon\n\\]\nTaking into account the column of ones we can form a \\(n \\times (p + 1)\\) sized data matrix and again use \\(QR\\) factorization to solve the corresponding linear least squares problem for the residual \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\). Now, our coefficient vector \\({\\bf \\beta}\\) will have length \\(p+1\\).\nMultiple linear regression is a significant generalization of simple linear regression because it not only allows us to account for multiple predictor variables, but also allows us to account for certain types of nonlinearity and also predictor variables that are categorical. This is because:\n\nThe “linear” part of linear regression refers to linearity with respect to the coefficients \\({\\bf \\beta}\\).\nWe can use dummy variables to represent categorical predictor variables.\n\nThe point is, as long as our data can be represented by a data matrix \\(X\\), then we can try to use \\(QR\\) factorization to solve the linear least squares problem to minimize the residuals \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\).\nWe can use the lm function to easily fit multiple linear regression models. Let’s work through some examples together."
  },
  {
    "objectID": "lesson09/index.html#linear-regression-in-machine-learning",
    "href": "lesson09/index.html#linear-regression-in-machine-learning",
    "title": "Lesson 9",
    "section": "Linear Regression in Machine Learning",
    "text": "Linear Regression in Machine Learning\nThrough our study of linear regression, we have derived and implemented our first supervised machine learning algorithm. We have also seen in our worked examples how to use tidymodels to apply linear regression as a machine learning algorithm. In particular, we have seen how to:\n\nAccount for certain types of nonlinearity.\nAccount for categorical predictors.\nSeparate data into a training set and a test set.\nSet up and fit a model.\nUse a model to make predictions.\nAssess model accuracy by computing the root mean square error.\n\nHowever, there are several additional considerations we still need to address. For example,\n\nAssessing model uncertainty.\nChoosing which predictors to include or not in a model.\nDeciding between different classes of models.\n\nWe will take up these issues soon but before doing so, we will first look at a learning algorithm for a classification problem."
  },
  {
    "objectID": "lesson09/index.html#preparation-for-the-next-lesson",
    "href": "lesson09/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 9",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:"
  },
  {
    "objectID": "lesson09/index.html#references",
    "href": "lesson09/index.html#references",
    "title": "Lesson 9",
    "section": "References",
    "text": "References\n\n\nEfron, Bradley, and Trevor Hastie. 2022. “Computer Age Statistical Inference.”\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Vol. 26. Springer.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Sonoma 14.1.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-11-29\n pandoc   3.1.9 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.4   2023-11-17 [1] CRAN (R 4.3.1)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.3.0   2023-11-28 [1] CRAN (R 4.3.1)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.1   2023-11-14 [1] CRAN (R 4.3.1)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson08/index.html",
    "href": "lesson08/index.html",
    "title": "Lesson 8",
    "section": "",
    "text": "After this lesson, students will be able to\n\nunderstand and utilize the tidymodels framework for supervised machine learning."
  },
  {
    "objectID": "lesson08/index.html#learning-objectives",
    "href": "lesson08/index.html#learning-objectives",
    "title": "Lesson 8",
    "section": "",
    "text": "After this lesson, students will be able to\n\nunderstand and utilize the tidymodels framework for supervised machine learning."
  },
  {
    "objectID": "lesson08/index.html#readings-etc.",
    "href": "lesson08/index.html#readings-etc.",
    "title": "Lesson 8",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapters 5, 6, & 7 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nRead Chapter 2 of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nThe following two video lectures are also recommended:\n\n\nMotivating problems for machine (statistical) learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSupervised and unsupervised learning. Watch video on YouTube."
  },
  {
    "objectID": "lesson08/index.html#overview",
    "href": "lesson08/index.html#overview",
    "title": "Lesson 8",
    "section": "Overview",
    "text": "Overview\nMachine learning (ML) as we have defined it involves estimating a (mathematical) function and we divide problems into two general types: supervised and unsupervised learning. In supervised learning, we have a set of data that we use to train a model to predict a target variable. In unsupervised learning, we have a set of data that we use to train a model to find patterns in the data. In this chapter, we will focus on supervised learning. Further, we separate supervised learning into two types: regression and classification. In regression, the target variable is continuous. In classification, the target variable is categorical. We also consider two types of applications for machine learning: prediction and inference. In prediction, we are interested in predicting the target variable. In inference, we are interested in understanding the relationship between the target variable and the predictors."
  },
  {
    "objectID": "lesson08/index.html#introduction-to-machine-statistical-learning",
    "href": "lesson08/index.html#introduction-to-machine-statistical-learning",
    "title": "Lesson 8",
    "section": "Introduction to Machine (Statistical) Learning",
    "text": "Introduction to Machine (Statistical) Learning\nMachine learning or statistical learning generally refers to methods or tools that seek to derive insight or understanding from data by using models. Here by model we mean a mathematical or computational representation of some part of the real world. In machine learning, we fit or learn a model or class of models to data. The goal of fitting models is usually one of the following:\n\nPrediction - using what is known to make informed (hopefully accurate) claims about what we want to know.\nInference - using a sample to make informed (hopefully accurate) claims about a larger population.\n\nFor an example of prediction, suppose that we are advertising experts working with a customer that sales video games. Our customer cannot directly control their sales but they can directly control their marketing by deciding how much to invest in advertising. Say for example that our customer has three ways to advertise: via YouTube, via podcasts, or via Spotify. We can use our past knowledge about how much our customer has spent i.e., their advertising budget and the corresponding sales to make predictions using a model about how sales will be in the future depending on how the company changes its advertising in each of the three media.\nFor an example of inference, suppose we have data on how a sample of patients respond to a particular drug. We can use this to make claims about how a larger population of patients will respond to this same drug.\nThere are two prominent broad classes of machine learning models:\n\nSupervised - In supervised learning, data comes in pairs \\((y_{i},{\\bf x}_{i})\\) where we view \\({\\bf x}_{i}\\) (which may be a vector) as a predictor and \\(y_{i}\\) as a response. Often, We the predictors are something we can influence directly like the advertising budget from our earlier example while the response is something we don’t have direct control over like the sales from our example. Thus, there is an assumed functional relationship between predictors and the response of the form\n\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] where we think of \\(f({\\bf x})\\) as the mean value for \\(y\\) viewed as a random variable and \\(\\epsilon\\) as containing the variance of \\(y\\) so that \\(E[\\epsilon] = 0\\).\nWe note that \\(y\\) may be numerical in which case we have a regression problem or it may be categorical in which case we have a classification problem.\n\nUnsupervised - In unsupervised learning, there is no response variable. Some common unsupervised problems include clustering and density estimation. Both of these essentially seek to discover a pattern in the data.\n\nFigure 3 illustrates the distinction between supervised and unsupervised learning models.\n\n\n\nFigure 3: Illustration credit: https://vas3k.com/blog/machine_learning/\n\n\nIn this course, we will focus on supervised learning and save discussions on unsupervised learning for future courses.\n\nFitting Supervised Models\nFitting an unsupervised learning model typically amounts to estimating the function \\(f\\) in the assumed relationship\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] between the predictor and response variables. When we estimate \\(f\\) we denote the estimate by \\(\\hat{f}\\). Then, we can use \\(\\hat{f}\\) to predict the response for each predictor value \\({\\bf x}\\) by computing\n\\[\n\\hat{y} = \\hat{f}({\\bf x})\n\\]\nHow do we estimate a function \\(f\\)? In machine learning, we use the data together with some algorithm to construct \\(\\hat{f}\\).\n\nRegression\nLet’s consider an illustrative example where \\({\\bf x}\\) represents the years of education of some individuals and \\(y\\) is the income they earn in their profession. Thus, both variables are numerical so we are dealing with a regression problem. We are assuming that there is a true but unknown functional relationship between the years of education and the income they earn.\nThe left panel of Figure 4 shows a scatter plot of our education versus income data while the right panel shows the data again but with a curve corresponding to the graph of a function \\(\\hat{f}\\) that passes through the data.\n\n\n\nFigure 4: Illustration of supervised learning through a regression problem. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nHow did we come up with the function \\(\\hat{f}\\)? Basically, minimized the error between our predicted and observed response. That is, for each response value \\({\\bf x}\\) we minimized how far \\(y=f({\\bf x})\\) can be from \\(\\hat{y}=\\hat{f}({\\bf x})\\). There are three important points that need to be addressed before we can implement regression in a practical situation.\n\nThe set of all functions is too large to work with in practice so we must make some choices that allow us to narrow down the class of functions from which \\(\\hat{f}\\) will be taken. For example, we could restrict to only linear functions, or only quadratic functions, or only polynomial functions. These classes of functions are easy to describe because these types of functions are uniquely described by a finite number of parameters. However, sometimes data can not be modeled well by, e.g., polynomials so more sophisticated non-parametric ways of describing classes of functions have been developed that allow for more flexible modeling.\nWe must decide on how we will define and measure error. For regression problems, a typical way to measure error is the squared-error. Referring back to the right side of Figure 4, we define the \\(i\\)-th residual \\(r_{i}\\) to be the vertical (signed) distance between the observed response value \\(y_{i}\\) and the corresponding predicted value \\(\\hat{y}_{i} = \\hat{f}({\\bf x}_{i})\\). That is,\n\n\\[\nr_{i} = y_{i} - \\hat{y}_{i}\n\\] Then the squared error (SE) is\n\\[\n\\text{SE} = \\sum_{i=1}^{n}r_{i}^{2} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2} = \\sum_{i=1}^{n}(y_{i} - \\hat{f}({\\bf{x}_{i}}))^{2}\n\\]\nIn this case, we take \\(\\hat{f}\\) to be the function from some specified class of functions such that it minimizes the corresponding SE.\nImportant Point: A main component of many if not most supervised machine learning problems is solving some kind of optimization problem. Usually when one speaks of a machine learning algorithm (or learning algorithm), what they are actually referring to is some algorithm that is used to solve an appropriate optimization problem.\n\nWe have to distinguish between reducible error and irreducible error. No machine learning model will ever be perfect. Suppose that we have an estimate \\(\\hat{f}\\) that yields a prediction \\(\\hat{y} = \\hat{f}({\\bf x})\\). Since in reality the response is a random variable\n\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] we have\n\\[\n\\begin{align*}\n\\text{E}[(y - \\hat{y})^{2}] &= \\text{E}[(f({\\bf x}) + \\epsilon - \\hat{f}({\\bf x}))^2] \\\\\n&= \\text{E}[((f({\\bf x}) - \\hat{f}({\\bf x})) + \\epsilon)^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2 - 2\\epsilon (f({\\bf x}) - \\hat{f}({\\bf x})) + \\epsilon^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2] - 2(f({\\bf x}) - \\hat{f}({\\bf x}))\\text{E}[\\epsilon] + \\text{E}[(\\epsilon - 0)^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2] + \\text{Var}[\\epsilon]\n\\end{align*}\n\\]\nBy choosing a good enough family of functions or a good enough learning algorithm we can reduce \\(\\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2]\\) as much as we want. This corresponds to the reducible error. However, we have no control over \\(\\text{Var}[\\epsilon]\\) and this corresponds to the irreducible error.\n\n\nClassification\nFor classification problems in supervised machine learning, the response variable is categorical. Figure 5 illustrates this, showing a scatter plot of data where coloring is used to distinguish the data points as belonging to one of two different classes.\n\n\n\nFigure 5: Illustration of a classification problem in which the response variable is a binary categorical variable. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nFor classification problems, our goal is still to estimate a functional relationship of the form \\(y = f({\\bf x}) + \\epsilon\\). However, we can no longer measure error using the squared error because the response values are not numerical. A common method for measuring error in classification problems is classification error (CE) defined by\n\\[\n\\text{CE} = \\sum_{i=1}^{n}I(y_{i} \\neq \\hat{y}_{i})\n\\]\nwhere \\(I\\) is the indicator function that is equal to 1 whenever \\(y_{i} \\neq \\hat{y}_{i}\\) and equal to 0 whenever \\(y_{i} = \\hat{y}_{i}\\). Essentially, CE counts the number of misclassifications.\nSimilar to regression, fitting a classification model involves finding a function \\(\\hat{f}\\) from some specified class of functions such that the corresponding CE is minimized.\nNote that it is possible to convert a regression problem to a classification problem by binning or discretizing the response variable in some way.\n\n\n\nComplexity Vs. Interpretability\nAnother issue that needs to be taken into account when fitting models is the tradeoff between how easy it is to interpret a model versus the maximum degree of accuracy for the model. Figure 6 illustrates this through a representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity.\n\n\n\nFigure 6: A representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity. Figure from (Tibshirani, James, and Trevor 2017).\n\n\n\n\nTraining Error Vs. Test Error\nWhen we fit a model to data, say by minimizing the error the resulting estimate function we get depends on the data used to fit the model. We refer to this data as the training data and the corresponding error as the training error. By choosing a sufficiently flexible set of functions from which to fit to the data, we can make the training error as small as we want. This might seem like a great thing, but there is a major problem with it.\nSuppose we want to use a model to make predictions about future unseen values of our predictor \\({\\bf x}\\). If a model is fit too well to the training data, then in general it tends not to be very good at making accurate predictions for future values. One says that models that are overfit to the training data are poor at generalization.\nHow do we build models that generalize well and avoid overfitting? A common approach is to separate data into a training set that is used to fit a model and a test set which is used to assess how well the models generalizes to unseen data via the test error. Figure 7 shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data.\n\n\n\nFigure 7: The left panel shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nWhile the training/test set approach to fitting accurate models while avoiding overfitting is very good in principle, there are some practical limitations. For example,\n\nHow do we know the training data is sufficiently representative?\nWhat if we don’t have a sufficiently large data set to split into a training and a test set?\nHow do we know what the minimum possible test error is?\n\nWe will spend a lot of time later talking more about these issues and ways to deal with them.\n\n\nThe Bias-Variance Trade-Off\nReferring back to Figure 7, notice the distinct U-shape in the curve for the test error. This is more than just a curiosity, it is the result of another type of trade-off known as the bias-variance trade-off.\nLet’s try to get a sense for this starting with some intuition. Suppose we having a regression problem with a single predictor. If we restrict to the class of linear functions, that is functions with graph that is a straight line in the plane, then any such function is uniquely specified by two parameters, the slope and intercept. Intuitively, such as model is highly biased because it’s going to make very rigid predictions. However, linear functions have low variance in the sense than models fit to similar data will have very similar slope and intercept values. On the other hand, a cubic polynomial being described uniquely by four parameters is much less biased than a linear function but will have higher variance.\nIt is outside the scope of this course, but it can be shown that the expected squared error for an observed value \\({\\bf x}_{0}\\) can be decomposed as follows:\n\\[\n\\text{E}[(y_{0} - \\hat{f}({\\bf x}))^2] = \\text{Var}(\\hat{f}({\\bf x}_{0})) + [\\text{Bias}(\\hat{f}({\\bf x}_{0}))]^2 + \\text{Var}(\\epsilon)\n\\]\nWe refer to the first two terms as\n\nthe variance of \\(\\hat{f}({\\bf x}_{0})\\)\nthe squared bias of \\(\\hat{f}({\\bf x}_{0})\\)\n\nFigure 8 illustrates this formula.\n\n\n\nFigure 8: Squared bias (blue curve), variance (orange curve), Var(ε) (dashed line), and test MSE (red curve). The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nImportant: What you should keep in mind as we proceed through the course is the following:\n\nSimple models tend to have high bias but much lower variance.\nComplex models tend to have lower bias but much higher variance.\n\nAnytime you choose a particular modeling approach for a specific application or data set, you should take into account the bias-variance trade-off."
  },
  {
    "objectID": "lesson08/index.html#nearest-neighbors",
    "href": "lesson08/index.html#nearest-neighbors",
    "title": "Lesson 8",
    "section": "Nearest Neighbors",
    "text": "Nearest Neighbors\nThe nearest neighbors approach to supervised machine learning is a very simple and intuitive approach to modeling. For example, suppose we have a data set consisting of a single predictor \\({\\bf x}\\) and a response \\(y\\). The nearest neighbors approach to regression is to predict the response for a new value of \\({\\bf x}\\) by averaging the responses for the \\(k\\) nearest values of \\({\\bf x}\\) in the training data. Nearest neighbors can also be used for classification problems. In this case, we predict the class of a new value of \\({\\bf x}\\) by taking a majority vote of the classes of the \\(k\\) nearest values of \\({\\bf x}\\) in the training data.\nLet’s proceed to an online interactive demo for nearest neighbors applied to a classification problem. View the demo."
  },
  {
    "objectID": "lesson08/index.html#some-data-sources",
    "href": "lesson08/index.html#some-data-sources",
    "title": "Lesson 8",
    "section": "Some Data Sources",
    "text": "Some Data Sources\nOne thing students often struggle with is finding and picking a good data set for their projects. Appendix B of the online textbook contains a very helpful list of data sources (Alexander, n.d.). View appendix B. Two other very interesting and useful sources of data are the Tidy Tuesday data repositories and Kaggle. There are also many R packages that either include data or that can be used to download data. The ROpenSci project is a good resource for finding R packages that can be used to obtain data, view the project."
  },
  {
    "objectID": "lesson08/index.html#preparation-for-the-next-lesson",
    "href": "lesson08/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 8",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead Chapter 8 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nYou may also want to read section 3.1 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nWatch the corresponding video lecture on regression. View on YouTube."
  },
  {
    "objectID": "lesson08/index.html#references",
    "href": "lesson08/index.html#references",
    "title": "Lesson 8",
    "section": "References",
    "text": "References\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. \" O’Reilly Media, Inc.\".\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS Sonoma 14.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-11-05\n pandoc   3.1.9 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0      2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3      2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4      2023-10-12 [1] CRAN (R 4.3.1)\n ggthemes     * 4.2.4      2021-01-20 [1] CRAN (R 4.3.0)\n glmnet       * 4.1-8      2023-08-22 [1] CRAN (R 4.3.0)\n infer        * 1.0.5      2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2      2022-11-20 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4      2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3      2023-09-27 [1] CRAN (R 4.3.1)\n Matrix       * 1.6-1.1    2023-09-18 [1] CRAN (R 4.3.2)\n modeldata    * 1.2.0      2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1      2023-08-17 [1] CRAN (R 4.3.0)\n parttree     * 0.0.1.9004 2023-10-02 [1] Github (grantmcdermott/parttree@d2b60ac)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n ranger       * 0.15.1     2023-04-03 [1] CRAN (R 4.3.0)\n readr        * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8      2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0      2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1      2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2      2023-08-23 [1] CRAN (R 4.3.0)\n vip          * 0.4.1      2023-08-21 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3      2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1      2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0      2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "stat_infer/index.html",
    "href": "stat_infer/index.html",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe real-world examples of questions that can be answered with statistical inference.\nDefine common population parameters (e.g., mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.\nDefine the following statistical sampling terms: population, sample, population parameter, point estimate, and sampling distribution.\nDefine bootstrapping.\nUse R to create a bootstrap distribution to approximate a sampling distribution."
  },
  {
    "objectID": "stat_infer/index.html#learning-objectives",
    "href": "stat_infer/index.html#learning-objectives",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe real-world examples of questions that can be answered with statistical inference.\nDefine common population parameters (e.g., mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.\nDefine the following statistical sampling terms: population, sample, population parameter, point estimate, and sampling distribution.\nDefine bootstrapping.\nUse R to create a bootstrap distribution to approximate a sampling distribution."
  },
  {
    "objectID": "stat_infer/index.html#readings-etc.",
    "href": "stat_infer/index.html#readings-etc.",
    "title": "Lesson",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapter 10 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nYou might also want to look at chapters 8 & 9 from Statistical Inference via Data Science."
  },
  {
    "objectID": "stat_infer/index.html#overview",
    "href": "stat_infer/index.html#overview",
    "title": "Lesson",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "stat_infer/index.html#preparation-for-the-next-lesson",
    "href": "stat_infer/index.html#preparation-for-the-next-lesson",
    "title": "Lesson",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead Chapters 5, 6, & 7 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\nRead Chapter 2 of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nThe following two video lectures are also recommended:\nMotivating problems for machine (statistical) learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSupervised and unsupervised learning. Watch video on YouTube."
  },
  {
    "objectID": "stat_infer/index.html#references",
    "href": "stat_infer/index.html#references",
    "title": "Lesson",
    "section": "References",
    "text": "References\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-19\n pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine linear regression and appreciate it from the perspective of machine learning.\nUse R to fit linear regression modes."
  },
  {
    "objectID": "regression/index.html#learning-objectives",
    "href": "regression/index.html#learning-objectives",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine linear regression and appreciate it from the perspective of machine learning.\nUse R to fit linear regression modes."
  },
  {
    "objectID": "regression/index.html#readings-etc.",
    "href": "regression/index.html#readings-etc.",
    "title": "Lesson",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson:\n\nRead Chapter 8 from from Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). View book online.\n\nYou may also be interested in:\n\nRead chapter 3 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 2 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on regression. View on YouTube."
  },
  {
    "objectID": "regression/index.html#motivation-for-linear-regression",
    "href": "regression/index.html#motivation-for-linear-regression",
    "title": "Lesson",
    "section": "Motivation for Linear Regression",
    "text": "Motivation for Linear Regression\nRecall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] and then we seek to find a function \\(\\hat{f}\\) from a predetermined class of functions that does a good job in approximating \\(f\\). Let’s study this problem in more detail but in a very simplest setting. Specifically, we will assume that \\({\\bf x}\\) and \\(y\\) are both single numerical variables and that \\(f\\) is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers \\(\\beta_{0}\\) and \\(\\beta_{1}\\) such that\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\] for all values of \\(x\\) and \\(y\\). Recall that we are assuming that \\(\\text{E}[\\epsilon] = 0\\) so \\(\\epsilon\\) is a random variable with expected value (or mean) equal to zero.\nIf we restrict ourselves to the class of single-variable linear functions, then finding an approximation to \\(f(x) = \\beta_{0} + \\beta_{1} x\\) is equivalent to finding values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) so that\n\\[\n\\hat{f}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x \\approx f(x) = \\beta_{0} + \\beta_{1} x\n\\] Thus, this would be a parametric model since any candidate approximating function is uniquely specified by specifying the values for the parameters \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\).\nFigure 1 shows the plot of data that has been generated by a relationship of the form \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\). You should examine the code used to create or simulate the data in this example and see how it relates to the expression \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\).\n\n\nCode\nset.seed(1287)\nN &lt;- 25\nx &lt;- rnorm(N,mean=72,sd=12)\ny &lt;- 1.2 + 0.75 * x + rnorm(N,sd=2)\nxy_data &lt;- tibble(x=x,y=y)\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point()\n\n\n\n\n\nFigure 1: A data set with two numerical variables \\(x\\) and \\(y\\) generated by an underlying linear function so that \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\).\n\n\n\n\nFrom a (supervised) machine learning perspective, fitting a line to such data means “learning” the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) from the data. How do we learn \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\)? Figure 2 shows the same data as in Figure 1 but where we have added a best fit line as well as a single residual value.\n\n\nCode\nfitted_linear_model &lt;- lm(y ~ x, data=xy_data) %&gt;%\n  augment()\n\na_point &lt;- fitted_linear_model[1,1:3] %&gt;% as.numeric()\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) + \n  geom_point(data=NULL,aes(x=a_point[2],y=a_point[3]),color=\"purple\",size=3) + \n  geom_segment(aes(x = a_point[2], y = a_point[1], xend = a_point[2], yend = a_point[3]), \n               data = NULL,\n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 2: The same data as shown in Figure 1 but with a best fit line as well as a single residual also shown.\n\n\n\n\nFigure 3 shows the same data as in Figure 1 but where we have added a best fit line as well as all the residual values. One way to learn the values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) is to minimize the squared error for the residuals.\n\n\nCode\nfitted_linear_model %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) +\n  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), \n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 3: The same data as shown in Figure 1 but with a best fit line as well as all residuals also shown.\n\n\n\n\nNotice that we can write the squared error for the residuals as a function of two variables \\(L(\\beta_{0},\\beta_{1})\\) defined by\n\\[\nL(\\beta_{0},\\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2\n\\]\nThen, in order to minimize this function we need to find the critical values for the function \\(L\\) by computing partial derivatives and solving\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_{0}} &= 0 \\\\\n\\frac{\\partial L}{\\partial \\beta_{1}} &= 0\n\\end{align*}\n\\]\nHowever, there is an alternative approach that uses tools from linear algebra such as matrices and we will examine this approach for a few reasons:\n\nIt motivates the use of linear algebra and matrices in machine learning.\nIt helps provide a geometric perspective to machine learning.\nIt generalizes well to the situation when we have more than one predictor variable.\n\nThe next section treats the linear algebra tools we will use for linear regression and the section after that applies linear algebra to do linear regression.\nNote: Linear regression can and often is used even in situations where we do not know a priori that \\(f\\) in the relation \\(y = f(x) + \\epsilon\\) is linear.\nQuestion: What do you think some of the pros and cons of using linear regression for supervised learning even if the function \\(f\\) in the relationship \\(y = f(x) + \\epsilon\\) might not be linear?"
  },
  {
    "objectID": "regression/index.html#simple-linear-regression",
    "href": "regression/index.html#simple-linear-regression",
    "title": "Lesson",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nReturn to the data shown in Figure 1 and Figure 2. Fitting a line to this data means finding values for the intercept \\(\\hat{\\beta}_{0}\\) and slope \\(\\hat{\\beta}_{1}\\) so that for each data point \\((x_{i},y_{i})\\) we have that\n\\[\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i} = \\hat{\\beta}_{0} \\cdot 1 + \\hat{\\beta}_{1} x_{i}\n\\]\nis as close as possible to \\(y_{i}\\). We can rewrite the last expression as a matrix vector product:\n\\[\nX {\\bf \\beta} =  \\left[\\begin{array}{cc} 1 & x_{1} \\\\ 1 & x_{2} \\\\ \\vdots & \\vdots \\\\ 1 & x_{n} \\end{array}\\right]\\left[\\begin{array}{c} \\beta_{0} \\\\ \\beta_{1} \\end{array}\\right]\n\\]\nIn order to account for the intercept coefficient, we have to add the column of ones. We call the matrix \\(X\\) so formed the data matrix and the vector \\({\\bf \\beta}\\) the parameter vector. Thus, we can write the linear regression problem as a linear least squares problem to minimize the residual \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\). Minimizing this is the 2-norm is the same as minimizing the squared error. Let’s see this worked out in a computational example.\n\nFitting Linear Models with R\nThere are many functions in base R and other packages that can be used to fit not only linear models but a variety of many different types of models. In base R, we have a function lm that can be used to fit a linear regression model. Let’s examine the documentation for lm. We see that the first argument for lm is a formula. Many modeling functions in base R and other packages accept utilize a formula to represent the model specification. The easiest way to understand this is to see some examples.\nLet’s compare what R does when we use the function lm to fit a linear model with our earlier approach of using \\(QR\\) factorization.\n\nlm_fit &lt;- lm(y ~ x,data=xy_data)\n\ncoefficients(lm_fit)\n\n(Intercept)           x \n   1.774549    0.742658 \n\n\nUnder the hood, the lm function in R is using the \\(QR\\) factorization to compute the slope and intercept for the line in figures like Figure 2 and Figure 3.\nWe can also make predictions on new data with lm models:\n\npredict(lm_fit,newdata=tibble(x=67))\n\n       1 \n51.53264"
  },
  {
    "objectID": "regression/index.html#multiple-linear-regression",
    "href": "regression/index.html#multiple-linear-regression",
    "title": "Lesson",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSuppose that we have data of the form \\((y_{i},{\\bf x}_{i}) = (y_{i},x_{i1},x_{i2},\\ldots , x_{ip})\\) so that there are \\(p\\) predictor variables. The multiple linear regression model takes the form\n\\[\ny = \\beta_{0} + \\beta_{1}{\\bf x}_{1} + \\beta_{2}{\\bf x}_{2} + \\cdots + \\beta_{p}{\\bf x}_{p} + \\epsilon\n\\]\nTaking into account the column of ones we can form a \\(n \\times (p + 1)\\) sized data matrix and again use \\(QR\\) factorization to solve the corresponding linear least squares problem for the residual \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\). Now, our coefficient vector \\({\\bf \\beta}\\) will have length \\(p+1\\).\nMultiple linear regression is a significant generalization of simple linear regression because it not only allows us to account for multiple predictor variables, but also allows us to account for certain types of nonlinearity and also predictor variables that are categorical. This is because:\n\nThe “linear” part of linear regression refers to linearity with respect to the coefficients \\({\\bf \\beta}\\).\nWe can use dummy variables to represent categorical predictor variables.\n\nThe point is, as long as our data can be represented by a data matrix \\(X\\), then we can try to use \\(QR\\) factorization to solve the linear least squares problem to minimize the residuals \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\).\nWe can use the lm function to easily fit multiple linear regression models. Let’s work through some examples together."
  },
  {
    "objectID": "regression/index.html#linear-regression-in-machine-learning",
    "href": "regression/index.html#linear-regression-in-machine-learning",
    "title": "Lesson",
    "section": "Linear Regression in Machine Learning",
    "text": "Linear Regression in Machine Learning\nThrough our study of linear regression, we have derived and implemented our first supervised machine learning algorithm. We have also seen in our worked examples how to use tidymodels to apply linear regression as a machine learning algorithm. In particular, we have seen how to:\n\nAccount for certain types of nonlinearity.\nAccount for categorical predictors.\nSeparate data into a training set and a test set.\nSet up and fit a model.\nUse a model to make predictions.\nAssess model accuracy by computing the root mean square error.\n\nHowever, there are several additional considerations we still need to address. For example,\n\nAssessing model uncertainty.\nChoosing which predictors to include or not in a model.\nDeciding between different classes of models.\n\nWe will take up these issues soon but before doing so, we will first look at a learning algorithm for a classification problem."
  },
  {
    "objectID": "regression/index.html#preparation-for-the-next-lesson",
    "href": "regression/index.html#preparation-for-the-next-lesson",
    "title": "Lesson",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:"
  },
  {
    "objectID": "regression/index.html#references",
    "href": "regression/index.html#references",
    "title": "Lesson",
    "section": "References",
    "text": "References\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-19\n pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson08/index.html#tidymodels-machine-learning-workflow",
    "href": "lesson08/index.html#tidymodels-machine-learning-workflow",
    "title": "Lesson 8",
    "section": "  tidymodels Machine Learning Workflow",
    "text": "tidymodels Machine Learning Workflow\nThe tidymodels framework provides a consistent set of steps for training and evaluating machine learning models. The following is a list of the steps in the tidymodels workflow:\n\nData Preparation:\n\nLoad and prepare your dataset following tidy data principles.\n\nData Splitting:\n\nUse the rsample package to create data splits for training and testing.\n\nPreprocessing and Feature Engineering:\n\nDefine a data preprocessing plan using the recipes package.\nCreate a “recipe” for data cleaning, transformation, and feature engineering.\n\nModel Specification:\n\nSpecify machine learning models with the parsnip package.\nChoose from a variety of models for regression, classification, etc.\n\nHyperparameter Tuning:\n\nUtilize the tune package for hyperparameter tuning.\nDefine a grid of hyperparameters and use resampling methods for evaluation.\n\nModel Training:\n\nTrain models with the specified data splits, preprocessing plan, and hyperparameters.\nUse the fit function to train on the training data.\n\nModel Evaluation:\n\nAssess model performance using the yardstick package.\nCalculate relevant evaluation metrics (e.g., accuracy, RMSE).\n\nModel Selection:\n\nCompare model performance and hyperparameter settings.\nSelect the best-performing model or create ensembles (e.g., stacking, bagging).\n\nModel Interpretation and Visualization:\n\nUse the broom package to extract and tidy model results.\nVisualize model outputs and interpret results.\n\nDeployment and Prediction:\n\nDeploy the final model for predictions on new data.\nUse the trained model in production environments.\n\nDocumentation and Reproducibility:\n\nDocument the entire workflow, including preprocessing, model specifications, tuning, and results.\nEnsure reproducibility and transparency.\n\nSharing and Collaboration:\n\nShare code and findings with collaborators for reproducibility and collaboration.\n\nOngoing Monitoring and Maintenance:\n\nIn production, monitor model performance and update models as needed.\n\n\nThe best references for tidymodels are the tidymodels.org website and the Tidy Modeling with R book. The Tidy Modeling with R book is available for free online and is a great resource for learning the tidymodels framework (Kuhn and Silge 2022). The blog posts by Julia Silge are also a great resource for learning tidymodels. View Silge’s blog."
  },
  {
    "objectID": "lesson08/index.html#some-examples",
    "href": "lesson08/index.html#some-examples",
    "title": "Lesson 8",
    "section": "  Some Examples",
    "text": "Some Examples\nWe’ve already seen examples of utilizing the tidymodels framework for nearest neighbors, let’s examine some further examples.\n\nExample 1: tidymodels Workflow for Linear Regression for Predictive Modeling\nThe following is an example of the tidymodels workflow for linear regression for predictive modeling. The example uses the mtcars dataset and the parsnip and recipes packages. The example is adapted from the Tidy Modeling with R book (Kuhn and Silge 2022).\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the mtcars dataset\ndata(mtcars)\n\n# create a training and testing split\nmtcars_split &lt;- initial_split(mtcars, prop = 0.75)\n\n# create a training and testing dataset\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\n\n# create a recipe for preprocessing\nmtcars_recipe &lt;- recipe(mpg ~ ., data = mtcars_train) %&gt;%\n  step_center(all_predictors()) %&gt;%\n  step_scale(all_predictors())\n\n# create a linear regression model\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# fit the model\nlm_fit &lt;- workflow() %&gt;%\n  add_recipe(mtcars_recipe) %&gt;%\n  add_model(lm_spec) %&gt;%\n  fit(data = mtcars_train)\n\n# make predictions\nlm_pred &lt;- predict(lm_fit, mtcars_test) %&gt;%\n  bind_cols(mtcars_test)\n\n# evaluate the model\nlm_eval &lt;- lm_pred %&gt;%\n  metrics(truth = mpg, estimate = .pred) %&gt;%\n  bind_rows(\n    lm_pred %&gt;%\n      rsq(truth = mpg, estimate = .pred) %&gt;%\n      mutate(metric = \"rsq\")\n  )\n\n# tidy the model results\nlm_tidy &lt;- tidy(lm_fit)\n\n# visualize the model results\nlm_tidy %&gt;%\n  ggplot(aes(x = term, y = estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error)) +\n  coord_flip() +\n  labs(\n    title = \"Linear Regression Model Results\",\n    subtitle = \"Model: mpg ~ .\",\n    x = \"Term\",\n    y = \"Estimate\"\n  )\n\n\n\n\n\nFigure 1: Tidy Models Workflow for Regression\n\n\n\n\n\n\n\n\n\nTable 1: Linear Regression Model Results\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n21.0708333\n0.6035556\n34.9111701\n0.0000000\n\n\ncyl\n-1.8085954\n2.7450923\n-0.6588469\n0.5214933\n\n\ndisp\n2.7886147\n3.3586784\n0.8302714\n0.4213717\n\n\nhp\n-1.4572830\n2.4512678\n-0.5945018\n0.5623810\n\n\ndrat\n0.0941833\n1.1813897\n0.0797225\n0.9376722\n\n\nwt\n-4.4737683\n2.3983919\n-1.8653200\n0.0848621\n\n\nqsec\n1.6098052\n1.7188679\n0.9365497\n0.3660636\n\n\nvs\n0.4021956\n1.3188879\n0.3049506\n0.7652312\n\n\nam\n1.2501668\n1.4919647\n0.8379332\n0.4172084\n\n\ngear\n-0.0667576\n1.5665234\n-0.0426151\n0.9666559\n\n\ncarb\n0.5220534\n1.8506869\n0.2820863\n0.7823182\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Linear Regression Model Evaluation\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n2.4214858\n\n\nrsq\nstandard\n0.7487043\n\n\nmae\nstandard\n2.0481386\n\n\nrsq\nstandard\n0.7487043\n\n\n\n\n\n\n\n\n\n\nExample 2: tidymodels Workflow for Linear Regression for Inferential Modeling\nThe following is an example of the tidymodels workflow for linear regression for inferential modeling. The example uses the mtcars dataset and the parsnip and recipes packages.\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the mtcars dataset\ndata(mtcars)\n\n# create bootstrap samples\nmtcars_boot &lt;- reg_intervals(mpg ~ disp + cyl + wt + am, \n                             data = mtcars,\n                             model_fn = \"lm\",\n                             type = \"percentile\",\n                             keep_reps = TRUE)\n\n\nmtcars_boot %&gt;% \n  select(term, .replicates) %&gt;% \n  unnest(cols = .replicates) %&gt;% \n  ggplot(aes(x = estimate)) + \n  geom_histogram(bins = 30,color=\"white\",fill=\"lightblue\") + \n  facet_wrap(~ term, scales = \"free_x\") + \n  geom_vline(data = mtcars_boot, aes(xintercept = .lower), col = \"purple\") + \n  geom_vline(data = mtcars_boot, aes(xintercept = .upper), col = \"purple\") + \n  geom_vline(xintercept = 0, col = \"black\",linetype=\"dashed\")\n\n\n\n\n\nFigure 2: Tidy Models Workflow for Regression\n\n\n\n\n\n\nExample 3: tidymodels Workflow for Tuning Random Forest for Regression\nThe following is an example of the tidymodels workflow for tuning random forest for regression. The example uses the mtcars dataset and the parsnip and recipes packages.\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the mtcars dataset\ndata(mtcars)\n\n# create a training and testing split\nmtcars_split &lt;- initial_split(mtcars, prop = 0.75)\n\n# create a training and testing dataset\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\n\n# create a cross-validation set\nmtcars_cv &lt;- vfold_cv(mtcars_train, v = 5)\n\n# create a recipe for preprocessing\nmtcars_recipe &lt;- recipe(mpg ~ ., data = mtcars_train)\n\n# create a random forest model\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = 1000,\n  min_n = tune()\n) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\")\n\n# create a workflow\ntune_wf &lt;- workflow() %&gt;%\n  add_recipe(mtcars_recipe) %&gt;%\n  add_model(rf_spec)\n\n# create tuning grid\nrf_grid &lt;- grid_regular(\n  mtry(range = c(3, 11)),\n  min_n(),\n  levels = 15\n)\n\n# tune the model\ntune_res &lt;- tune_grid(\n  tune_wf,\n  resamples = mtcars_cv,\n  grid = rf_grid\n)\n\n# visualize the tuning results\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  mutate(min_n = factor(min_n)) %&gt;%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"RMSE\")\n\n\n\n\n\nFigure 3: Tidy Models Workflow for Random Forest\n\n\n\n\nNow we will select a final model based on the tuning results.\n\n\nCode\nbest_rmse &lt;- select_best(tune_res, \"rmse\")\n\nfinal_rf &lt;- finalize_model(\n  rf_spec,\n  best_rmse\n)\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(mtcars_recipe) %&gt;%\n  add_model(final_rf)\n\nfinal_res &lt;- final_wf %&gt;%\n  last_fit(mtcars_split)\n\nfinal_res %&gt;%\n  collect_metrics() %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n1.9587187\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.7234572\nPreprocessor1_Model1\n\n\n\n\n\n\n\nLet’s see how the model predictions compare with the observed data for the test set observations.\n\n\nCode\nfinal_res %&gt;%\n  collect_predictions() %&gt;%\n  ggplot(aes(mpg, .pred)) +\n  geom_abline(color = \"purple\") +\n  geom_point() +\n  labs(x = \"Observed\", y = \"Predicted\")\n\n\n\n\n\nFigure 4: Results on Test Set for Tuned Random Forest for Regression\n\n\n\n\n\n\nExample 4: tidymodels Workflow for Classification with Multinomial Logistic Regression\nThe following is an example of the tidymodels workflow for multinomial logistic regression. The example uses the penguins dataset and the parsnip and recipes packages.\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the penguins dataset\ndata(penguins)\n\n# remove missing values from the dataset\npenguins &lt;- penguins %&gt;% drop_na()\n\n# create a training and testing split\npenguins_split &lt;- initial_split(penguins, prop = 0.75)\n\n# create a training and testing dataset\npenguins_train &lt;- training(penguins_split)\npenguins_test &lt;- testing(penguins_split)\n\n# create a cross-validation set\npenguins_cv &lt;- vfold_cv(penguins_train, v = 5)\n\n# create a recipe for preprocessing\npenguins_recipe &lt;- recipe(island ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data = penguins_train)\n\n# create a multinomial logistic regression model\nlogit_spec &lt;- multinom_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n# create a workflow\nnber_wf &lt;- workflow(penguins_recipe, logit_spec)\n\n# create grid for tuning\nnber_grid &lt;- grid_regular(penalty(range = c(-5, 0)), \n                          mixture(range = c(0, 1)),\n                          levels = 20)\n\n# tune the model\nnber_rs &lt;-\n  tune_grid(\n    nber_wf,\n    penguins_cv,\n    grid = nber_grid\n  )\n\nautoplot(nber_rs)\n\n\n\n\n\nFigure 5: Tidy Models Workflow for Multinomial Logistic Regression\n\n\n\n\nShow best models.\n\n\nCode\nshow_best(nber_rs) %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\npenalty\nmixture\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0.0000100\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model161\n\n\n0.0000183\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model162\n\n\n0.0000336\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model163\n\n\n0.0000616\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model164\n\n\n0.0001129\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model165\n\n\n\n\n\n\n\nSelect the best model.\n\n\nCode\nfinal_penalty &lt;-\n  nber_rs %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", desc(penalty))\n\n\nFit the final model.\n\n\nCode\nfinal_rs &lt;-\n  nber_wf %&gt;%\n  finalize_workflow(final_penalty) %&gt;%\n  last_fit(penguins_split)\n\n\nThe confusion matrix for the final model prdictions on the test set.\n\n\nCode\ncollect_predictions(final_rs) %&gt;%\n  conf_mat(island, .pred_class) %&gt;%\n  autoplot()\n\n\n\n\n\nFigure 6: Confusion Matrix for Final Model Predictions on Test Set\n\n\n\n\nThe ROC curve.\n\n\nCode\ncollect_predictions(final_rs) %&gt;%\n  roc_curve(truth = island, .pred_Biscoe:.pred_Torgersen) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(size = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  coord_fixed()\n\n\n\n\n\nFigure 7: ROC Curve for Final Model Predictions on Test Set"
  },
  {
    "objectID": "lesson08/index.html#comparing-multiple-modeling-approaches",
    "href": "lesson08/index.html#comparing-multiple-modeling-approaches",
    "title": "Lesson 8",
    "section": "Comparing Multiple Modeling Approaches",
    "text": "Comparing Multiple Modeling Approaches\nIn her blog post Evaluate multiple modeling approaches for spam email, Julia Silge uses workflow sets to evaluate multiple possible models. Let’s work through that post together. View the post.\nSome other posts that give examples of comparing many different models are Train and analyze many models for crop yields and Predict giant pumpkin weights with workflowsets."
  },
  {
    "objectID": "lesson07/index.html#linear-regression",
    "href": "lesson07/index.html#linear-regression",
    "title": "Lesson 7",
    "section": "Linear Regression",
    "text": "Linear Regression\nRecall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] and then we seek to find a function \\(\\hat{f}\\) from a predetermined class of functions that does a good job in approximating \\(f\\). Let’s study this problem in more detail but in a very simplest setting. Specifically, we will assume that \\({\\bf x}\\) and \\(y\\) are both single numerical variables and that \\(f\\) is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers \\(\\beta_{0}\\) and \\(\\beta_{1}\\) such that\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\] for all values of \\(x\\) and \\(y\\). Recall that we are assuming that \\(\\text{E}[\\epsilon] = 0\\) so \\(\\epsilon\\) is a random variable with expected value (or mean) equal to zero.\nIf we restrict ourselves to the class of single-variable linear functions, then finding an approximation to \\(f(x) = \\beta_{0} + \\beta_{1} x\\) is equivalent to finding values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) so that\n\\[\n\\hat{f}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x \\approx f(x) = \\beta_{0} + \\beta_{1} x\n\\] Thus, this would be a parametric model since any candidate approximating function is uniquely specified by specifying the values for the parameters \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\).\nFigure 11 shows the plot of data that has been generated by a relationship of the form \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\). You should examine the code used to create or simulate the data in this example and see how it relates to the expression \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\).\n\n\nCode\nset.seed(1287)\nN &lt;- 25\nx &lt;- rnorm(N,mean=72,sd=12)\ny &lt;- 1.2 + 0.75 * x + rnorm(N,sd=2)\nxy_data &lt;- tibble(x=x,y=y)\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point()\n\n\n\n\n\nFigure 11: A data set with two numerical variables \\(x\\) and \\(y\\) generated by an underlying linear function so that \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\)."
  },
  {
    "objectID": "lesson07/index.html#examples-of-supervised-learning",
    "href": "lesson07/index.html#examples-of-supervised-learning",
    "title": "Lesson 7",
    "section": "Examples of Supervised Learning",
    "text": "Examples of Supervised Learning\n\nNearest Neighbors\nThe nearest neighbors approach to supervised machine learning is a very simple and intuitive approach to modeling. For example, suppose we have a data set consisting of a single predictor \\({\\bf x}\\) and a response \\(y\\). The nearest neighbors approach to regression is to predict the response for a new value of \\({\\bf x}\\) by averaging the responses for the \\(k\\) nearest values of \\({\\bf x}\\) in the training data. Nearest neighbors can also be used for classification problems. In this case, we predict the class of a new value of \\({\\bf x}\\) by taking a majority vote of the classes of the \\(k\\) nearest values of \\({\\bf x}\\) in the training data.\nLet’s proceed to an online interactive demo for nearest neighbors applied to a classification problem. View the demo.\nThis GitHub repository contains code in R that implements nearest neighbors for both regression and classification problems. Let’s go through this together.\n\n\nLinear Regression\nRecall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:\n\\[\ny = f({\\bf x}) + \\epsilon\n\\]\nand then we seek to find a function \\(\\hat{f}\\) from some specified class of functions that does a good job in approximating \\(f\\). Let’s study this problem in more detail but in a very simple setting. Specifically, we will assume that \\({\\bf x}\\) and \\(y\\) are both single numerical variables and that \\(f\\) is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers \\(\\beta_{0}\\) and \\(\\beta_{1}\\) such that\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\] for all values of \\(x\\) and \\(y\\). Recall that we are assuming that \\(\\text{E}[\\epsilon] = 0\\) so \\(\\epsilon\\) is a random variable with expected value (or mean) equal to zero.\nIf we restrict ourselves to the class of single-variable linear functions, then finding an approximation to \\(f(x) = \\beta_{0} + \\beta_{1} x\\) is equivalent to finding values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) so that\n\\[\n\\hat{f}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x \\approx f(x) = \\beta_{0} + \\beta_{1} x\n\\]\nThus, this would be a parametric model since any candidate approximating function is uniquely specified by specifying the values for the parameters \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\). Note that this is in contrast to nearest neighbors which is nonparametric.\nFigure 11 shows the plot of data that has been generated by a relationship of the form \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\). You should examine the code used to create or simulate the data in this example and see how it relates to the expression \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\).\n\n\nCode\nset.seed(1287)\nN &lt;- 25\nx &lt;- rnorm(N,mean=72,sd=12)\ny &lt;- 1.2 + 0.75 * x + rnorm(N,sd=2)\nxy_data &lt;- tibble(x=x,y=y)\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point()\n\n\n\n\n\nFigure 11: A data set with two numerical variables \\(x\\) and \\(y\\) generated by an underlying linear function so that \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\).\n\n\n\n\nFrom a (supervised) machine learning perspective, fitting a line to such data means “learning” the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) from the data. How do we learn \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\)? One way to do this is to minimize an appropriate loss function which is a function that provides a measure of error between the observed response values and the response values predicted by the model.\nFigure 12 shows the same data as in Figure 11 but where we have added a best fit line as well as all the residual values. One way to learn the values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) is to minimize the squared error for the residuals.\n\n\nCode\nfitted_linear_model &lt;- lm(y ~ x, data=xy_data) %&gt;%\n  augment()\n\nfitted_linear_model %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) +\n  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), \n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 12: The same data as shown in Figure 11 but with a best fit line as well as all residuals also shown.\n\n\n\n\nNotice that we can write our loss function, that is, the squared error for the residuals as a function of two variables \\(L(\\beta_{0},\\beta_{1})\\) defined by\n\\[\nL(\\beta_{0},\\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2\n\\]\nwhere \\(n\\) is the number of observations in the data set. To fit our model, we need to find the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) that minimize \\(L(\\beta_{0},\\beta_{1})\\).\n\nMultiple Linear Regression\nSuppose that we have data of the form \\((y_{i},{\\bf x}_{i}) = (y_{i},x_{i1},x_{i2},\\ldots , x_{ip})\\) so that there are \\(p\\) predictor variables. A multiple linear regression model takes the form\n\\[\ny = \\beta_{0} + \\beta_{1}{\\bf x}_{1} + \\beta_{2}{\\bf x}_{2} + \\cdots + \\beta_{p}{\\bf x}_{p} + \\epsilon\n\\] Multiple linear regression is a significant generalization of simple linear regression because it not only allows us to account for multiple predictor variables, but also allows us to account for certain types of nonlinearity and also predictor variables that are categorical. This is because:\n\nThe “linear” part of linear regression refers to linearity with respect to the coefficients \\({\\bf \\beta}\\).\nWe can use dummy variables to represent categorical predictor variables.\n\n\n\n\nNeural Networks\nNeural networks are a class of machine learning models that are inspired by the structure of the brain. They are composed of a series of layers of nodes or “neurons” that are connected to each other. Each artificial neuron is a simple computational unit that takes in a set of inputs, performs a computation, and produces an output. The output of one neuron is then used as the input to the next neuron. The first layer of artificial neurons is called the input layer and the last layer of neurons is called the output layer. The layers in between the input and output layers are called hidden layers. The number of hidden layers in a neural network is called the depth of the network. The number of neurons in each layer is called the width of the network. Figure 13 shows a neural network with one hidden layer consisting of 5 neurons or nodes.\n\n\n\nFigure 13: A neural network with a single hidden layer consisting of four neurons or nodes.\n\n\nNeural networks are conceptually simple but the mathematical details can be confusing. The general idea is that a neural network takes an input of \\(p\\) predictor variables \\(X = (X_{1},X_{2},\\ldots , X_{p})\\) and builds a nonlinear function \\(f(X)\\) to predict the response \\(Y\\). What distinguishes neural networks from other nonlinear methods is the particular structure of the model function \\(f\\).\n\nExploring a Neural Network Interactively\nIn order to develop some intuition, we will start by exploring an interactive visualization of a neural network via the Neural Network Playground website. Visit the Neural Network Playground.\nThe visualization allows you to create a neural network and then train it on a data set. The data set can be a classification problem or a regression problem. The visualization allows you to change the defining components of a neural network such as activation function, the number of hidden layers, the number of neurons in each layer."
  },
  {
    "objectID": "lesson08/index.html#case-studies",
    "href": "lesson08/index.html#case-studies",
    "title": "Lesson 8",
    "section": "Case Studies",
    "text": "Case Studies\nLet’s see how the tidymodels workflow can be used in a case study. View one or more of the following blog posts by the data scientist Julia Silge.\n\nGet started with tidymodels Palmer penguins\nFit and predict with tidymodels for bird baths in Australia\nTune and interpret decision trees for wind turbines\nPredict which Scooby Doo monsters are REAL with a tuned decision tree model\nModeling NCAA women’s basketball tournament seeds\nExplore art media over time in the Tate collection dataset\nPredicting injuries for Chicago traffic crashes\nTune random forests for IKEA prices\nMulticlass predictive modeling for NBER papers\nBootstrap confidence intervals for Super Bowl commercials\nResampling to understand gender in art history data\nPredict availability in water sources with random forest models\nEstimate change in CEO departures with bootstrap resampling\nBagging with tidymodels and astronaut missions\nPredict housing prices in Austin TX with tidymodels and xgboost\nPredicting class membership for the Datasaurus Dozen\nWhere are haunted cemeteries compared to haunted schools?"
  },
  {
    "objectID": "lesson07/index.html#cross-validation",
    "href": "lesson07/index.html#cross-validation",
    "title": "Lesson 7",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nThinking back to nearest neighbors, there is a hyperparameter, namely \\(k\\) that we must set in order to train a model. Ideally, \\(k\\) should be chosen so that the resulting model has the best possible performance with regard to test error. In order to do this, we need to be able to estimate test error. Cross-validation (CV) is a computational technique that involves holding out pieces of the data in order to assess the predictive accuracy of a model or to select hyperparameter values.\nThere are difference approaches to cross-validation but the most common method is \\(V\\)-fold cross-validation1. This method involves randomly dividing the training set into \\(V\\) groups called folds of approximately equal size. The first fold is treated as a validation set and the model is fit on the remaining \\(V-1\\) folds. Then, the error \\(E_{1}\\) is computed for the held out set. This procedure is repeated \\(V\\) times and the \\(V\\)-fold CV estimate for the test error is\n\\[\n\\text{CV}_{V} = \\frac{1}{V}\\sum_{i=1}^{k}E_{i}\n\\] Figure 14 illustrates \\(V\\)-fold cross-validation for \\(V=5\\).\n\n\n\nFigure 14: An illustration of the. idea of V-fold cross-validation.\n\n\nWhenever \\(V=1\\), we have what is known as leave one out cross-validation (LOOCV). The most common values for \\(V\\) are five and ten. In the next lesson, we will see how to efficiently create the folds for cross-validation using the rsample package which is part of the tidymodels family of packages. Then later, we will look at several detailed applications of cross-validation to some machine learning models."
  },
  {
    "objectID": "lesson07/index.html#footnotes",
    "href": "lesson07/index.html#footnotes",
    "title": "Lesson 7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(V\\)-fold cross-validation is also commonly referred to as \\(k\\)-fold cross-validation but we avoid the \\(k\\) notation because \\(k\\) will appear as a hyperparameter for several of the machine learning algorithms we will discuss.↩︎"
  },
  {
    "objectID": "lesson08/index.html#the-tidymodels-machine-learning-workflow",
    "href": "lesson08/index.html#the-tidymodels-machine-learning-workflow",
    "title": "Lesson 8",
    "section": "The tidymodels Machine Learning Workflow",
    "text": "The tidymodels Machine Learning Workflow\nThe tidymodels framework provides a consistent set of steps for training and evaluating machine learning models. The following is a list of the steps in the tidymodels workflow:\n\nData Preparation:\n\nLoad and prepare your data set following tidy data principles.\n\nData Splitting:\n\nUse the rsample package to create data splits for training and testing.\n\nPreprocessing and Feature Engineering:\n\nDefine a data preprocessing plan using the recipes package.\nCreate a “recipe” for data cleaning, transformation, and feature engineering.\n\nModel Specification:\n\nSpecify machine learning models with the parsnip package.\nChoose from a variety of models for regression, classification, etc.\n\nHyperparameter Tuning:\n\nUtilize the tune package for hyperparameter tuning.\nDefine a grid of hyperparameters and use resampling methods for evaluation.\n\nModel Training:\n\nTrain models with the specified data splits, preprocessing plan, and hyperparameters.\nUse the fit function to train on the training data.\n\nModel Evaluation:\n\nAssess model performance using the yardstick package.\nCalculate relevant evaluation metrics (e.g., accuracy, RMSE).\n\nModel Selection:\n\nCompare model performance and hyperparameter settings.\nSelect the best-performing model or create ensembles (e.g., stacking, bagging).\n\nModel Interpretation and Visualization:\n\nUse the broom package to extract and tidy model results.\nVisualize model outputs and interpret results.\n\nDeployment and Prediction:\n\nDeploy the final model for predictions on new data.\nUse the trained model in production environments.\n\nDocumentation and Reproducibility:\n\nDocument the entire workflow, including preprocessing, model specifications, tuning, and results.\nEnsure reproducibility and transparency.\n\nSharing and Collaboration:\n\nShare code and findings with collaborators for reproducibility and collaboration.\n\nOngoing Monitoring and Maintenance:\n\nIn production, monitor model performance and update models as needed.\n\n\nThe best references for tidymodels are the tidymodels.org website and the Tidy Modeling with R book. The Tidy Modeling with R book is available for free online and is a great resource for learning the tidymodels framework (Kuhn and Silge 2022). The blog posts by Julia Silge are also a great resource for learning tidymodels. View Silge’s blog.\n\ntidymodels Family of Packages\nThe tidymodels package in R is an integrated ecosystem of packages designed to streamline the process of creating, evaluating, and deploying machine learning models while adhering to tidy data principles. The tidymodels framework follows a structured and consistent approach to machine learning, making it easier for data scientists and analysts to work with data and build predictive models. Here’s a brief description of the key components of the tidymodels package:\n\ntidyverse Integration:\n\nDescription: tidymodels seamlessly integrates with the popular tidyverse suite of packages, allowing for the use of tidy data frames and other tidy tools.\n\nrsample:\n\nDescription: The rsample package is for resampling and creating data splits, essential for tasks such as cross-validation and bootstrapping.\n\nrecipes:\n\nDescription: The recipes package provides a systematic way to define and preprocess feature engineering steps for your data, creating a “recipe” that includes data preprocessing and variable transformations.\n\nparsnip:\n\nDescription: The parsnip package defines a common interface for specifying machine learning models, making it easier to work with different modeling engines.\n\nworkflows:\n\nDescription: The workflows package simplifies the process of building, tuning, and evaluating models by combining models, recipes, and tuning into a unified workflow.\n\ntune:\n\nDescription: The tune package offers tools for hyperparameter tuning, allowing you to optimize model performance by systematically searching for the best hyperparameters.\n\nyardstick:\n\nDescription: The yardstick package provides a wide range of functions for model evaluation, including metrics for classification, regression, and survival analysis tasks.\n\nbroom:\n\nDescription: The broom package helps tidy up the results of model fits, making it easy to extract coefficients, predictions, and other model-related information in a tidy data format.\n\nCommunity and Extensibility:\n\nDescription: The tidymodels ecosystem has a growing community of users and contributors. It supports the creation of custom modeling engines, extending the framework to new algorithms.\n\nReproducibility and Best Practices:\n\nDescription: tidymodels promotes best practices in machine learning, emphasizing tidy data principles, clear model specification, and reproducibility.\n\n\n\n\nBasic Models\nHere we provide lists of the most common supervised machine learning models. Remember that the choice of the machine learning model depends on the specific problem, the nature of the data, and the trade-off between interpretability and predictive performance. It’s often a good practice to experiment with multiple models and evaluate their performance to select the most suitable one for a given task.\n\nSummary and Characteristics of Common Supervised Machine Learning Models\n\nLinear Regression:\n\nSummary: Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.\nCharacteristics:\n\nSuitable for regression tasks (predicting continuous numeric values).\nAssumes a linear relationship between predictors and the target.\nSimple and interpretable.\n\n\nLogistic Regression:\n\nSummary: Logistic regression is used for binary classification, modeling the probability that an instance belongs to a particular class.\nCharacteristics:\n\nSuitable for binary classification tasks.\nUses the logistic function to model probabilities.\nProvides probabilities and interpretable coefficients.\n\n\nDecision Trees:\n\nSummary: Decision trees divide the data into subsets based on the most significant attributes, making them suitable for both classification and regression tasks.\nCharacteristics:\n\nNon-linear and can capture complex relationships.\nProne to overfitting but can be regularized.\nEasily interpretable.\n\n\nRandom Forest:\n\nSummary: Random forests are an ensemble of decision trees that improve predictive accuracy and reduce overfitting.\nCharacteristics:\n\nCombines multiple decision trees for better performance.\nHandles feature importance and reduces variance.\nWorks well for classification and regression.\n\n\nSupport Vector Machines (SVM):\n\nSummary: SVMs aim to find a hyperplane that best separates data points into different classes.\nCharacteristics:\n\nEffective for binary classification and can be extended to multiclass.\nUses kernel functions for non-linear separations.\nGood for high-dimensional data.\n\n\nK-Nearest Neighbors (K-NN):\n\nSummary: K-NN assigns a class to a data point based on the majority class among its k-nearest neighbors in feature space.\nCharacteristics:\n\nSimple and intuitive.\nCan handle both classification and regression.\nSensitive to the choice of k.\n\n\nNaive Bayes:\n\nSummary: Naive Bayes is a probabilistic classifier based on Bayes’ theorem, assuming that features are conditionally independent.\nCharacteristics:\n\nEfficient and suitable for text classification.\nAssumes feature independence (naive assumption).\nWorks well for high-dimensional data.\n\n\nGradient Boosting Machines (GBM):\n\nSummary: GBMs build an ensemble of weak learners (usually decision trees) to create a strong predictive model.\nCharacteristics:\n\nCombines multiple weak learners for high accuracy.\nHandles regression and classification tasks.\nProne to overfitting, but can be regularized.\n\n\nNeural Networks (Deep Learning):\n\nSummary: Neural networks, especially deep learning models, are highly flexible and can model complex relationships in data.\nCharacteristics:\n\nExtremely powerful for various tasks.\nRequires large amounts of data and computational resources.\nInterpretability can be a challenge for deep models.\n\n\nEnsemble Methods:\n\nSummary: Ensemble methods combine multiple base models to improve predictive performance.\nCharacteristics:\n\nInclude bagging (e.g., Random Forest), boosting (e.g., AdaBoost), and stacking.\nOften more robust and accurate than individual models.\nHandle various types of data and tasks.\n\n\n\n\n\nModel Specifications using parsnip\nThe parsnip package in the tidymodels family allows one to specify a variety of supervised machine learning models. Here is a list of some of the parsnip models:\n\nlinear_reg:\n\nDescription: Linear Regression Model\nType: Regression\nDocumentation: linear_reg Documentation\n\nlogistic_reg:\n\nDescription: Logistic Regression Model\nType: Classification\nDocumentation: logistic_reg Documentation\n\ndecision_tree:\n\nDescription: Decision Tree Model\nType: Both (Regression and Classification)\nDocumentation: decision_tree Documentation\n\nrand_forest:\n\nDescription: Random Forest Model\nType: Both (Regression and Classification)\nDocumentation: rand_forest Documentation\n\nsvm_rbf:\n\nDescription: Support Vector Machine with Radial Basis Function Kernel Model\nType: Both (Regression and Classification)\nDocumentation: svm_rbf Documentation\n\nsvm_linear:\n\nDescription: Support Vector Machine with Linear Kernel Model\nType: Both (Regression and Classification)\nDocumentation: svm_linear Documentation\n\nnearest_neighbor:\n\nDescription: k-Nearest Neighbors Model\nType: Both (Regression and Classification)\nDocumentation: nearest_neighbor Documentation\n\nnaive_Bayes:\n\nDescription: Naive Bayes Model\nType: Classification\nDocumentation: naive_Bayes Documentation\n\nboost_tree:\n\nDescription: Gradient Boosting Model\nType: Both (Regression and Classification)\nDocumentation: boost_tree Documentation\n\nmlp:\n\nDescription: Multilayer Perceptron (Neural Network) Model\nType: Both (Regression and Classification)\nDocumentation: mlp Documentation\n\nbrulee:\n\nDescription: The R brulee package contains several basic modeling functions that use the torch package infrastructure, such as: linear regression, logistic regression, and neural networks.\nType: Both (Regression and Classification)\nDocumentation: brulee Documentation\n\n\nNote: Some of the model specifications in parsnip are wrappers for other packages and you will need to have those packages installed for everything to work. Make sure to refer to the the parsnip documentation. before fitting models."
  },
  {
    "objectID": "lesson09/index.html#overview",
    "href": "lesson09/index.html#overview",
    "title": "Lesson 9",
    "section": "Overview",
    "text": "Overview\nStatistical inference is the process of using data analysis to infer properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.\nIn this lesson, we will get a feel for the inferential method. There are many details of statistics that we will leave for a later course. Further, our approach will be computational, utilizing the bootstrap resampling method."
  },
  {
    "objectID": "lesson09/index.html#motivation",
    "href": "lesson09/index.html#motivation",
    "title": "Lesson 9",
    "section": "Motivation",
    "text": "Motivation\nRandom variables and their distributions model processes that produce data. For example, a binomial random variable with probability of success \\(\\pi = 0.5\\) can be used to model the process of tossing a coin and observing the number of heads. This is an illustration of the domain of probability. Statistical inference is concerned with the inverse problem: given data, what can we say about the process that produced it? For example, given a sample of coin tosses, what can we say about the probability of heads? This is an illustration of the domain of statistics.\nQuestion: Given a coin, how can you determine if it is fair or not? Think about how you could approach answering this question.\nObviously, to address the previous question we should collect data. That is, we should toss the coin some number (probably many) times and record the number of heads. Then, we can use the data to estimate the probability of heads.\nQuestion: Suppose that we toss a coin 10 times and observe 7 heads. What is your best guess for the probability of heads? Suppose that we toss a coin 100 times and observe 70 heads. What is your best guess for the probability of heads?\nAt this point, there are a few things to take note of:\n\nWhen we do statistical inference, we are often trying to estimate a parameter(s) of a distribution. The parameter(s) should be viewed as fixed but with unknown values. In this case, we call the parameter or parameters a population parameter. In the coin example, the population parameter is the probability of heads, \\(\\pi\\).\nWe have some process to estimate the population parameter. This process inputs observed data and returns an estimated value for the parameter(s). In this case, we call the estimate a point estimate. In the coin example, the point estimate is the number of heads divided by the number of tosses.\nThe point estimate is a random variable. That is, if we were to repeat the experiment, we would get a different point estimate. This is because the observed data is random. In this case, we call the point estimate a statistic. In the coin example, the statistic is the number of heads divided by the number of tosses.\nSince the point estimate is a random variable, it has a distribution. In this case, we call the distribution a sampling distribution. In the coin example, the sampling distribution is the distribution of the number of heads divided by the number of tosses.\nA key problem in statistical inference is to determine or describe the sampling distribution of a statistic. For example, we might be interested to know what is the mean and variance of the sampling distribution. The standard deviation of the sampling distribution of a statistic is called the standard error. We call the process of determining the sampling distribution statistical inference. In the coin example, we want to determine the sampling distribution of the number of heads divided by the number of tosses.\nOne can use the sampling distribution of a statistic to make statements about the uncertainty of point estimates.\n\nIn mathematical statistics, there is a heavy focus on deriving closed form or asymptotically exact expressions for the sampling distribution of a statistic. This is amazing and beautiful and well worth learning. For this, we highly recommend (Wasserman 2004). However, this approach is beyond the scope of this course. Further, the precise formulas of mathematical statistics apply in a much too limited range of application for the purposes of modern data science.\nAs an alternative to mathematical statistics, one can take a computational approach. That is, use the computer to simulate the sampling distribution of a statistic. This approach is called bootstrapping. The bootstrap is a powerful tool that can be used to approximate the sampling distribution of a statistic. It is also a useful tool for hypothesis testing and obtaining confidence intervals. Even our coverage of the bootstrap will be only introductory. To learn more beyond what we cover, we recommend (Efron and Hastie 2022)."
  },
  {
    "objectID": "lesson09/index.html#resampling",
    "href": "lesson09/index.html#resampling",
    "title": "Lesson 9",
    "section": "Resampling",
    "text": "Resampling\nLet’s start with a simulation. Here’s what we will do: Simulate 100 tosses of a fair coin, that is, with \\(\\pi = 0.5\\), count the number of heads, then estimate \\(\\pi\\) by dividing the number of heads by 100. Then, we will repeat this for 500 times. The code in below does this and displays the results in a table.\n\n\nCode\nset.seed(1234)\ncoin_df &lt;- tibble(num_heads = rbinom(500,size=100,prob=0.5))\n\ncoin_df &lt;- coin_df %&gt;%\n  mutate(p_hat = num_heads/100)\n\ncoin_df %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nnum_heads\np_hat\n\n\n\n\n47\n0.47\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n53\n0.53\n\n\n53\n0.53\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n47\n0.47\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n56\n0.56\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n58\n0.58\n\n\n53\n0.53\n\n\n46\n0.46\n\n\n62\n0.62\n\n\n46\n0.46\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n60\n0.60\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n56\n0.56\n\n\n44\n0.44\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n53\n0.53\n\n\n56\n0.56\n\n\n46\n0.46\n\n\n57\n0.57\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n50\n0.50\n\n\n53\n0.53\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n46\n0.46\n\n\n47\n0.47\n\n\n58\n0.58\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n56\n0.56\n\n\n45\n0.45\n\n\n46\n0.46\n\n\n54\n0.54\n\n\n56\n0.56\n\n\n48\n0.48\n\n\n59\n0.59\n\n\n53\n0.53\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n58\n0.58\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n54\n0.54\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n53\n0.53\n\n\n54\n0.54\n\n\n42\n0.42\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n61\n0.61\n\n\n45\n0.45\n\n\n54\n0.54\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n56\n0.56\n\n\n53\n0.53\n\n\n56\n0.56\n\n\n63\n0.63\n\n\n42\n0.42\n\n\n57\n0.57\n\n\n61\n0.61\n\n\n62\n0.62\n\n\n54\n0.54\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n39\n0.39\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n49\n0.49\n\n\n53\n0.53\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n39\n0.39\n\n\n49\n0.49\n\n\n56\n0.56\n\n\n63\n0.63\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n49\n0.49\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n48\n0.48\n\n\n58\n0.58\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n60\n0.60\n\n\n45\n0.45\n\n\n48\n0.48\n\n\n58\n0.58\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n44\n0.44\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n53\n0.53\n\n\n56\n0.56\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n48\n0.48\n\n\n58\n0.58\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n60\n0.60\n\n\n47\n0.47\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n58\n0.58\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n61\n0.61\n\n\n51\n0.51\n\n\n55\n0.55\n\n\n68\n0.68\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n40\n0.40\n\n\n54\n0.54\n\n\n59\n0.59\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n39\n0.39\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n44\n0.44\n\n\n55\n0.55\n\n\n59\n0.59\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n43\n0.43\n\n\n57\n0.57\n\n\n39\n0.39\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n42\n0.42\n\n\n57\n0.57\n\n\n55\n0.55\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n56\n0.56\n\n\n41\n0.41\n\n\n58\n0.58\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n60\n0.60\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n49\n0.49\n\n\n41\n0.41\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n64\n0.64\n\n\n52\n0.52\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n51\n0.51\n\n\n59\n0.59\n\n\n62\n0.62\n\n\n55\n0.55\n\n\n51\n0.51\n\n\n51\n0.51\n\n\n54\n0.54\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n54\n0.54\n\n\n55\n0.55\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n59\n0.59\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n51\n0.51\n\n\n44\n0.44\n\n\n45\n0.45\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n58\n0.58\n\n\n39\n0.39\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n41\n0.41\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n60\n0.60\n\n\n46\n0.46\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n49\n0.49\n\n\n42\n0.42\n\n\n50\n0.50\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n45\n0.45\n\n\n54\n0.54\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n63\n0.63\n\n\n52\n0.52\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n47\n0.47\n\n\n55\n0.55\n\n\n51\n0.51\n\n\n51\n0.51\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n57\n0.57\n\n\n53\n0.53\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n43\n0.43\n\n\n53\n0.53\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n42\n0.42\n\n\n55\n0.55\n\n\n48\n0.48\n\n\n41\n0.41\n\n\n56\n0.56\n\n\n54\n0.54\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n56\n0.56\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n53\n0.53\n\n\n43\n0.43\n\n\n57\n0.57\n\n\n37\n0.37\n\n\n59\n0.59\n\n\n46\n0.46\n\n\n58\n0.58\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n43\n0.43\n\n\n54\n0.54\n\n\n59\n0.59\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n58\n0.58\n\n\n53\n0.53\n\n\n44\n0.44\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n48\n0.48\n\n\n61\n0.61\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n51\n0.51\n\n\n53\n0.53\n\n\n58\n0.58\n\n\n52\n0.52\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n60\n0.60\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n57\n0.57\n\n\n53\n0.53\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n64\n0.64\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n56\n0.56\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n56\n0.56\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n59\n0.59\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n41\n0.41\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n52\n0.52\n\n\n56\n0.56\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n\n\n\n\n\nFigure 1 plots the 500 different estimates for \\(\\pi\\).\n\n\nCode\ncoin_df %&gt;%\n  ggplot(aes(x = p_hat)) +\n  geom_histogram(bins = 12,color=\"white\") + \n  labs(x = \"Estimate\",y = \"Count\") \n\n\n\n\n\nFigure 1: 500 estimates of \\(\\pi\\) from simulated data.\n\n\n\n\nQuestion: What are your takeaways from our simulation experiment and plot in Figure 1? What, if anything, do you think we can say about the sampling distribution of \\(\\pi\\)?\nNow, let’s try a slightly different simulation. We will do a single round of tossing a fair coin 100 times. Then, we will resample from the 100 tosses with replacement 500 times. For each resample, we will count the number of heads and again estimate \\(\\pi\\) by dividing the number of heads by 100. The code in below does this and displays the results in a table.\n\n\nCode\nset.seed(1234)\ncoin_sample &lt;- rbinom(100,size=1,prob=0.5)\n\ncoin_resample_df &lt;- tibble(num_heads = replicate(500,sum(sample(coin_sample,100,replace=TRUE)))) %&gt;%\n  mutate(p_hat_b = num_heads/100)\n\n\ncoin_resample_df %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nnum_heads\np_hat_b\n\n\n\n\n52\n0.52\n\n\n41\n0.41\n\n\n49\n0.49\n\n\n41\n0.41\n\n\n30\n0.30\n\n\n34\n0.34\n\n\n39\n0.39\n\n\n34\n0.34\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n54\n0.54\n\n\n39\n0.39\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n43\n0.43\n\n\n37\n0.37\n\n\n43\n0.43\n\n\n44\n0.44\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n36\n0.36\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n41\n0.41\n\n\n55\n0.55\n\n\n49\n0.49\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n47\n0.47\n\n\n37\n0.37\n\n\n42\n0.42\n\n\n43\n0.43\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n36\n0.36\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n51\n0.51\n\n\n42\n0.42\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n42\n0.42\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n38\n0.38\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n41\n0.41\n\n\n46\n0.46\n\n\n40\n0.40\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n42\n0.42\n\n\n54\n0.54\n\n\n40\n0.40\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n35\n0.35\n\n\n37\n0.37\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n36\n0.36\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n40\n0.40\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n31\n0.31\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n29\n0.29\n\n\n36\n0.36\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n40\n0.40\n\n\n45\n0.45\n\n\n38\n0.38\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n53\n0.53\n\n\n54\n0.54\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n41\n0.41\n\n\n50\n0.50\n\n\n57\n0.57\n\n\n38\n0.38\n\n\n45\n0.45\n\n\n33\n0.33\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n40\n0.40\n\n\n42\n0.42\n\n\n54\n0.54\n\n\n38\n0.38\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n38\n0.38\n\n\n41\n0.41\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n41\n0.41\n\n\n40\n0.40\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n53\n0.53\n\n\n38\n0.38\n\n\n44\n0.44\n\n\n52\n0.52\n\n\n42\n0.42\n\n\n48\n0.48\n\n\n39\n0.39\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n42\n0.42\n\n\n38\n0.38\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n44\n0.44\n\n\n41\n0.41\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n44\n0.44\n\n\n34\n0.34\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n41\n0.41\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n37\n0.37\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n55\n0.55\n\n\n34\n0.34\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n34\n0.34\n\n\n34\n0.34\n\n\n44\n0.44\n\n\n32\n0.32\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n42\n0.42\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n36\n0.36\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n36\n0.36\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n38\n0.38\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n44\n0.44\n\n\n41\n0.41\n\n\n48\n0.48\n\n\n40\n0.40\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n39\n0.39\n\n\n56\n0.56\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n46\n0.46\n\n\n57\n0.57\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n36\n0.36\n\n\n46\n0.46\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n51\n0.51\n\n\n37\n0.37\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n38\n0.38\n\n\n39\n0.39\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n36\n0.36\n\n\n51\n0.51\n\n\n38\n0.38\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n45\n0.45\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n41\n0.41\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n51\n0.51\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n56\n0.56\n\n\n42\n0.42\n\n\n52\n0.52\n\n\n40\n0.40\n\n\n54\n0.54\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n38\n0.38\n\n\n37\n0.37\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n37\n0.37\n\n\n43\n0.43\n\n\n46\n0.46\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n65\n0.65\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n41\n0.41\n\n\n51\n0.51\n\n\n41\n0.41\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n55\n0.55\n\n\n42\n0.42\n\n\n52\n0.52\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n54\n0.54\n\n\n38\n0.38\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n40\n0.40\n\n\n36\n0.36\n\n\n36\n0.36\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n28\n0.28\n\n\n40\n0.40\n\n\n36\n0.36\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n57\n0.57\n\n\n40\n0.40\n\n\n36\n0.36\n\n\n51\n0.51\n\n\n46\n0.46\n\n\n51\n0.51\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n40\n0.40\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n42\n0.42\n\n\n43\n0.43\n\n\n42\n0.42\n\n\n53\n0.53\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n32\n0.32\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n44\n0.44\n\n\n39\n0.39\n\n\n34\n0.34\n\n\n52\n0.52\n\n\n40\n0.40\n\n\n38\n0.38\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n33\n0.33\n\n\n41\n0.41\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n42\n0.42\n\n\n36\n0.36\n\n\n44\n0.44\n\n\n53\n0.53\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n40\n0.40\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n46\n0.46\n\n\n42\n0.42\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n40\n0.40\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n30\n0.30\n\n\n47\n0.47\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n31\n0.31\n\n\n44\n0.44\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n49\n0.49\n\n\n39\n0.39\n\n\n\n\n\n\n\nNote the the number of heads in the original sample is 45.\nFigure 2 plots the 500 different estimates for \\(\\pi\\) based on the resampled data.\n\n\nCode\ncoin_resample_df %&gt;%\n  ggplot(aes(x = p_hat_b)) +\n  geom_histogram(bins = 12,color=\"white\") + \n  labs(x = \"Estimate\",y = \"Count\") \n\n\n\n\n\nFigure 2: 500 estimates of \\(\\pi\\) from resampled data.\n\n\n\n\nQuestion: What are your takeaways from our simulation experiment and plot in Figure 2? How do the results compare and contrast with those of the previous simulation and plot in Figure 1?"
  },
  {
    "objectID": "lesson09/index.html#bootstrap",
    "href": "lesson09/index.html#bootstrap",
    "title": "Lesson 9",
    "section": "Bootstrap",
    "text": "Bootstrap\nOur resampling simulation in the last section is an example of the bootstrap. Notice what we did, we took a sample from the population, then resampled from the sample. This is called resampling. The bootstrap is a special case of resampling where the resampling is done with replacement. What we did was generate a bootstrap distribution for our statistic. The idea is,\n\nif the sample is representative of the population, then the bootstrap distribution of our statistics obtained by resampling with replacement from the sample should approximate the sampling distribution for our statistic.\n\nWhile the mean of the bootstrap will be the mean of the sample and not necessarily the mean of the population, the standard deviation of the bootstrap will be a good estimate for the standard error of the statistic. Thus, the bootstrap distribution allows us to assess the uncertainty of an estimate. The power of the bootstrap technique is that it works for any statistic. For example, we can use the bootstrap to estimate the sampling distribution for the mean for a normal random variable, or parameters in models such as the coefficients in a linear regression model. We can even use the bootstrap to estimate the uncertainty in model predictions, even if the model is non-parametric.\n\nExample: Bootstrap Mean\nLet’s take \\(n = 50\\) samples \\(X_{1},X_{2},\\ldots, X_{n}\\) from a normal distribution with \\(\\mu = 10\\) and \\(\\sigma = 1.75\\), that is, with \\(X_{i} \\sim \\text{Norm}(\\mu = 10, \\sigma = 1.75 )\\):\n\nset.seed(4321)\nn &lt;- 50\nmu &lt;- 10\nsigma &lt;- 1.75\nx &lt;- rnorm(n,mu,sigma)\n\nA point estimate for the mean \\(\\mu\\) is the sample mean \\(\\hat{\\mu}\\) defined by\n\\[\n\\hat{\\mu} = \\frac{X_{1} + X_{2} + \\cdots + X_{n}}{n}\n\\] Let’s compute the sample mean for our sample:\n\n(x_bar &lt;- mean(x))\n\n[1] 10.20559\n\n\nNow, let’s build the bootstrap distribution for the sample mean. We will do this by resampling with replacement from the sample 500 times. For each resample, we will compute the sample mean. The code below does this and displays the results.\n\nmean_resample_df &lt;- tibble(x_bar_b = replicate(500,mean(sample(x,n,replace=TRUE)))) \n\nmean_resample_df %&gt;%\n  ggplot(aes(x = x_bar_b)) +\n  geom_histogram(bins = 12,color=\"white\") + \n  labs(x = \"Sample mean\",y = \"Count\") \n\n\n\n\nNow, statistical theory tell us that the standard error of the sample mean is given by\n\\[\n\\text{SE}(\\hat{\\mu}) = \\frac{\\sigma}{\\sqrt{n}}\n\\] where \\(\\sigma\\) is the standard deviation of the population. Let’s compute the standard error of the sample mean for our sample:\n\n(se &lt;- sigma/sqrt(n))\n\n[1] 0.2474874\n\n\nLet’s compare this with the standard deviation of our bootstrap distribution:\n\nsd(mean_resample_df$x_bar_b)\n\n[1] 0.1938924\n\n\nWe use confidence intervals (CIs) to quantify the uncertainty in our estimates. Classically, a 95% CI for the mean is computed in R using:\n\nt.test(x)$conf.int\n\n[1]  9.81419 10.59699\nattr(,\"conf.level\")\n[1] 0.95\n\n\nor using a function from the infer package:\n\nsampling_dist &lt;- tibble(x=x) %&gt;%\n  specify(response = x) %&gt;%\n  assume(\"t\")\n\nsample_mean &lt;- tibble(x=x) %&gt;%\n  specify(response = x) %&gt;%\n  calculate(stat = \"mean\")\n\nget_confidence_interval(sampling_dist,point_estimate = sample_mean,level=0.95,type=\"se\")\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     9.81     10.6\n\n\nWe can estimate this CI using the bootstrap. The code below computes an estimated 95% CI for the mean using the bootstrap:\n\nquantile(mean_resample_df$x_bar_b,probs = c(0.025,0.975))\n\n     2.5%     97.5% \n 9.831469 10.597012 \n\n\nor using the infer package:\n\nmean_resample_df %&gt;%\n  get_confidence_interval(level=0.95)\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     9.83     10.6\n\n\nExercise: Redo what we have done in bootstrapping the mean of a sample from a normal distribution but instead bootstrap the median and estimate the standard error and a 95% CI for the statistic. You can use the fact that the function median computes the sample median. How do the results compare and contrast with those of the mean?\n\n\nExample: Linear Regression\nLet’s suppose that we have samples from a normal distribution but where the mean varies as a linear function of a variable \\(x\\). That is, we have samples from a distribution \\(\\text{Norm}(\\beta_{0} + \\beta_{1}x, \\sigma)\\), where \\(x\\) ranges over some values. This is equivalent to assuming that we have a random variable \\(Y\\) that satisfies\n\\[\nY = \\beta_{0} + \\beta_{1}x + \\epsilon\n\\] and \\(\\epsilon \\sim \\text{Norm}(0,\\sigma)\\).\nWe can use the bootstrap to estimate the uncertainty in estimates for the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) in a manner similar to how we estimated the uncertainty in the sample mean in the last subsection. However, in this case, the mean is estimated by a linear regression model. That is, we have estimators \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\), values of which are obtained by fitting a linear model to the data. Bootstrapping allows us to approximate the sampling distributions for these estimators.\nFirst, we will simulate some data. The simulated data is shown in Figure 3. Notice that the true mean is given by \\(y = 0.5 x + 2\\) which appears as the red dashed line in the figure.\n\n# sample size\nN &lt;- 25\n\n# true intercept and slope parameters\nbeta_0 &lt;- 2\nbeta_1 &lt;- 0.5\n\n# x-values\nx &lt;- seq(0, 3, length.out = N)\n\n# data simulation\nlm_samp &lt;- function(x,samp_n=10,sd_val=1){\n  \n  # sample from a normal distribution with mean = beta_0 + beta_1*x\n  y &lt;- rnorm(samp_n, mean=beta_0 + beta_1*x, sd=sd_val)\n  \n  tibble(x=x,y=list(y))\n  \n}\n\n# obtain simulated data\nlm_samp_df &lt;- map_dfr(x,lm_samp)\n\nlm_samp_df_l &lt;- lm_samp_df %&gt;%\n  unnest(y)\n\n# plot simulated data\nlm_samp_df_l %&gt;%\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha=0.6) +\n  geom_abline(intercept=beta_0,slope=beta_1,color=\"red\",\n              linewidth=1,linetype=\"dashed\") + \n  geom_smooth(method=\"lm\",color=\"orange\",fill=\"lightblue\")\n\n\n\n\nFigure 3: Simulated data for simple linear regression. The true mean is given by \\(y = 0.5 x + 2\\)\n\n\n\n\nThe orange line in Figure 3 is the fitted linear model and the light blue shaded region shows the corresponding standard error computed by traditional methods.\nIn R, the function lm is used to fit a linear model. The code below fits a linear model to the simulated data and prints the estimated intercept and slope parameters, that is, the point estimates for \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) in Table 1.\n\nlinear_mod &lt;- lm(y ~ x, data=lm_samp_df_l)\n\ntidy(linear_mod) %&gt;%\n  select(estimate) %&gt;%\n  mutate(coeff=c(\"beta_0\",\"beta_1\")) %&gt;%\n  kable() %&gt;%\n  kable_styling()\n\n\n\nTable 1: Estimated intercept and slope parameters for the linear model fit to the simulated data.\n\n\nestimate\ncoeff\n\n\n\n\n1.7423412\nbeta_0\n\n\n0.5963686\nbeta_1\n\n\n\n\n\n\n\n\nWe would like to assess the uncertainty in our point estimates. Bootstrapping is one way to do this and the following code implements the bootstrap for our example. Figure 4 shows the bootstrap distribution for the intercept and slope parameters.\n\nlm_fit_resamp &lt;- function(int_val,df=lm_samp_df_l){\n  lm(y ~ x, data=slice_sample(df,prop=1,replace=TRUE)) %&gt;%\n    tidy() %&gt;%\n    select(estimate) %&gt;%\n    mutate(coeff=c(\"beta_0\",\"beta_1\"))\n}\n\n\nlm_fit_boot &lt;- map_dfr(1:500,lm_fit_resamp)\n\nlm_fit_boot %&gt;%\n  ggplot(aes(x=estimate)) +\n  geom_histogram(color=\"white\",fill=\"lightblue\") +\n  facet_wrap(~coeff,scales=\"free_x\")\n\n\n\n\nFigure 4: Bootstrap distributions for the intercept and slope parameters.\n\n\n\n\nIn the last line of code, we manually resampled the data and fit a linear model to each resample. The code below does the same thing but uses the function reg_intervals from the `rsample`` package to do the resampling and fitting. The results shown in Figure 5 are similar to those we saw in Figure 4.\n\nreg_intervals(y ~ x, data=lm_samp_df_l,\n              type=\"percentile\",filter=NULL,\n              keep_reps = TRUE) %&gt;%\n  unnest(.replicates) %&gt;%\n  ggplot(aes(x=estimate)) + \n  geom_histogram(color=\"white\",fill=\"lightblue\") +\n  facet_wrap(~term,scales=\"free_x\")\n\n\n\n\nFigure 5: Bootstrap distributions for the intercept and slope parameters. This time, the function reg_intervals is used to do the resampling and fitting.\n\n\n\n\nWe can also use reg_intervals to obtain the confidence intervals for each parameter estimate. The results are shown in\n\nreg_intervals(y ~ x, data=lm_samp_df_l,\n              type=\"percentile\",filter=NULL) %&gt;%\n  kable() %&gt;%\n  kable_styling()\n\n\n\nTable 2: Confidence intervals for the intercept and slope parameters obtained using the reg_intervals function.\n\n\nterm\n.lower\n.estimate\n.upper\n.alpha\n.method\n\n\n\n\n(Intercept)\n1.5134585\n1.7368193\n1.9720297\n0.05\npercentile\n\n\nx\n0.4696498\n0.5978922\n0.7210832\n0.05\npercentile\n\n\n\n\n\n\n\n\nFinally, we can use our bootstrapped estimates to plot the bootstrap distribution of the regression line. The results are shown in Figure 6. The orange line is the fitted linear model, the yellow shaded region shows the corresponding standard error computed by traditional methods, and the light blue lines are regression lines fitted to the resampled data. The light blue lines are the bootstrap distribution of the regression line.\n\ntpl &lt;- lm_fit_boot %&gt;% pivot_wider(names_from=coeff,values_from=estimate) %&gt;%\n  unnest(cols = c(beta_0, beta_1))\n\n\nlm_samp_df_l %&gt;%\n  ggplot(aes(x=x,y=y)) +\n  geom_point(alpha=0.6) +\n  geom_abline(data=tpl,\n              aes(intercept=beta_0,slope=beta_1),\n              color=\"lightblue\") +\n  geom_smooth(method=\"lm\",color=\"orange\",fill=\"yellow\") \n\n\n\n\nFigure 6: Bootstrap distribution of the regression line for the simulated data. The true value is given by \\(y = 0.5 x + 2\\)"
  },
  {
    "objectID": "lesson09/index.html#motivating-example",
    "href": "lesson09/index.html#motivating-example",
    "title": "Lesson 9",
    "section": "Motivating Example",
    "text": "Motivating Example\nRandom variables and their distributions model processes that produce data. For example, a binomial random variable with probability of success \\(\\pi = 0.5\\) can be used to model the process of tossing a coin and observing the number of heads. This is an illustration of the domain of probability.\nStatistical inference is concerned with the inverse problem: given data, what can we say about the process that produced it? For example, given a sample of coin tosses, what can we say about the probability of heads? This is an illustration of the domain of statistics.\nQuestion: Given a coin, how can you determine if it is fair or not? Think about how you could approach answering this question.\nObviously, to address the previous question we should collect data. That is, we should toss the coin some number of times (probably very many) and record the number of heads. Then, we can use the data to estimate the probability of heads.\nQuestion: Suppose that we toss a coin 10 times and observe 7 heads. What is your best guess for the probability of heads? Suppose that we toss a coin 100 times and observe 70 heads. What is your best guess for the probability of heads?\nAt this point, there are a few things to take note of:\n\nWhen we do statistical inference, we are often trying to estimate a parameter(s) of a distribution. The parameter(s) should be viewed as fixed but with unknown values. In this case, we call the parameter or parameters a population parameter. In the coin example, the population parameter is the probability of heads, \\(\\pi\\).\nWe have some process to estimate the population parameter. This process inputs observed data and returns an estimated value for the parameter(s). In this case, we call the estimate a point estimate. In the coin example, the point estimate is the number of heads divided by the number of tosses, we denote this by \\(\\hat{\\pi}\\).\nThe point estimate is a random variable. That is, if we were to repeat the experiment, we would get a different point estimate. This is because the observed data is random. In this case, we call the point estimate a statistic. In the coin example, the statistic is the number of heads divided by the number of tosses, that is, \\(\\hat{\\pi}\\).\nSince the point estimate is a random variable, it has a distribution. In this case, we call the distribution a sampling distribution. In the coin example, the sampling distribution is the distribution of \\(\\hat{\\pi}\\).\nA key problem in statistical inference is to determine or describe the sampling distribution of a statistic. For example, we might be interested to know what is the mean and variance of the sampling distribution. The standard deviation of the sampling distribution of a statistic is called the standard error. We call the process of determining the sampling distribution statistical inference. In the coin example, we want to determine the sampling distribution of \\(\\hat{\\pi}\\).\nOne can use the sampling distribution of a statistic to make statements about the uncertainty of point estimates.\n\nIn mathematical statistics, there is a heavy focus on deriving closed form or asymptotically exact expressions for the sampling distribution of a statistic. This is amazing and beautiful and well worth learning. For this, we highly recommend (Wasserman 2004) or the online book Introduction to Probability for Data Science. However, this approach is beyond the scope of this course. Further, the precise formulas of mathematical statistics apply in a much too limited range of application for the purposes of modern data science.\nAs an alternative to mathematical statistics, one can take a computational approach. That is, use the computer to simulate the sampling distribution of a statistic. This approach is called bootstrapping. The bootstrap is a powerful tool that can be used to approximate the sampling distribution of a statistic. It is also a useful tool for hypothesis testing and obtaining confidence intervals. Even our coverage of the bootstrap will be only introductory. To learn more beyond what we cover, we recommend (Efron and Hastie 2022)."
  },
  {
    "objectID": "lesson09/index.html#resampling-experiment",
    "href": "lesson09/index.html#resampling-experiment",
    "title": "Lesson 9",
    "section": "Resampling Experiment",
    "text": "Resampling Experiment\nLet’s start with a simulation. Here’s what we will do: Simulate 100 tosses of a fair coin, that is, sample from a binomial random variable with \\(\\pi = 0.5\\), count the number of heads, then estimate \\(\\pi\\) by dividing the number of heads by 100. Then, we will repeat this for 500 times. The code below does this and displays the results in a table.\n\n\nCode\nset.seed(1234)\ncoin_df &lt;- tibble(num_heads = rbinom(500,size=100,prob=0.5))\n\ncoin_df &lt;- coin_df %&gt;%\n  mutate(p_hat = num_heads/100)\n\ncoin_df %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nnum_heads\np_hat\n\n\n\n\n47\n0.47\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n53\n0.53\n\n\n53\n0.53\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n47\n0.47\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n56\n0.56\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n58\n0.58\n\n\n53\n0.53\n\n\n46\n0.46\n\n\n62\n0.62\n\n\n46\n0.46\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n60\n0.60\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n56\n0.56\n\n\n44\n0.44\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n53\n0.53\n\n\n56\n0.56\n\n\n46\n0.46\n\n\n57\n0.57\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n50\n0.50\n\n\n53\n0.53\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n46\n0.46\n\n\n47\n0.47\n\n\n58\n0.58\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n56\n0.56\n\n\n45\n0.45\n\n\n46\n0.46\n\n\n54\n0.54\n\n\n56\n0.56\n\n\n48\n0.48\n\n\n59\n0.59\n\n\n53\n0.53\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n58\n0.58\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n54\n0.54\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n53\n0.53\n\n\n54\n0.54\n\n\n42\n0.42\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n61\n0.61\n\n\n45\n0.45\n\n\n54\n0.54\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n56\n0.56\n\n\n53\n0.53\n\n\n56\n0.56\n\n\n63\n0.63\n\n\n42\n0.42\n\n\n57\n0.57\n\n\n61\n0.61\n\n\n62\n0.62\n\n\n54\n0.54\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n39\n0.39\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n49\n0.49\n\n\n53\n0.53\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n39\n0.39\n\n\n49\n0.49\n\n\n56\n0.56\n\n\n63\n0.63\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n49\n0.49\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n48\n0.48\n\n\n58\n0.58\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n60\n0.60\n\n\n45\n0.45\n\n\n48\n0.48\n\n\n58\n0.58\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n44\n0.44\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n53\n0.53\n\n\n56\n0.56\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n48\n0.48\n\n\n58\n0.58\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n60\n0.60\n\n\n47\n0.47\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n58\n0.58\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n61\n0.61\n\n\n51\n0.51\n\n\n55\n0.55\n\n\n68\n0.68\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n40\n0.40\n\n\n54\n0.54\n\n\n59\n0.59\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n39\n0.39\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n44\n0.44\n\n\n55\n0.55\n\n\n59\n0.59\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n43\n0.43\n\n\n57\n0.57\n\n\n39\n0.39\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n42\n0.42\n\n\n57\n0.57\n\n\n55\n0.55\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n56\n0.56\n\n\n41\n0.41\n\n\n58\n0.58\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n60\n0.60\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n49\n0.49\n\n\n41\n0.41\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n64\n0.64\n\n\n52\n0.52\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n51\n0.51\n\n\n59\n0.59\n\n\n62\n0.62\n\n\n55\n0.55\n\n\n51\n0.51\n\n\n51\n0.51\n\n\n54\n0.54\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n54\n0.54\n\n\n55\n0.55\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n59\n0.59\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n51\n0.51\n\n\n44\n0.44\n\n\n45\n0.45\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n58\n0.58\n\n\n39\n0.39\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n41\n0.41\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n60\n0.60\n\n\n46\n0.46\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n49\n0.49\n\n\n42\n0.42\n\n\n50\n0.50\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n45\n0.45\n\n\n54\n0.54\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n63\n0.63\n\n\n52\n0.52\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n47\n0.47\n\n\n55\n0.55\n\n\n51\n0.51\n\n\n51\n0.51\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n57\n0.57\n\n\n53\n0.53\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n43\n0.43\n\n\n53\n0.53\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n56\n0.56\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n42\n0.42\n\n\n55\n0.55\n\n\n48\n0.48\n\n\n41\n0.41\n\n\n56\n0.56\n\n\n54\n0.54\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n56\n0.56\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n53\n0.53\n\n\n43\n0.43\n\n\n57\n0.57\n\n\n37\n0.37\n\n\n59\n0.59\n\n\n46\n0.46\n\n\n58\n0.58\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n43\n0.43\n\n\n54\n0.54\n\n\n59\n0.59\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n58\n0.58\n\n\n53\n0.53\n\n\n44\n0.44\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n48\n0.48\n\n\n61\n0.61\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n51\n0.51\n\n\n53\n0.53\n\n\n58\n0.58\n\n\n52\n0.52\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n60\n0.60\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n54\n0.54\n\n\n51\n0.51\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n57\n0.57\n\n\n53\n0.53\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n64\n0.64\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n56\n0.56\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n56\n0.56\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n54\n0.54\n\n\n50\n0.50\n\n\n59\n0.59\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n57\n0.57\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n54\n0.54\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n41\n0.41\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n52\n0.52\n\n\n56\n0.56\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n52\n0.52\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n\n\n\n\n\nFigure 1 plots the 500 different estimates for \\(\\pi\\).\n\n\nCode\ncoin_df %&gt;%\n  ggplot(aes(x = p_hat)) +\n  geom_histogram(bins = 12,color=\"white\") + \n  labs(x = \"Estimate\",y = \"Count\") \n\n\n\n\n\nFigure 1: 500 estimates of the probability of success, \\(\\pi\\) from simulated data.\n\n\n\n\nQuestion: What are your takeaways from our simulation experiment and plot in Figure 1? What, if anything, do you think we can say about the sampling distribution of \\(\\pi\\)?\nNow, let’s try a slightly different simulation. We will do a single round of tossing a fair coin 100 times. Then, we will resample from the 100 tosses with replacement 500 times. For each resample, we will count the number of heads and again estimate \\(\\pi\\) by dividing the number of heads by 100. The code in below does this and displays the results in a table.\n\n\nCode\nset.seed(1234)\ncoin_sample &lt;- rbinom(100,size=1,prob=0.5)\n\ncoin_resample_df &lt;- tibble(num_heads = replicate(500,sum(sample(coin_sample,100,replace=TRUE)))) %&gt;%\n  mutate(p_hat_b = num_heads/100)\n\n\ncoin_resample_df %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\nnum_heads\np_hat_b\n\n\n\n\n52\n0.52\n\n\n41\n0.41\n\n\n49\n0.49\n\n\n41\n0.41\n\n\n30\n0.30\n\n\n34\n0.34\n\n\n39\n0.39\n\n\n34\n0.34\n\n\n53\n0.53\n\n\n52\n0.52\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n54\n0.54\n\n\n39\n0.39\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n43\n0.43\n\n\n37\n0.37\n\n\n43\n0.43\n\n\n44\n0.44\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n36\n0.36\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n41\n0.41\n\n\n55\n0.55\n\n\n49\n0.49\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n47\n0.47\n\n\n37\n0.37\n\n\n42\n0.42\n\n\n43\n0.43\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n36\n0.36\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n51\n0.51\n\n\n42\n0.42\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n42\n0.42\n\n\n48\n0.48\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n38\n0.38\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n41\n0.41\n\n\n46\n0.46\n\n\n40\n0.40\n\n\n52\n0.52\n\n\n55\n0.55\n\n\n42\n0.42\n\n\n54\n0.54\n\n\n40\n0.40\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n35\n0.35\n\n\n37\n0.37\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n36\n0.36\n\n\n46\n0.46\n\n\n55\n0.55\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n40\n0.40\n\n\n49\n0.49\n\n\n46\n0.46\n\n\n31\n0.31\n\n\n51\n0.51\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n29\n0.29\n\n\n36\n0.36\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n52\n0.52\n\n\n40\n0.40\n\n\n45\n0.45\n\n\n38\n0.38\n\n\n50\n0.50\n\n\n42\n0.42\n\n\n53\n0.53\n\n\n54\n0.54\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n41\n0.41\n\n\n50\n0.50\n\n\n57\n0.57\n\n\n38\n0.38\n\n\n45\n0.45\n\n\n33\n0.33\n\n\n45\n0.45\n\n\n52\n0.52\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n40\n0.40\n\n\n42\n0.42\n\n\n54\n0.54\n\n\n38\n0.38\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n38\n0.38\n\n\n41\n0.41\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n41\n0.41\n\n\n40\n0.40\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n53\n0.53\n\n\n38\n0.38\n\n\n44\n0.44\n\n\n52\n0.52\n\n\n42\n0.42\n\n\n48\n0.48\n\n\n39\n0.39\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n42\n0.42\n\n\n38\n0.38\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n44\n0.44\n\n\n41\n0.41\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n51\n0.51\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n44\n0.44\n\n\n34\n0.34\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n41\n0.41\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n49\n0.49\n\n\n37\n0.37\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n55\n0.55\n\n\n34\n0.34\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n34\n0.34\n\n\n34\n0.34\n\n\n44\n0.44\n\n\n32\n0.32\n\n\n40\n0.40\n\n\n48\n0.48\n\n\n42\n0.42\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n36\n0.36\n\n\n42\n0.42\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n36\n0.36\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n38\n0.38\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n53\n0.53\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n44\n0.44\n\n\n41\n0.41\n\n\n48\n0.48\n\n\n40\n0.40\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n46\n0.46\n\n\n39\n0.39\n\n\n56\n0.56\n\n\n51\n0.51\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n46\n0.46\n\n\n57\n0.57\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n53\n0.53\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n36\n0.36\n\n\n46\n0.46\n\n\n44\n0.44\n\n\n49\n0.49\n\n\n50\n0.50\n\n\n50\n0.50\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n44\n0.44\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n51\n0.51\n\n\n37\n0.37\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n38\n0.38\n\n\n39\n0.39\n\n\n47\n0.47\n\n\n46\n0.46\n\n\n36\n0.36\n\n\n51\n0.51\n\n\n38\n0.38\n\n\n48\n0.48\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n48\n0.48\n\n\n45\n0.45\n\n\n48\n0.48\n\n\n49\n0.49\n\n\n51\n0.51\n\n\n41\n0.41\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n47\n0.47\n\n\n54\n0.54\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n51\n0.51\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n34\n0.34\n\n\n51\n0.51\n\n\n45\n0.45\n\n\n56\n0.56\n\n\n42\n0.42\n\n\n52\n0.52\n\n\n40\n0.40\n\n\n54\n0.54\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n43\n0.43\n\n\n47\n0.47\n\n\n50\n0.50\n\n\n38\n0.38\n\n\n37\n0.37\n\n\n51\n0.51\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n37\n0.37\n\n\n43\n0.43\n\n\n46\n0.46\n\n\n48\n0.48\n\n\n55\n0.55\n\n\n65\n0.65\n\n\n41\n0.41\n\n\n42\n0.42\n\n\n41\n0.41\n\n\n51\n0.51\n\n\n41\n0.41\n\n\n43\n0.43\n\n\n49\n0.49\n\n\n40\n0.40\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n49\n0.49\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n55\n0.55\n\n\n42\n0.42\n\n\n52\n0.52\n\n\n50\n0.50\n\n\n48\n0.48\n\n\n51\n0.51\n\n\n54\n0.54\n\n\n38\n0.38\n\n\n49\n0.49\n\n\n43\n0.43\n\n\n50\n0.50\n\n\n46\n0.46\n\n\n40\n0.40\n\n\n36\n0.36\n\n\n36\n0.36\n\n\n44\n0.44\n\n\n48\n0.48\n\n\n28\n0.28\n\n\n40\n0.40\n\n\n36\n0.36\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n42\n0.42\n\n\n46\n0.46\n\n\n57\n0.57\n\n\n40\n0.40\n\n\n36\n0.36\n\n\n51\n0.51\n\n\n46\n0.46\n\n\n51\n0.51\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n48\n0.48\n\n\n40\n0.40\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n50\n0.50\n\n\n44\n0.44\n\n\n42\n0.42\n\n\n43\n0.43\n\n\n42\n0.42\n\n\n53\n0.53\n\n\n42\n0.42\n\n\n47\n0.47\n\n\n42\n0.42\n\n\n32\n0.32\n\n\n45\n0.45\n\n\n51\n0.51\n\n\n44\n0.44\n\n\n39\n0.39\n\n\n34\n0.34\n\n\n52\n0.52\n\n\n40\n0.40\n\n\n38\n0.38\n\n\n52\n0.52\n\n\n43\n0.43\n\n\n40\n0.40\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n33\n0.33\n\n\n41\n0.41\n\n\n47\n0.47\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n45\n0.45\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n42\n0.42\n\n\n36\n0.36\n\n\n44\n0.44\n\n\n53\n0.53\n\n\n46\n0.46\n\n\n50\n0.50\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n44\n0.44\n\n\n40\n0.40\n\n\n52\n0.52\n\n\n47\n0.47\n\n\n48\n0.48\n\n\n54\n0.54\n\n\n46\n0.46\n\n\n42\n0.42\n\n\n49\n0.49\n\n\n48\n0.48\n\n\n53\n0.53\n\n\n45\n0.45\n\n\n45\n0.45\n\n\n46\n0.46\n\n\n45\n0.45\n\n\n53\n0.53\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n40\n0.40\n\n\n39\n0.39\n\n\n48\n0.48\n\n\n50\n0.50\n\n\n43\n0.43\n\n\n30\n0.30\n\n\n47\n0.47\n\n\n39\n0.39\n\n\n45\n0.45\n\n\n31\n0.31\n\n\n44\n0.44\n\n\n47\n0.47\n\n\n45\n0.45\n\n\n47\n0.47\n\n\n43\n0.43\n\n\n45\n0.45\n\n\n40\n0.40\n\n\n49\n0.49\n\n\n39\n0.39\n\n\n\n\n\n\n\nNote the the number of heads in the original sample is 45.\nFigure 2 plots the 500 different estimates for \\(\\pi\\) based on the resampled data.\n\n\nCode\ncoin_resample_df %&gt;%\n  ggplot(aes(x = p_hat_b)) +\n  geom_histogram(bins = 12,color=\"white\") + \n  labs(x = \"Estimate\",y = \"Count\") \n\n\n\n\n\nFigure 2: 500 estimates of the probability of success, \\(\\pi\\) from resampled data.\n\n\n\n\nQuestion: What are your takeaways from our simulation experiment and plot in Figure 2? How do the results compare and contrast with those of the previous simulation and plot in Figure 1?"
  },
  {
    "objectID": "lesson09/index.html#statistical-hypothesis-testing",
    "href": "lesson09/index.html#statistical-hypothesis-testing",
    "title": "Lesson 9",
    "section": "Statistical Hypothesis Testing",
    "text": "Statistical Hypothesis Testing\nConsider the question, “is a given coin fair”? One approach to addressing this question is by using point estimation and confidence intervals. For example, we could flip the coin 100 times and observe 60 heads. We could then estimate the probability of heads as \\(\\hat{\\pi} = 0.6\\) and construct a 95% confidence interval for \\(\\pi\\). If the confidence interval contains 0.5, then we have little reason to doubt that the coin is fair. If the confidence interval does not contain 0.5, then we have evidence to doubt that the coin is fair.\nExercise: Explain the reasoning behind the previous paragraph.\nAnother approach to addressing the question of whether a coin is fair is by using statistical hypothesis testing. That is, we rephrase our problem as follows:\n\nNull Hypothesis: The coin is fair, that is, \\(\\pi_{0} = 0.5\\).\nAlternative Hypothesis: The coin is not fair, that is, \\(\\pi_{0} \\neq 0.5\\).\n\nWe then collect data, i.e., toss our coin many times to try to refute the null hypothesis. Then, we assess whether the evidence is sufficiently strong to reject the null hypothesis or not. The way we do this is by computing a test statistic. The test statistic is a function of the data that we use to decide whether to reject the null hypothesis. For example, we could flip the coin 100 times and observe 60 heads. We could then compute the test statistic \\(T = \\frac{60}{100} = 0.6\\). If the null hypothesis is true and coin is fair, then \\(T\\) should be close to 0.5. If the coin is not fair, then \\(T\\) should be far from 0.5. One thing we need to address is, “how far is far?” That is, how far from 0.5 does \\(T\\) need to be for us to reject the null hypothesis? This is where the concept of a p-value comes in.\n\nThe p-value is the probability of observing a test statistic as or more extreme than the one we observed if the null hypothesis is true.\n\nLet’s work through a computational example to get a better sense of how statistical hypothesis testing works. We begin by simulating 100 tosses of a fair coin.\n\nset.seed(4312)\n\nN_tosses &lt;- 100\n\ncoin_toss_df &lt;- tibble(coin_toss = rbinom(N_tosses,1,0.5)) %&gt;%\n  mutate(heads_tails = ifelse(coin_toss == 1, \"heads\", \"tails\"))\n\nglimpse(coin_toss_df)\n\nRows: 100\nColumns: 2\n$ coin_toss   &lt;int&gt; 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0…\n$ heads_tails &lt;chr&gt; \"heads\", \"tails\", \"heads\", \"tails\", \"tails\", \"tails\", \"hea…\n\n\nTable 3 shows the results of the 100 tosses of the fair coin.\n\n\nCode\ntoss_results &lt;- table(coin_toss_df$heads_tails)\n\ntoss_results %&gt;%\n  kable() %&gt;%\n  kable_styling()\n\n\n\n\nTable 3: Results of 100 tosses of a fair coin. The coin toss results are stored in a variable named toss_results.\n\n\nVar1\nFreq\n\n\n\n\nheads\n52\n\n\ntails\n48\n\n\n\n\n\n\n\n\nWe can also visualize the results using a bar chart as shown in Figure 7.\n\n\nCode\ncoin_toss_df %&gt;%\n  ggplot(aes(x = heads_tails)) +\n  geom_bar(fill=\"lightblue\") +\n  labs(x = \"Coin Toss\", y = \"Count\", title = \"Coin Toss Results\")\n\n\n\n\n\nFigure 7: Bar chart showing the results of 100 tosses of a fair coin. The coin toss results are stored in a variable named toss_results.\n\n\n\n\nOur question is, could this be the result of a fair coin? Let’s proceed to address this question first by computing our point estimate for the probability of heads.\n\n\nCode\n#| code-fold: false\n#| message: false\n#| warning: false\n\n\np_hat &lt;- coin_toss_df %&gt;%\n  specify(response = heads_tails, success = \"heads\") %&gt;%\n  calculate(stat = \"prop\")\n\np_hat\n\n\nResponse: heads_tails (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  0.52\n\n\nThus, our point estimate is 0.52. Note that this value was obtained using functions from the infer package.\nWe can use bootstrapping to estimate the uncertainty in our point estimate. In order to do this, we will create bootstrap resamples using the function bootstraps from the rsample package.\n\ncoin_boots &lt;- bootstraps(coin_toss_df, times = 1000)\n\ncoin_boots\n\n# Bootstrap sampling \n# A tibble: 1,000 × 2\n   splits           id           \n   &lt;list&gt;           &lt;chr&gt;        \n 1 &lt;split [100/37]&gt; Bootstrap0001\n 2 &lt;split [100/37]&gt; Bootstrap0002\n 3 &lt;split [100/37]&gt; Bootstrap0003\n 4 &lt;split [100/37]&gt; Bootstrap0004\n 5 &lt;split [100/33]&gt; Bootstrap0005\n 6 &lt;split [100/37]&gt; Bootstrap0006\n 7 &lt;split [100/33]&gt; Bootstrap0007\n 8 &lt;split [100/37]&gt; Bootstrap0008\n 9 &lt;split [100/36]&gt; Bootstrap0009\n10 &lt;split [100/38]&gt; Bootstrap0010\n# ℹ 990 more rows\n\n\nNote that this results in a data frame that stores the bootstrap resamples in a list column named splits. In order to extract the data from a split object, we use the function analysis.\n\ncoin_boots$splits[[1]] %&gt;% \n  analysis()\n\n# A tibble: 100 × 2\n   coin_toss heads_tails\n       &lt;int&gt; &lt;chr&gt;      \n 1         1 heads      \n 2         0 tails      \n 3         1 heads      \n 4         1 heads      \n 5         1 heads      \n 6         1 heads      \n 7         0 tails      \n 8         0 tails      \n 9         0 tails      \n10         1 heads      \n# ℹ 90 more rows\n\n\nNow, we need to compute the point estimate for each of the resampled data sets. We can do this using the function map_dbl from the purrr package. First, we need a function that will compute our point estimate from any given split object. We create such a function and call it resample_prop. Then, we use map_dbl to apply this function to each of the splits in our data frame and add the results to the data frame that contains our bootstrap resamples.\n\nresample_prop &lt;- function(split){\n  \n num_heads &lt;-  split %&gt;%\n    analysis() %&gt;%\n    .$coin_toss %&gt;%\n    sum()\n \n return(num_heads/N_tosses)\n \n}\n\ncoin_boots &lt;- coin_boots %&gt;%\n  mutate(prop_heads = map_dbl(splits, resample_prop))\n\nFigure 8 shows a histogram of the bootstrap distribution for \\(\\hat{\\pi}\\).\n\n\nCode\ncoin_boots %&gt;%\n  ggplot(aes(x = prop_heads)) +\n  geom_histogram(bins = 20,fill=\"lightblue\") +\n  geom_vline(xintercept = p_hat$stat[1], color = \"darkgreen\", linewidth = 1, linetype=\"dashed\") +\n  labs(x = \"Proportion of Heads\", y = \"Count\", title = \"Bootstrapped Proportions of Heads\")\n\n\n\n\n\nFigure 8: Histogram of the bootstrap distribution for \\(\\hat{\\pi}\\).\n\n\n\n\nThen, our estimated confidence interval is obtained using the following command:\n\nquantile(coin_boots$prop_heads, c(0.025, 0.975))\n\n 2.5% 97.5% \n 0.42  0.61 \n\n\nWe see that 0.5 is contained in the 95% bootstrap confidence interval. One can also compute a 95% confidence interval using the function prop.test from the stats package. This function does not use bootstrapping but instead uses classical formulas for the confidence interval.\n\nprop.test(toss_results[1],N_tosses,correct = FALSE)$conf.int\n\n[1] 0.4231658 0.6153545\nattr(,\"conf.level\")\n[1] 0.95\n\n\nWe obtain nearly the same results. Let’s call prop.test again but this time print out more than just the CI:\n\nprop.test(toss_results[1],N_tosses,correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  toss_results[1] out of N_tosses, null probability 0.5\nX-squared = 0.16, df = 1, p-value = 0.6892\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4231658 0.6153545\nsample estimates:\n   p \n0.52 \n\n\nIn addition to the CI and other information, this function reports a p-value. Again, the p-value is the probability of observing a point estimate that is as or more extreme that the one we initially computed from the data, assuming that the null hypothesis is true. Here the p-value is computed using classical formulas and not bootstrapping. In this example, the function prop.test returns a p-value of about 0.69. We interpret this value as follows:\n\nAssuming the null hypothesis to be true (i.e., the coin is fair), there is about a 69% chance of observing a point estimate that is as or more extreme than the one we obtained from the data. Thus, the results of our coin toss are not at all rare so we should probably not consider our data as providing strong evidence against the null hypothesis. One would typically say, “we decide to fail to reject the null hypothesis.”\n\nThe infer package contains functions that use resampling-based methods to obtain a p-value. Specifically, we use simulations to generate a null distribution using the function generate. The function get_p_value can be used to obtain a p-value for any statistic that is computed using the calculate function. In order to use this function, we need to first specify the null hypothesis. We do this using the function hypothesize. Then, we generate a null distribution using the function generate. Finally, we use the function get_p_value to obtain the p-value.\n\nnull_dist &lt;- coin_toss_df %&gt;%\n  specify(response = heads_tails, success = \"heads\") %&gt;%\n  hypothesize(null = \"point\", p = .5) %&gt;%\n  generate(reps = 1000) %&gt;%\n  calculate(stat = \"prop\")\n\nOnce we have generated a null distribution, we can use the function get_p_value to obtain a p-value for any statistic that is computed using the calculate function. In this case, we want to obtain a p-value for the statistic \\(\\hat{\\pi}\\). We do this using the following command:\n\nnull_dist %&gt;%\n  get_p_value(obs_stat = p_hat, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.742\n\n\nWhile we do not obtain the same exact p-value, our conclusion remains the same. We fail to reject the null hypothesis. We can visualize this result using the functions visualize and shade_p_value. The resulting visualization is shown in Figure 9. The p-value is the area under the curve that is shaded in red. The p-value is the probability of observing a point estimate that is as or more extreme than the one we obtained from the data, assuming that the null hypothesis is true; and this area is shaded in red in the plot.\n\n\nCode\nvisualize(null_dist) +\n  shade_p_value(obs_stat = p_hat, direction = \"two-sided\")\n\n\n\n\n\nFigure 9: Visualization of the p-value for the coin toss example. The p-value is the area that is shaded in red.\n\n\n\n\nExercise: Modify the code we have used to simulate data from a coin toss where the probability of heads is 0.4 instead of 0.5. If we test a hypothesis that the coin is fair, that is, that \\(\\pi = 0.5\\), do you think we should likely reject or fail to reject the null hypothesis? Use the techniques we have developed to test the hypothesis that the coin is fair, that is, that \\(\\pi = 0.5\\).\nGenerally, if we have a parameter, then we can test a statistical hypothesis about that parameter. For example, we can test a statistical hypothesis about a value for the mean \\(\\mu\\) associated with a normal distribution. Generalizing that, we can test a statistical hypothesis about a value for a slope in a linear regression model. In practically any such problem, what we can do is generate a null distribution via simulation and then see how likely the a value as or more extreme than the observed value of the statistic is under the null distribution. This is the basic idea behind resampling-based inference.\nNow that you have a background in the general idea of statistical inference and the approach we will take to inference using the bootstrap, let’s work together in an RStudio project to apply our knowledge to actual data science problems."
  },
  {
    "objectID": "links.html#podcasts",
    "href": "links.html#podcasts",
    "title": "Links",
    "section": "Podcasts",
    "text": "Podcasts\n\nEzra Klein Show episode featuring Brian Christian"
  },
  {
    "objectID": "lesson10/index.html",
    "href": "lesson10/index.html",
    "title": "Lesson 10",
    "section": "",
    "text": "Watch Why develop a data science code of ethics? video on YouTube.\n\n\n\n\n\n\n\n\n\nIn this lesson, we will learn about some of the ethical considerations of data science. We will explore the potential risks of algorithmic bias and the importance of data transparency and accountability. We will also examine the ethical implications of using data for targeted advertising, political influence, and other manipulative purposes. Finally, we will investigate the ethical considerations surrounding the use of artificial intelligence in healthcare.\nWe begin with an interactive activity. Take a moment to think about the following questions:\n\nWhat are some of the ethical considerations relevant to data science?\n\nThink of some one or two word answers to this question. Then, add your answers to the word cloud activity displayed momentarily.\nWatch Algorithmic Bias and Fairness video on YouTube."
  },
  {
    "objectID": "lesson10/index.html#overview",
    "href": "lesson10/index.html#overview",
    "title": "Lesson 10",
    "section": "",
    "text": "Watch Why develop a data science code of ethics? video on YouTube.\n\n\n\n\n\n\n\n\n\nIn this lesson, we will learn about some of the ethical considerations of data science. We will explore the potential risks of algorithmic bias and the importance of data transparency and accountability. We will also examine the ethical implications of using data for targeted advertising, political influence, and other manipulative purposes. Finally, we will investigate the ethical considerations surrounding the use of artificial intelligence in healthcare.\nWe begin with an interactive activity. Take a moment to think about the following questions:\n\nWhat are some of the ethical considerations relevant to data science?\n\nThink of some one or two word answers to this question. Then, add your answers to the word cloud activity displayed momentarily.\nWatch Algorithmic Bias and Fairness video on YouTube."
  },
  {
    "objectID": "lesson10/index.html#ethical-considerations-in-data-science",
    "href": "lesson10/index.html#ethical-considerations-in-data-science",
    "title": "Lesson 10",
    "section": "Ethical Considerations in Data Science",
    "text": "Ethical Considerations in Data Science\n\nAlgorithmic Bias:\n\nPrompt: Investigate a case where algorithmic bias has had significant real-world consequences. Discuss the impact on individuals or groups and consider the ethical implications of biased algorithms in decision-making processes.\nResource: To stop algorithmic bias, we first have to define it\n\nPrivacy and Data Collection:\n\nPrompt: Examine a situation where large-scale data collection has raised privacy concerns. Explore how the collected data was used and the potential consequences for individuals. Discuss the ethical considerations of balancing data-driven insights with the need to protect privacy.\nResource: The State of Consumer Data Privacy Laws in the US (And Why It Matters)\n\nPredictive Policing:\n\nPrompt: Explore the ethical implications of using predictive analytics in law enforcement. Discuss the potential biases and social implications of relying on algorithms to predict criminal behavior. Consider the impact on marginalized communities and the potential for reinforcing existing inequalities.\nResource: What’s Wrong with Predictive Policing?\n\nData Transparency and Accountability:\n\nPrompt: Research a case where lack of transparency in data collection or model development has led to controversy. Discuss the importance of transparency in data science, the potential risks of opaque algorithms, and the ethical responsibility of data scientists to ensure accountability.\nResource: Towards Transparent AI: A Survey of Techniques\n\nSocial Media and Manipulation:\n\nPrompt: Analyze a situation where social media data has been used for manipulation or misinformation. Explore the ethical considerations of using data for targeted advertising, political influence, or other manipulative purposes. Discuss the responsibility of data scientists in preventing the misuse of data.\nResource: The Ethics of Social Media Data Use\n\nEthical Use of AI in Healthcare:\n\nPrompt: Investigate the ethical considerations surrounding the use of artificial intelligence in healthcare. Explore issues such as patient consent, data security, and the potential biases in healthcare algorithms. Discuss the balance between using data for medical advancements and ensuring the well-being and privacy of patients.\nResource: Ethical Challenges of AI in Healthcare"
  },
  {
    "objectID": "lesson10/index.html#examples-of-ethical-issues-in-data-science",
    "href": "lesson10/index.html#examples-of-ethical-issues-in-data-science",
    "title": "Lesson 10",
    "section": "Examples of Ethical Issues in Data Science",
    "text": "Examples of Ethical Issues in Data Science\n\nFacebook-Cambridge Analytica Scandal (2018):\n\n\nDescription: The improper use of Facebook user data by Cambridge Analytica for political advertising purposes, leading to concerns about user privacy and data manipulation.\nResource: Cambridge Analytica: What we know so far\n\n\nPredictive Policing Bias in the United States:\n\nDescription: Instances where predictive policing algorithms demonstrated biases, potentially leading to discrimination against specific communities and reinforcing existing racial disparities.\nResource: The Perils of Predictive Policing\n\nGoogle Photos Mislabeling:\n\nDescription: Google Photos’ image recognition system mislabeling photos of people, including categorizing individuals of certain ethnicities as animals, highlighting issues of bias in machine learning models.\nResource: Google Photos, racial bias, and flawed machine learning\n\nProPublica’s Analysis of COMPAS (Correctional Offender Management Profiling for Alternative Sanctions):\n\nDescription: ProPublica’s investigation into the COMPAS algorithm used in criminal sentencing, revealing racial bias and disparities in the risk scores assigned to individuals.\nResource: Machine Bias\n\nAmazon’s Gender-Biased Recruitment Tool:\n\nDescription: Amazon’s AI-based recruitment tool favoring male candidates over female candidates, raising concerns about gender bias in the hiring process.\nResource: Amazon scraps secret AI recruiting tool that showed bias against women\n\nYouTube’s Algorithmic Recommendation of Extremist Content:\n\nDescription: Instances where YouTube’s recommendation algorithm unintentionally promoted extremist content, raising concerns about the role of algorithms in spreading misinformation and radicalization.\nResource: How YouTube’s Algorithm Distorts Reality\n\nHealth Insurance Premiums Based on Predictive Analytics:\n\nDescription: Concerns about the use of predictive analytics in setting health insurance premiums, potentially leading to discrimination against individuals based on predicted future health risks.\nResource: The Algorithmic Accountability Act\n\nUber’s Use of Greyball to Evade Authorities:\n\nDescription: Uber’s use of the Greyball program to identify and evade law enforcement, illustrating ethical concerns about the misuse of data and technology for regulatory evasion.\nResource: How Uber Deceives the Authorities Worldwide"
  },
  {
    "objectID": "lesson10/index.html#some-ways-to-address-ethical-issues-in-data-science",
    "href": "lesson10/index.html#some-ways-to-address-ethical-issues-in-data-science",
    "title": "Lesson 10",
    "section": "Some Ways to Address Ethical Issues in Data Science",
    "text": "Some Ways to Address Ethical Issues in Data Science\n\nEthical AI Guidelines by Major Tech Companies:\n\nDescription: Many major tech companies, including Google, Microsoft, and IBM, have developed ethical AI guidelines to promote responsible and ethical use of AI technologies.\nResource: Google AI Principles, Microsoft AI Principles, IBM AI Ethics\n\nFairness, Accountability, and Transparency in Machine Learning (FAT/ML) Research Community:\n\nDescription: The FAT/ML community focuses on developing research and best practices to address issues related to fairness, accountability, and transparency in machine learning systems.\nResource: FAT/ML\n\nAlgorithmic Impact Assessments:\n\nDescription: Proposals for conducting impact assessments to evaluate the potential social, ethical, and legal implications of algorithms before their deployment.\nResource: Algorithmic Impact Assessments: A Practical Framework for Public Policy\n\nOpen Source and Transparent AI Models:\n\nDescription: Advocacy for transparency in AI models, making the source code and model architecture openly available to encourage scrutiny and accountability.\nResource: OpenAI’s Charter\n\nData Ethics Frameworks for Organizations:\n\nDescription: Development and implementation of data ethics frameworks by organizations to guide responsible data collection, processing, and usage.\nResource: The Data Ethics Canvas\n\nAI Ethics Committees and Review Boards:\n\nDescription: Establishment of independent committees and review boards to evaluate and ensure the ethical implications of AI and data science projects.\nResource: The Privacy Tools Project\n\nResponsible Data Science Education:\n\nDescription: Integration of ethics education into data science curricula to ensure that practitioners are equipped with the knowledge and awareness of ethical considerations.\nResource: Data Science Ethics Resources\n\nLegislation and Regulations:\n\nDescription: Introduction of laws and regulations aimed at governing the ethical use of data and AI technologies, such as the European General Data Protection Regulation (GDPR).\nResource: EU General Data Protection Regulation (GDPR)"
  }
]