---
title: "Lesson 9"
subtitle: "Introduction to Statistics for Data Science"
author: "JMG"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson09.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| message: false
#| warning: false
#| echo: false

# load packages used in document
library(tidyverse)
library(tidymodels)
library(kableExtra)

theme_set(theme_minimal(base_size = 13))

tidymodels_prefer()

doParallel::registerDoParallel()
```

## Learning Objectives

After this lesson, students will be able to:

- Describe real-world examples of questions that can be answered with statistical inference.

- Define common population parameters (e.g., mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.

- Define the following statistical sampling terms: population, sample, population parameter, point estimate, and sampling distribution.

- Define bootstrapping.

- Use R to create a bootstrap distribution to approximate a sampling distribution.

## Readings, etc.

1) Read Chapter 10 from from [Data Science: A First Introduction](https://datasciencebook.ca/preface.html) [@timbers2022data]. [View book online](https://datasciencebook.ca/preface.html).

2) You might also want to look at chapters 8 & 9 from [Statistical Inference via Data Science](https://moderndive.com/).



## Overview

Statistical inference is the process of using data analysis to infer properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. 

In this lesson, we will get a feel for the inferential method. There are many details of statistics that we will leave for a later course. Further, our approach will be computational, utilizing the [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) resampling method.


## Motivating Example

Random variables and their distributions model processes that produce data. For example, a binomial random variable with probability of success $\pi = 0.5$ can be used to model the process of tossing a coin and observing the number of heads. This is an illustration of the domain of probability. 

Statistical inference is concerned with the inverse problem: given data, what can we say about the process that produced it? For example, given a sample of coin tosses, what can we say about the probability of heads? This is an illustration of the domain of statistics.

**Question:** Given a coin, how can you determine if it is fair or not? Think about how you could approach answering this question. 

Obviously, to address the previous question we should collect data. That is, we should toss the coin some number of times (probably very many)  and record the number of heads. Then, we can use the data to estimate the probability of heads.

**Question:** Suppose that we toss a coin 10 times and observe 7 heads. What is your best guess for the probability of heads? Suppose that we toss a coin 100 times and observe 70 heads. What is your best guess for the probability of heads?

At this point, there are a few things to take note of:

1. When we do statistical inference, we are often trying to estimate a parameter(s) of a distribution. The parameter(s) should be viewed as fixed but with unknown values. In this case, we call the parameter or parameters a **population parameter**. In the coin example, the population parameter is the probability of heads, $\pi$.  

2. We have some process to estimate the population parameter. This process inputs observed data and returns an estimated value for the parameter(s). In this case, we call the estimate a **point estimate**. In the coin example, the point estimate is the number of heads divided by the number of tosses, we denote this by $\hat{\pi}$.

3. The point estimate is a random variable. That is, if we were to repeat the experiment, we would get a different point estimate. This is because the observed data is random. In this case, we call the point estimate a **statistic**. In the coin example, the statistic is the number of heads divided by the number of tosses, that is, $\hat{\pi}$.

4. Since the point estimate is a random variable, it has a distribution. In this case, we call the distribution a **sampling distribution**. In the coin example, the sampling distribution is the distribution of $\hat{\pi}$. 

5. A key problem in statistical inference is to determine or describe the sampling distribution of a statistic. For example, we might be interested to know what is the mean and variance of the sampling distribution. The standard deviation of the sampling distribution of a statistic is called the **standard error**. We call the process of determining the sampling distribution **statistical inference**. In the coin example, we want to determine the sampling distribution of $\hat{\pi}$.

6. One can use the sampling distribution of a statistic to make statements about the uncertainty of point estimates. 
  

In mathematical statistics, there is a heavy focus on deriving closed form or asymptotically exact expressions for the sampling distribution of a statistic. This is amazing and beautiful and well worth learning. For this, we highly recommend [@wasserman2004all] or the online book [Introduction to Probability for Data Science](https://probability4datascience.com/). However, this approach is beyond the scope of this course. Further, the precise formulas of mathematical statistics apply in a much too limited range of application for the purposes of modern data science.   

As an alternative to mathematical statistics, one can take a computational approach. That is, use the computer to simulate the sampling distribution of a statistic. This approach is called **bootstrapping**. The bootstrap is a powerful tool that can be used to approximate the sampling distribution of a statistic. It is also a useful tool for hypothesis testing and obtaining confidence intervals. Even our coverage of the bootstrap will be only introductory. To learn more beyond what we cover, we recommend [@efron2022computer].

## Resampling Experiment

Let's start with a simulation. Here's what we will do: Simulate 100 tosses of a fair coin, that is, sample from a binomial random variable with $\pi = 0.5$, count the number of heads, then estimate $\pi$ by dividing the number of heads by 100. Then, we will repeat this for 500 times. The code below does this and displays the results in a table. 

```{r}
set.seed(1234)
coin_df <- tibble(num_heads = rbinom(500,size=100,prob=0.5))

coin_df <- coin_df %>%
  mutate(p_hat = num_heads/100)

coin_df %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```


@fig-coin-plot plots the 500 different estimates for $\pi$.

```{r}
#| label: fig-coin-plot 
#| fig-cap: 500 estimates of the probability of success, $\pi$ from simulated data.
coin_df %>%
  ggplot(aes(x = p_hat)) +
  geom_histogram(bins = 12,color="white") + 
  labs(x = "Estimate",y = "Count") 
```


**Question:** What are your takeaways from our simulation experiment and plot in @fig-coin-plot? What, if anything, do you think we can say about the sampling distribution of $\pi$?

Now, let's try a slightly different simulation. We will do a single round of tossing a fair coin 100 times. Then, we will resample from the 100 tosses *with replacement* 500 times. For each resample, we will count the number of heads and again estimate $\pi$ by dividing the number of heads by 100. The code in below does this and displays the results in a table. 

```{r}    
set.seed(1234)
coin_sample <- rbinom(100,size=1,prob=0.5)

coin_resample_df <- tibble(num_heads = replicate(500,sum(sample(coin_sample,100,replace=TRUE)))) %>%
  mutate(p_hat_b = num_heads/100)


coin_resample_df %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```


Note the the number of heads in the original sample is `r sum(coin_sample)`. 

@fig-coin-resample-plot plots the 500 different estimates for $\pi$ based on the *resampled* data.

```{r}
#| label: fig-coin-resample-plot
#| fig-cap: 500 estimates of the probability of success, $\pi$ from resampled data.

coin_resample_df %>%
  ggplot(aes(x = p_hat_b)) +
  geom_histogram(bins = 12,color="white") + 
  labs(x = "Estimate",y = "Count") 

```


**Question:** What are your takeaways from our simulation experiment and plot in @fig-coin-resample-plot? How do the results compare and contrast with those of the previous simulation and plot in @fig-coin-plot? 

## Bootstrap

Our resampling simulation in the last section is an example of the **bootstrap**. Notice what we did, we took a sample from the population, then resampled from the sample. This is called **resampling**. The bootstrap is a special case of resampling where the resampling is done with replacement. What we did was generate a bootstrap distribution for our statistic. The idea is, 

> if the sample is representative of the population, then the bootstrap distribution of our statistics obtained by resampling with replacement from the sample should approximate the sampling distribution for our statistic.

While the mean of the bootstrap will be the mean of the sample and not necessarily the mean of the population, the standard deviation of the bootstrap will be a good estimate for the standard error of the statistic. Thus, the bootstrap distribution allows us to assess the uncertainty of an estimate. The power of the bootstrap technique is that it works for any statistic. For example, we can use the bootstrap to estimate the sampling distribution for the mean for a normal random variable, or parameters in models such as the coefficients in a linear regression model. We can even use the bootstrap to estimate the uncertainty in model predictions, even if the model is non-parametric.  

### Example: Bootstrap Mean

Let's take $n = 50$ samples $X_{1},X_{2},\ldots, X_{n}$ from a normal distribution with $\mu = 10$ and $\sigma = 1.75$, that is, with $X_{i} \sim \text{Norm}(\mu = 10, \sigma = 1.75 )$:

```{r}
#| code-fold: false

set.seed(4321)
n <- 50
mu <- 10
sigma <- 1.75
x <- rnorm(n,mu,sigma)
```

A point estimate for the mean $\mu$ is the sample mean $\hat{\mu}$ defined by

$$
\hat{\mu} = \frac{X_{1} + X_{2} + \cdots + X_{n}}{n}
$$
Let's compute the sample mean for our sample:

```{r}
#| code-fold: false


(x_bar <- mean(x))
```

Now, let's build the bootstrap distribution for the sample mean. We will do this by resampling with replacement from the sample 500 times. For each resample, we will compute the sample mean. The code below does this and displays the results. 

```{r}
#| code-fold: false


mean_resample_df <- tibble(x_bar_b = replicate(500,mean(sample(x,n,replace=TRUE)))) 

mean_resample_df %>%
  ggplot(aes(x = x_bar_b)) +
  geom_histogram(bins = 12,color="white") + 
  labs(x = "Sample mean",y = "Count") 
```

Now, statistical theory tell us that the standard error of the sample mean is given by

$$
\text{SE}(\hat{\mu}) = \frac{\sigma}{\sqrt{n}}
$$
where $\sigma$ is the standard deviation of the population. Let's compute the standard error of the sample mean for our sample:

```{r}
#| code-fold: false


(se <- sigma/sqrt(n))
```

Let's compare this with the standard deviation of our bootstrap distribution:

```{r}
#| code-fold: false


sd(mean_resample_df$x_bar_b)
```

We use confidence intervals (CIs) to quantify the uncertainty in our estimates. Classically, a 95% CI for the mean is computed in R using:

```{r}
#| code-fold: false


t.test(x)$conf.int
```

or using a function from the `infer` package:

```{r}
#| code-fold: false


sampling_dist <- tibble(x=x) %>%
  specify(response = x) %>%
  assume("t")

sample_mean <- tibble(x=x) %>%
  specify(response = x) %>%
  calculate(stat = "mean")

get_confidence_interval(sampling_dist,point_estimate = sample_mean,level=0.95,type="se")
```

We can estimate this CI using the bootstrap. The code below computes an estimated 95% CI for the mean using the bootstrap:

```{r}
#| code-fold: false


quantile(mean_resample_df$x_bar_b,probs = c(0.025,0.975))

```

or using the `infer` package:

```{r}
#| code-fold: false


mean_resample_df %>%
  get_confidence_interval(level=0.95)
  

```

**Exercise:** Redo what we have done in bootstrapping the mean of a sample from a normal distribution but instead bootstrap the median and estimate the standard error and a 95% CI for the statistic. You can use the fact that the function `median` computes the sample median. How do the results compare and contrast with those of the mean?


### Example: Linear Regression

Let's suppose that we have samples from a normal distribution but where the mean varies as a linear function of a variable $x$. That is, we have samples from a distribution $\text{Norm}(\beta_{0} + \beta_{1}x, \sigma)$, where $x$ ranges over some values. This is equivalent to assuming that we have a random variable $Y$ that satisfies

$$
Y = \beta_{0} + \beta_{1}x + \epsilon
$$
and $\epsilon \sim \text{Norm}(0,\sigma)$. 

We can use the bootstrap to estimate the uncertainty in estimates for the parameters $\beta_{0}$ and $\beta_{1}$ in a manner similar to how we estimated the uncertainty in the sample mean in the last subsection. However, in this case, the mean is estimated by a linear regression model. That is, we have estimators $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$, values of which are obtained by fitting a linear model to the data. Bootstrapping allows us to approximate the sampling distributions for these estimators.

First, we will simulate some data. The simulated data is shown in @fig-lm-sim. Notice that the true mean is given by $y = 0.5 x + 2$ which appears as the red dashed line in the figure.

```{r}
#| code-fold: false
#| message: false
#| label: fig-lm-sim
#| fig-cap: Simulated data for simple linear regression. The true mean is given by $y = 0.5 x + 2$ 

# sample size
N <- 25

# true intercept and slope parameters
beta_0 <- 2
beta_1 <- 0.5

# x-values
x <- seq(0, 3, length.out = N)

# data simulation
lm_samp <- function(x,samp_n=10,sd_val=1){
  
  # sample from a normal distribution with mean = beta_0 + beta_1*x
  y <- rnorm(samp_n, mean=beta_0 + beta_1*x, sd=sd_val)
  
  tibble(x=x,y=list(y))
  
}

# obtain simulated data
lm_samp_df <- map_dfr(x,lm_samp)

lm_samp_df_l <- lm_samp_df %>%
  unnest(y)

# plot simulated data
lm_samp_df_l %>%
  ggplot(aes(x=x,y=y)) +
  geom_point(alpha=0.6) +
  geom_abline(intercept=beta_0,slope=beta_1,color="red",
              linewidth=1,linetype="dashed") + 
  geom_smooth(method="lm",color="orange",fill="lightblue")
```


The orange line in @fig-lm-sim is the fitted linear model and the light blue shaded region shows the corresponding standard error computed by traditional methods.

In R, the function `lm` is used to fit a linear model. The code below fits a linear model to the simulated data and prints the estimated intercept and slope parameters, that is, the point estimates for $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ in @tbl-lm-coeffs.

```{r}
#| code-fold: false
#| message: false
#| label: tbl-lm-coeffs
#| tbl-cap: Estimated intercept and slope parameters for the linear model fit to the simulated data.

linear_mod <- lm(y ~ x, data=lm_samp_df_l)

tidy(linear_mod) %>%
  select(estimate) %>%
  mutate(coeff=c("beta_0","beta_1")) %>%
  kable() %>%
  kable_styling()
```


We would like to assess the  uncertainty in our point estimates. Bootstrapping is one way to do this and the following code implements the bootstrap for our example. @fig-lm-boot-coeffs-1 shows the bootstrap distribution for the intercept and slope parameters. 


```{r}
#| code-fold: false
#| message: false
#| label: fig-lm-boot-coeffs-1
#| fig-cap: Bootstrap distributions for the intercept and slope parameters.

lm_fit_resamp <- function(int_val,df=lm_samp_df_l){
  lm(y ~ x, data=slice_sample(df,prop=1,replace=TRUE)) %>%
    tidy() %>%
    select(estimate) %>%
    mutate(coeff=c("beta_0","beta_1"))
}


lm_fit_boot <- map_dfr(1:500,lm_fit_resamp)

lm_fit_boot %>%
  ggplot(aes(x=estimate)) +
  geom_histogram(color="white",fill="lightblue") +
  facet_wrap(~coeff,scales="free_x")
```


In the last line of code, we manually resampled the data and fit a linear model to each resample. The code below does the same thing but uses the function `reg_intervals` from the `rsample`` package to do the resampling and fitting. The results shown in @fig-lm-boot-coeffs-2 are similar to those we saw in @fig-lm-boot-coeffs-1.

```{r}
#| code-fold: false
#| message: false
#| label: fig-lm-boot-coeffs-2
#| fig-cap: Bootstrap distributions for the intercept and slope parameters. This time, the function `reg_intervals` is used to do the resampling and fitting.

reg_intervals(y ~ x, data=lm_samp_df_l,
              type="percentile",filter=NULL,
              keep_reps = TRUE) %>%
  unnest(.replicates) %>%
  ggplot(aes(x=estimate)) + 
  geom_histogram(color="white",fill="lightblue") +
  facet_wrap(~term,scales="free_x")
```


We can also use `reg_intervals` to obtain the confidence intervals for each parameter estimate. The results are shown in 

```{r}
#| code-fold: false
#| label: tbl-lm-ci
#| tbl-cap: Confidence intervals for the intercept and slope parameters obtained using the `reg_intervals` function.


reg_intervals(y ~ x, data=lm_samp_df_l,
              type="percentile",filter=NULL) %>%
  kable() %>%
  kable_styling()
```

Finally, we can use our bootstrapped estimates to plot the bootstrap distribution of the regression line. The results are shown in @fig-lm-boot. The orange line is the fitted linear model, the yellow shaded region shows the corresponding standard error computed by traditional methods,  and the light blue lines are regression lines fitted to the resampled data. The light blue lines are the bootstrap distribution of the regression line. 

```{r}
#| code-fold: false
#| message: false
#| warning: false
#| label: fig-lm-boot
#| fig-cap: Bootstrap distribution of the regression line for the simulated data. The true value is given by $y = 0.5 x + 2$

tpl <- lm_fit_boot %>% pivot_wider(names_from=coeff,values_from=estimate) %>%
  unnest(cols = c(beta_0, beta_1))


lm_samp_df_l %>%
  ggplot(aes(x=x,y=y)) +
  geom_point(alpha=0.6) +
  geom_abline(data=tpl,
              aes(intercept=beta_0,slope=beta_1),
              color="lightblue") +
  geom_smooth(method="lm",color="orange",fill="yellow") 
```


## Statistical Hypothesis Testing

Consider the question, "is a given coin fair"? One approach to addressing this question is by using point estimation and confidence intervals. For example, we could flip the coin 100 times and observe 60 heads. We could then estimate the probability of heads as $\hat{\pi} = 0.6$ and construct a 95% confidence interval for $\pi$. If the confidence interval contains 0.5, then we have little reason to doubt that the coin is fair. If the confidence interval does not contain 0.5, then we have evidence to doubt that the coin is fair. 

**Exercise:** Explain the reasoning behind the previous paragraph.

Another approach to addressing the question of whether a coin is fair is by using statistical hypothesis testing. That is, we rephrase our problem as follows:

- **Null Hypothesis:** The coin is fair, that is, $\pi = 0.5$.

- **Alternative Hypothesis:** The coin is not fair, that is, $\pi \neq 0.5$.

We then collect data, *i.e.*, toss our coin many times to try to refute the null hypothesis. Then, we assess whether the evidence sufficiently strong to reject the null hypothesis or not. The way we do this is by computing a test statistic. The test statistic is a function of the data that we use to decide whether to reject the null hypothesis. For example, we could flip the coin 100 times and observe 60 heads. We could then compute the test statistic $T = \frac{60}{100} = 0.6$. If the null hypothesis is true and coin is fair, then $T$ should be close to 0.5. If the coin is not fair, then $T$ should be far from 0.5. One thing we need to address is, "how far is far?" That is, how far from 0.5 does $T$ need to be for us to reject the null hypothesis? This is where the concept of a [**p-value**](https://en.wikipedia.org/wiki/P-value) comes in. 

> The p-value is the probability of observing a test statistic as extreme or more extreme than the one we observed if the null hypothesis is true. 

Let's work through a computational example to get a better sense of how statistical hypothesis testing works. We begin by simulating 100 tosses of a fair coin. 

```{r}
#| code-fold: false
#| message: false
#| warning: false


set.seed(4312)

N_tosses <- 100

coin_toss_df <- tibble(coin_toss = rbinom(N_tosses,1,0.5)) %>%
  mutate(heads_tails = ifelse(coin_toss == 1, "heads", "tails"))

glimpse(coin_toss_df)
```

@tbl-coin-toss-results shows the results of the 100 tosses of the fair coin. 

```{r}
#| code-fold: false
#| message: false
#| warning: false
#| label: tbl-coin-toss-results
#| tbl-cap: Results of 100 tosses of a fair coin. The coin toss results are stored in a variable named `toss_results`.

toss_results <- table(coin_toss_df$heads_tails)

toss_results %>%
  kable() %>%
  kable_styling()

```


We can also visualize the results using a bar chart as shown in @fig-coin-toss-results. 

```{r}
#| message: false
#| warning: false
#| label: fig-coin-toss-results
#| fig-cap: Bar chart showing the results of 100 tosses of a fair coin. The coin toss results are stored in a variable named `toss_results`.


coin_toss_df %>%
  ggplot(aes(x = heads_tails)) +
  geom_bar(fill="lightblue") +
  labs(x = "Coin Toss", y = "Count", title = "Coin Toss Results")
```

Our question is, could this be the result of a fair coin? Let's proceed to address this question first by computing our point estimate for the probability of heads.

```{r}

#| code-fold: false
#| message: false
#| warning: false


p_hat <- coin_toss_df %>%
  specify(response = heads_tails, success = "heads") %>%
  calculate(stat = "prop")

p_hat

```

Thus, our point estimate is `r p_hat`. Note that this value was obtained using functions from the `infer` package. 

We can use bootstrapping to estimate the uncertainty in our point estimate. In order to do this, we will create bootstrap resamples using the function `bootstraps` from the `rsample` package. 

```{r}
#| code-fold: false
#| message: false
#| warning: false


coin_boots <- bootstraps(coin_toss_df, times = 1000)

coin_boots
```


Note that this results in a data frame that stores the bootstrap resamples in  a list column named `splits`. In order to extract the data from a split object, we use the function `analysis`. 

```{r}
#| code-fold: false
#| message: false
#| warning: false


coin_boots$splits[[1]] %>% 
  analysis()
```

Now, we need to compute the point estimate for each of the resampled data sets. We can do this using the function `map_dbl` from the `purrr` package. First, we need a function that will compute our point estimate from any given split object. We create such a function and call it `resample_prop`. Then, we use `map_dbl` to apply this function to each of the splits in our data frame and add the results to the data frame that contains out bootstrap resamples.    

```{r}
#| code-fold: false
#| message: false
#| warning: false


resample_prop <- function(split){
  
 num_heads <-  split %>%
    analysis() %>%
    .$coin_toss %>%
    sum()
 
 return(num_heads/N_tosses)
 
}

coin_boots <- coin_boots %>%
  mutate(prop_heads = map_dbl(splits, resample_prop))

```

@fig-coin-toss-resamp-results shows a histogram of the bootstrap distribution for $\hat{\pi}$.


```{r}
#| message: false
#| warning: false
#| label: fig-coin-toss-resamp-results
#| fig-cap: Histogram of the bootstrap distribution for $\hat{\pi}$.

coin_boots %>%
  ggplot(aes(x = prop_heads)) +
  geom_histogram(bins = 20,fill="lightblue") +
  geom_vline(xintercept = p_hat$stat[1], color = "darkgreen", linewidth = 1, linetype="dashed") +
  labs(x = "Proportion of Heads", y = "Count", title = "Bootstrapped Proportions of Heads")
```

Then, our estimated confidence interval is obtained using the following command:

```{r}
#| code-fold: false
#| message: false
#| warning: false


quantile(coin_boots$prop_heads, c(0.025, 0.975))
```


We see that 0.5 is contained in the 95% bootstrap confidence interval. One can also compute a 95% confidence interval using the function `prop.test` from the `stats` package. This function does not use bootstrapping but instead uses classical formulas for the confidence interval.


```{r}
#| code-fold: false
#| message: false
#| warning: false


prop.test(toss_results[1],N_tosses,correct = FALSE)$conf.int

```

We obtain nearly the same results. Let's call `prop.test` again but this time print out more than just the CI:

```{r}
#| code-fold: false
#| message: false
#| warning: false


prop.test(toss_results[1],N_tosses,correct = FALSE)

```


In addition to the CI and other information, this function reports a p-value. Again, the p-value is the probability of observing a point estimate that is as or more extreme that the one we initially computed from the data, assuming that the null hypothesis is true.  Here  the p-value is computed using classical formulas and not bootstrapping. The function `prop_test` from `infer` gives a tidied version of the output from `prop.test`.


```{r}
#| code-fold: false
#| message: false
#| warning: false


prop_test(coin_toss_df,heads_tails ~ NULL, 
          succes = "heads", 
          p = 0.5, z = TRUE,
          conf_int = TRUE,
          conf_level = 0.95) 
```

Whichever of the functions `prop.test` or `prop_test` we use to compute it, the p-value is about 0.69. We interpret this value as follows: Assuming the null hypothesis to be true, there is about a 69% chance of observing a point estimate that is as or more extreme than the one we obtained from the data. Thus, the results of our coin toss are not at all rare so we should probably not consider our data as providing strong evidence against the null hypothesis. One would typically say, "we decide to fail to reject the null hypothesis." 


The `infer` package contains functions that use resampling-based methods to obtain a p-value. Specifically, we use simulations to we generate a null distribution using the function `generate`. the function `get_p_value` can be used to obtain a p-value for any statistic that is computed using the `calculate` function. In order to use this function, we need to first specify the null hypothesis. We do this using the function `hypothesize`. Then, we generate a null distribution using the function `generate`. Finally, we use the function `get_p_value` to obtain the p-value.

```{r}
#| code-fold: false
#| message: false
#| warning: false


null_dist <- coin_toss_df %>%
  specify(response = heads_tails, success = "heads") %>%
  hypothesize(null = "point", p = .5) %>%
  generate(reps = 1000) %>%
  calculate(stat = "prop")

```

Once we have generated a null distribution, we can use the function `get_p_value` to obtain a p-value for any statistic that is computed using the `calculate` function. In this case, we want to obtain a p-value for the statistic $\hat{\pi}$. We do this using the following command:

```{r}
#| code-fold: false
#| message: false
#| warning: false


null_dist %>%
  get_p_value(obs_stat = p_hat, direction = "two-sided")
```

While we do not obtain the same exact p-value, our conclusion remains the same. We fail to reject the null hypothesis. We can visualize this result using the functions `visualize` and `shade_p_value`. The resulting visualization is shown in @fig-coin-toss-p-value. The p-value is the area under the curve that is shaded in red. The p-value is the probability of observing a point estimate that is as or more extreme than the one we obtained from the data, assuming that the null hypothesis is true; and this area is shaded in red in the plot.

```{r}
#| message: false
#| warning: false
#| label: fig-coin-toss-p-value
#| fig-cap: Visualization of the p-value for the coin toss example. The p-value is the area that is shaded in red.

visualize(null_dist) +
  shade_p_value(obs_stat = p_hat, direction = "two-sided")
```

**Exercise:** Modify the code we have used to simulate data from a coin toss where the probability of heads is 0.4 instead of 0.5. If we test a hypothesis that the coin is fair, that is, that $\pi = 0.5$,  do you think we should likely reject or fail to reject the null hypothesis? Use the techniques we have developed to test the hypothesis that the coin is fair, that is, that $\pi = 0.5$.


Generally, if we have a parameter, then we can test a statistical hypothesis about that parameter. For example, we can test a statistical hypothesis about a value for the mean $\mu$ associated with a normal distribution. Generalizing that, we can test a statistical hypothesis about a value for a slope in a linear regression model. In practically any such problem, what we can do is generate a null distribution via simulation and then see how likely the a value as or more extreme than the observed value  of the statistic is under the null distribution. This is the basic idea behind resampling-based inference. 

Now that you have a background in the general idea of statistical inference and the approach we will take to inference using the bootstrap, let's work together in an RStudio project to apply our knowledge to actual data science problems.

## References

::: {#refs}
:::

::: {.callout-tip collapse="true"}
## Expand for Session Info

```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```
:::

[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width="15%"}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)


